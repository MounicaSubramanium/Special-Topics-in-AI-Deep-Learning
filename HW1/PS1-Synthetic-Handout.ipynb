{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mounica Subramani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS7180 Problem Set 1: Implement a teacher-student network setting for Gaussian inputs (20 points)\n",
    "\n",
    "Welcome to CS7180!\n",
    "\n",
    "Before you start, make sure to read the problem description in the handout pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborators: Apoorva Durai, Manaswini, Sinjini Bose. Discussed concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "batch_size = 100\n",
    "width = 5\n",
    "d_input = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Implement a two-layer neural network with ReLU activation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_input, width):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "        \n",
    "        # Linear layer obtaining input to the hidden layer from the input layer\n",
    "        self.fc1 = torch.nn.Linear(d_input,width)\n",
    "        \n",
    "        # Applying activation to the output of hidden layer before feeding into final layer\n",
    "        self.act = torch.nn.ReLU()\n",
    "        \n",
    "        # Linear layer -> narrowing to 1 output from hidden layer\n",
    "        self.fc2 = torch.nn.Linear(width,1)\n",
    "        \n",
    "        # ------------------\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, d_input)\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "        \n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "N = 5 * width * d_input\n",
    "\n",
    "# random data from standard normal distribution\n",
    "x_train = torch.randn(N, d_input)\n",
    "x_test = torch.randn(N, d_input)\n",
    "\n",
    "# teacher network with random weights\n",
    "teacher = Net(d_input, width)\n",
    "\n",
    "# generate labels using the teacher network\n",
    "y_train = torch.FloatTensor([teacher.forward(x) for x in x_train])\n",
    "y_test = torch.FloatTensor([teacher.forward(x) for x in x_test])\n",
    "\n",
    "# combine the data and labels into pytorch friendly format\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Set up the quadratic loss function and an SGD optimizer (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 21.061852\n",
      "Epoch: 2 \tTraining Loss: 19.397266\n",
      "Epoch: 3 \tTraining Loss: 18.120869\n",
      "Epoch: 4 \tTraining Loss: 17.117247\n",
      "Epoch: 5 \tTraining Loss: 16.312916\n",
      "Epoch: 6 \tTraining Loss: 15.657936\n",
      "Epoch: 7 \tTraining Loss: 15.116551\n",
      "Epoch: 8 \tTraining Loss: 14.664048\n",
      "Epoch: 9 \tTraining Loss: 14.281998\n",
      "Epoch: 10 \tTraining Loss: 13.956030\n",
      "Epoch: 11 \tTraining Loss: 13.675710\n",
      "Epoch: 12 \tTraining Loss: 13.432755\n",
      "Epoch: 13 \tTraining Loss: 13.220798\n",
      "Epoch: 14 \tTraining Loss: 13.034711\n",
      "Epoch: 15 \tTraining Loss: 12.870484\n",
      "Epoch: 16 \tTraining Loss: 12.724731\n",
      "Epoch: 17 \tTraining Loss: 12.594853\n",
      "Epoch: 18 \tTraining Loss: 12.478600\n",
      "Epoch: 19 \tTraining Loss: 12.374182\n",
      "Epoch: 20 \tTraining Loss: 12.280092\n",
      "Epoch: 21 \tTraining Loss: 12.194963\n",
      "Epoch: 22 \tTraining Loss: 12.117729\n",
      "Epoch: 23 \tTraining Loss: 12.047459\n",
      "Epoch: 24 \tTraining Loss: 11.983365\n",
      "Epoch: 25 \tTraining Loss: 11.924751\n",
      "Epoch: 26 \tTraining Loss: 11.871025\n",
      "Epoch: 27 \tTraining Loss: 11.821671\n",
      "Epoch: 28 \tTraining Loss: 11.776239\n",
      "Epoch: 29 \tTraining Loss: 11.734344\n",
      "Epoch: 30 \tTraining Loss: 11.695641\n",
      "Epoch: 31 \tTraining Loss: 11.659824\n",
      "Epoch: 32 \tTraining Loss: 11.626631\n",
      "Epoch: 33 \tTraining Loss: 11.595822\n",
      "Epoch: 34 \tTraining Loss: 11.567187\n",
      "Epoch: 35 \tTraining Loss: 11.540544\n",
      "Epoch: 36 \tTraining Loss: 11.515714\n",
      "Epoch: 37 \tTraining Loss: 11.492553\n",
      "Epoch: 38 \tTraining Loss: 11.470922\n",
      "Epoch: 39 \tTraining Loss: 11.450697\n",
      "Epoch: 40 \tTraining Loss: 11.431772\n",
      "Epoch: 41 \tTraining Loss: 11.414046\n",
      "Epoch: 42 \tTraining Loss: 11.397429\n",
      "Epoch: 43 \tTraining Loss: 11.381837\n",
      "Epoch: 44 \tTraining Loss: 11.367195\n",
      "Epoch: 45 \tTraining Loss: 11.353436\n",
      "Epoch: 46 \tTraining Loss: 11.340497\n",
      "Epoch: 47 \tTraining Loss: 11.328319\n",
      "Epoch: 48 \tTraining Loss: 11.316852\n",
      "Epoch: 49 \tTraining Loss: 11.306046\n",
      "Epoch: 50 \tTraining Loss: 11.295859\n",
      "Epoch: 51 \tTraining Loss: 11.286248\n",
      "Epoch: 52 \tTraining Loss: 11.277178\n",
      "Epoch: 53 \tTraining Loss: 11.268612\n",
      "Epoch: 54 \tTraining Loss: 11.260519\n",
      "Epoch: 55 \tTraining Loss: 11.252872\n",
      "Epoch: 56 \tTraining Loss: 11.245639\n",
      "Epoch: 57 \tTraining Loss: 11.238796\n",
      "Epoch: 58 \tTraining Loss: 11.232322\n",
      "Epoch: 59 \tTraining Loss: 11.226194\n",
      "Epoch: 60 \tTraining Loss: 11.220389\n",
      "Epoch: 61 \tTraining Loss: 11.214890\n",
      "Epoch: 62 \tTraining Loss: 11.209678\n",
      "Epoch: 63 \tTraining Loss: 11.204739\n",
      "Epoch: 64 \tTraining Loss: 11.200054\n",
      "Epoch: 65 \tTraining Loss: 11.195611\n",
      "Epoch: 66 \tTraining Loss: 11.191396\n",
      "Epoch: 67 \tTraining Loss: 11.187393\n",
      "Epoch: 68 \tTraining Loss: 11.183597\n",
      "Epoch: 69 \tTraining Loss: 11.179991\n",
      "Epoch: 70 \tTraining Loss: 11.176566\n",
      "Epoch: 71 \tTraining Loss: 11.173314\n",
      "Epoch: 72 \tTraining Loss: 11.170223\n",
      "Epoch: 73 \tTraining Loss: 11.167287\n",
      "Epoch: 74 \tTraining Loss: 11.164497\n",
      "Epoch: 75 \tTraining Loss: 11.161841\n",
      "Epoch: 76 \tTraining Loss: 11.159319\n",
      "Epoch: 77 \tTraining Loss: 11.156919\n",
      "Epoch: 78 \tTraining Loss: 11.154637\n",
      "Epoch: 79 \tTraining Loss: 11.152466\n",
      "Epoch: 80 \tTraining Loss: 11.150402\n",
      "Epoch: 81 \tTraining Loss: 11.148436\n",
      "Epoch: 82 \tTraining Loss: 11.146566\n",
      "Epoch: 83 \tTraining Loss: 11.144786\n",
      "Epoch: 84 \tTraining Loss: 11.143093\n",
      "Epoch: 85 \tTraining Loss: 11.141479\n",
      "Epoch: 86 \tTraining Loss: 11.139944\n",
      "Epoch: 87 \tTraining Loss: 11.138483\n",
      "Epoch: 88 \tTraining Loss: 11.137090\n",
      "Epoch: 89 \tTraining Loss: 11.135763\n",
      "Epoch: 90 \tTraining Loss: 11.134501\n",
      "Epoch: 91 \tTraining Loss: 11.133296\n",
      "Epoch: 92 \tTraining Loss: 11.132152\n",
      "Epoch: 93 \tTraining Loss: 11.131060\n",
      "Epoch: 94 \tTraining Loss: 11.130019\n",
      "Epoch: 95 \tTraining Loss: 11.129027\n",
      "Epoch: 96 \tTraining Loss: 11.128081\n",
      "Epoch: 97 \tTraining Loss: 11.127181\n",
      "Epoch: 98 \tTraining Loss: 11.126324\n",
      "Epoch: 99 \tTraining Loss: 11.125506\n",
      "Epoch: 100 \tTraining Loss: 11.124727\n",
      "Epoch: 101 \tTraining Loss: 11.123982\n",
      "Epoch: 102 \tTraining Loss: 11.123277\n",
      "Epoch: 103 \tTraining Loss: 11.122602\n",
      "Epoch: 104 \tTraining Loss: 11.121957\n",
      "Epoch: 105 \tTraining Loss: 11.121343\n",
      "Epoch: 106 \tTraining Loss: 11.120756\n",
      "Epoch: 107 \tTraining Loss: 11.120200\n",
      "Epoch: 108 \tTraining Loss: 11.119667\n",
      "Epoch: 109 \tTraining Loss: 11.119160\n",
      "Epoch: 110 \tTraining Loss: 11.118675\n",
      "Epoch: 111 \tTraining Loss: 11.118217\n",
      "Epoch: 112 \tTraining Loss: 11.117776\n",
      "Epoch: 113 \tTraining Loss: 11.117354\n",
      "Epoch: 114 \tTraining Loss: 11.116956\n",
      "Epoch: 115 \tTraining Loss: 11.116573\n",
      "Epoch: 116 \tTraining Loss: 11.116208\n",
      "Epoch: 117 \tTraining Loss: 11.115862\n",
      "Epoch: 118 \tTraining Loss: 11.115529\n",
      "Epoch: 119 \tTraining Loss: 11.115214\n",
      "Epoch: 120 \tTraining Loss: 11.114911\n",
      "Epoch: 121 \tTraining Loss: 11.114623\n",
      "Epoch: 122 \tTraining Loss: 11.114350\n",
      "Epoch: 123 \tTraining Loss: 11.114089\n",
      "Epoch: 124 \tTraining Loss: 11.113838\n",
      "Epoch: 125 \tTraining Loss: 11.113601\n",
      "Epoch: 126 \tTraining Loss: 11.113374\n",
      "Epoch: 127 \tTraining Loss: 11.113155\n",
      "Epoch: 128 \tTraining Loss: 11.112948\n",
      "Epoch: 129 \tTraining Loss: 11.112752\n",
      "Epoch: 130 \tTraining Loss: 11.112562\n",
      "Epoch: 131 \tTraining Loss: 11.112383\n",
      "Epoch: 132 \tTraining Loss: 11.112211\n",
      "Epoch: 133 \tTraining Loss: 11.112048\n",
      "Epoch: 134 \tTraining Loss: 11.111891\n",
      "Epoch: 135 \tTraining Loss: 11.111743\n",
      "Epoch: 136 \tTraining Loss: 11.111601\n",
      "Epoch: 137 \tTraining Loss: 11.111465\n",
      "Epoch: 138 \tTraining Loss: 11.111335\n",
      "Epoch: 139 \tTraining Loss: 11.111211\n",
      "Epoch: 140 \tTraining Loss: 11.111093\n",
      "Epoch: 141 \tTraining Loss: 11.110980\n",
      "Epoch: 142 \tTraining Loss: 11.110873\n",
      "Epoch: 143 \tTraining Loss: 11.110770\n",
      "Epoch: 144 \tTraining Loss: 11.110674\n",
      "Epoch: 145 \tTraining Loss: 11.110578\n",
      "Epoch: 146 \tTraining Loss: 11.110491\n",
      "Epoch: 147 \tTraining Loss: 11.110405\n",
      "Epoch: 148 \tTraining Loss: 11.110324\n",
      "Epoch: 149 \tTraining Loss: 11.110247\n",
      "Epoch: 150 \tTraining Loss: 11.110172\n",
      "Epoch: 151 \tTraining Loss: 11.110103\n",
      "Epoch: 152 \tTraining Loss: 11.110033\n",
      "Epoch: 153 \tTraining Loss: 11.109971\n",
      "Epoch: 154 \tTraining Loss: 11.109910\n",
      "Epoch: 155 \tTraining Loss: 11.109852\n",
      "Epoch: 156 \tTraining Loss: 11.109796\n",
      "Epoch: 157 \tTraining Loss: 11.109740\n",
      "Epoch: 158 \tTraining Loss: 11.109690\n",
      "Epoch: 159 \tTraining Loss: 11.109643\n",
      "Epoch: 160 \tTraining Loss: 11.109597\n",
      "Epoch: 161 \tTraining Loss: 11.109551\n",
      "Epoch: 162 \tTraining Loss: 11.109509\n",
      "Epoch: 163 \tTraining Loss: 11.109469\n",
      "Epoch: 164 \tTraining Loss: 11.109431\n",
      "Epoch: 165 \tTraining Loss: 11.109393\n",
      "Epoch: 166 \tTraining Loss: 11.109359\n",
      "Epoch: 167 \tTraining Loss: 11.109326\n",
      "Epoch: 168 \tTraining Loss: 11.109293\n",
      "Epoch: 169 \tTraining Loss: 11.109265\n",
      "Epoch: 170 \tTraining Loss: 11.109234\n",
      "Epoch: 171 \tTraining Loss: 11.109206\n",
      "Epoch: 172 \tTraining Loss: 11.109180\n",
      "Epoch: 173 \tTraining Loss: 11.109153\n",
      "Epoch: 174 \tTraining Loss: 11.109132\n",
      "Epoch: 175 \tTraining Loss: 11.109106\n",
      "Epoch: 176 \tTraining Loss: 11.109085\n",
      "Epoch: 177 \tTraining Loss: 11.109065\n",
      "Epoch: 178 \tTraining Loss: 11.109043\n",
      "Epoch: 179 \tTraining Loss: 11.109023\n",
      "Epoch: 180 \tTraining Loss: 11.109006\n",
      "Epoch: 181 \tTraining Loss: 11.108989\n",
      "Epoch: 182 \tTraining Loss: 11.108973\n",
      "Epoch: 183 \tTraining Loss: 11.108955\n",
      "Epoch: 184 \tTraining Loss: 11.108941\n",
      "Epoch: 185 \tTraining Loss: 11.108928\n",
      "Epoch: 186 \tTraining Loss: 11.108913\n",
      "Epoch: 187 \tTraining Loss: 11.108900\n",
      "Epoch: 188 \tTraining Loss: 11.108890\n",
      "Epoch: 189 \tTraining Loss: 11.108875\n",
      "Epoch: 190 \tTraining Loss: 11.108865\n",
      "Epoch: 191 \tTraining Loss: 11.108853\n",
      "Epoch: 192 \tTraining Loss: 11.108842\n",
      "Epoch: 193 \tTraining Loss: 11.108831\n",
      "Epoch: 194 \tTraining Loss: 11.108823\n",
      "Epoch: 195 \tTraining Loss: 11.108813\n",
      "Epoch: 196 \tTraining Loss: 11.108804\n",
      "Epoch: 197 \tTraining Loss: 11.108795\n",
      "Epoch: 198 \tTraining Loss: 11.108788\n",
      "Epoch: 199 \tTraining Loss: 11.108782\n",
      "Epoch: 200 \tTraining Loss: 11.108774\n",
      "Epoch: 201 \tTraining Loss: 11.108768\n",
      "Epoch: 202 \tTraining Loss: 11.108761\n",
      "Epoch: 203 \tTraining Loss: 11.108754\n",
      "Epoch: 204 \tTraining Loss: 11.108747\n",
      "Epoch: 205 \tTraining Loss: 11.108742\n",
      "Epoch: 206 \tTraining Loss: 11.108737\n",
      "Epoch: 207 \tTraining Loss: 11.108730\n",
      "Epoch: 208 \tTraining Loss: 11.108728\n",
      "Epoch: 209 \tTraining Loss: 11.108721\n",
      "Epoch: 210 \tTraining Loss: 11.108716\n",
      "Epoch: 211 \tTraining Loss: 11.108711\n",
      "Epoch: 212 \tTraining Loss: 11.108706\n",
      "Epoch: 213 \tTraining Loss: 11.108704\n",
      "Epoch: 214 \tTraining Loss: 11.108700\n",
      "Epoch: 215 \tTraining Loss: 11.108698\n",
      "Epoch: 216 \tTraining Loss: 11.108694\n",
      "Epoch: 217 \tTraining Loss: 11.108691\n",
      "Epoch: 218 \tTraining Loss: 11.108687\n",
      "Epoch: 219 \tTraining Loss: 11.108685\n",
      "Epoch: 220 \tTraining Loss: 11.108682\n",
      "Epoch: 221 \tTraining Loss: 11.108678\n",
      "Epoch: 222 \tTraining Loss: 11.108674\n",
      "Epoch: 223 \tTraining Loss: 11.108673\n",
      "Epoch: 224 \tTraining Loss: 11.108670\n",
      "Epoch: 225 \tTraining Loss: 11.108669\n",
      "Epoch: 226 \tTraining Loss: 11.108665\n",
      "Epoch: 227 \tTraining Loss: 11.108663\n",
      "Epoch: 228 \tTraining Loss: 11.108661\n",
      "Epoch: 229 \tTraining Loss: 11.108661\n",
      "Epoch: 230 \tTraining Loss: 11.108656\n",
      "Epoch: 231 \tTraining Loss: 11.108655\n",
      "Epoch: 232 \tTraining Loss: 11.108655\n",
      "Epoch: 233 \tTraining Loss: 11.108654\n",
      "Epoch: 234 \tTraining Loss: 11.108652\n",
      "Epoch: 235 \tTraining Loss: 11.108650\n",
      "Epoch: 236 \tTraining Loss: 11.108648\n",
      "Epoch: 237 \tTraining Loss: 11.108647\n",
      "Epoch: 238 \tTraining Loss: 11.108646\n",
      "Epoch: 239 \tTraining Loss: 11.108644\n",
      "Epoch: 240 \tTraining Loss: 11.108643\n",
      "Epoch: 241 \tTraining Loss: 11.108642\n",
      "Epoch: 242 \tTraining Loss: 11.108641\n",
      "Epoch: 243 \tTraining Loss: 11.108642\n",
      "Epoch: 244 \tTraining Loss: 11.108639\n",
      "Epoch: 245 \tTraining Loss: 11.108637\n",
      "Epoch: 246 \tTraining Loss: 11.108636\n",
      "Epoch: 247 \tTraining Loss: 11.108636\n",
      "Epoch: 248 \tTraining Loss: 11.108636\n",
      "Epoch: 249 \tTraining Loss: 11.108635\n",
      "Epoch: 250 \tTraining Loss: 11.108635\n",
      "Epoch: 251 \tTraining Loss: 11.108630\n",
      "Epoch: 252 \tTraining Loss: 11.108631\n",
      "Epoch: 253 \tTraining Loss: 11.108631\n",
      "Epoch: 254 \tTraining Loss: 11.108629\n",
      "Epoch: 255 \tTraining Loss: 11.108627\n",
      "Epoch: 256 \tTraining Loss: 11.108629\n",
      "Epoch: 257 \tTraining Loss: 11.108627\n",
      "Epoch: 258 \tTraining Loss: 11.108628\n",
      "Epoch: 259 \tTraining Loss: 11.108625\n",
      "Epoch: 260 \tTraining Loss: 11.108624\n",
      "Epoch: 261 \tTraining Loss: 11.108624\n",
      "Epoch: 262 \tTraining Loss: 11.108624\n",
      "Epoch: 263 \tTraining Loss: 11.108624\n",
      "Epoch: 264 \tTraining Loss: 11.108623\n",
      "Epoch: 265 \tTraining Loss: 11.108624\n",
      "Epoch: 266 \tTraining Loss: 11.108624\n",
      "Epoch: 267 \tTraining Loss: 11.108623\n",
      "Epoch: 268 \tTraining Loss: 11.108623\n",
      "Epoch: 269 \tTraining Loss: 11.108621\n",
      "Epoch: 270 \tTraining Loss: 11.108621\n",
      "Epoch: 271 \tTraining Loss: 11.108619\n",
      "Epoch: 272 \tTraining Loss: 11.108621\n",
      "Epoch: 273 \tTraining Loss: 11.108620\n",
      "Epoch: 274 \tTraining Loss: 11.108620\n",
      "Epoch: 275 \tTraining Loss: 11.108620\n",
      "Epoch: 276 \tTraining Loss: 11.108618\n",
      "Epoch: 277 \tTraining Loss: 11.108619\n",
      "Epoch: 278 \tTraining Loss: 11.108618\n",
      "Epoch: 279 \tTraining Loss: 11.108618\n",
      "Epoch: 280 \tTraining Loss: 11.108617\n",
      "Epoch: 281 \tTraining Loss: 11.108618\n",
      "Epoch: 282 \tTraining Loss: 11.108617\n",
      "Epoch: 283 \tTraining Loss: 11.108616\n",
      "Epoch: 284 \tTraining Loss: 11.108615\n",
      "Epoch: 285 \tTraining Loss: 11.108616\n",
      "Epoch: 286 \tTraining Loss: 11.108614\n",
      "Epoch: 287 \tTraining Loss: 11.108615\n",
      "Epoch: 288 \tTraining Loss: 11.108615\n",
      "Epoch: 289 \tTraining Loss: 11.108615\n",
      "Epoch: 290 \tTraining Loss: 11.108614\n",
      "Epoch: 291 \tTraining Loss: 11.108615\n",
      "Epoch: 292 \tTraining Loss: 11.108614\n",
      "Epoch: 293 \tTraining Loss: 11.108614\n",
      "Epoch: 294 \tTraining Loss: 11.108614\n",
      "Epoch: 295 \tTraining Loss: 11.108614\n",
      "Epoch: 296 \tTraining Loss: 11.108614\n",
      "Epoch: 297 \tTraining Loss: 11.108613\n",
      "Epoch: 298 \tTraining Loss: 11.108612\n",
      "Epoch: 299 \tTraining Loss: 11.108612\n",
      "Epoch: 300 \tTraining Loss: 11.108612\n",
      "Epoch: 301 \tTraining Loss: 11.108613\n",
      "Epoch: 302 \tTraining Loss: 11.108611\n",
      "Epoch: 303 \tTraining Loss: 11.108611\n",
      "Epoch: 304 \tTraining Loss: 11.108610\n",
      "Epoch: 305 \tTraining Loss: 11.108609\n",
      "Epoch: 306 \tTraining Loss: 11.108610\n",
      "Epoch: 307 \tTraining Loss: 11.108610\n",
      "Epoch: 308 \tTraining Loss: 11.108610\n",
      "Epoch: 309 \tTraining Loss: 11.108610\n",
      "Epoch: 310 \tTraining Loss: 11.108609\n",
      "Epoch: 311 \tTraining Loss: 11.108608\n",
      "Epoch: 312 \tTraining Loss: 11.108608\n",
      "Epoch: 313 \tTraining Loss: 11.108608\n",
      "Epoch: 314 \tTraining Loss: 11.108609\n",
      "Epoch: 315 \tTraining Loss: 11.108609\n",
      "Epoch: 316 \tTraining Loss: 11.108608\n",
      "Epoch: 317 \tTraining Loss: 11.108608\n",
      "Epoch: 318 \tTraining Loss: 11.108607\n",
      "Epoch: 319 \tTraining Loss: 11.108606\n",
      "Epoch: 320 \tTraining Loss: 11.108606\n",
      "Epoch: 321 \tTraining Loss: 11.108606\n",
      "Epoch: 322 \tTraining Loss: 11.108606\n",
      "Epoch: 323 \tTraining Loss: 11.108606\n",
      "Epoch: 324 \tTraining Loss: 11.108607\n",
      "Epoch: 325 \tTraining Loss: 11.108606\n",
      "Epoch: 326 \tTraining Loss: 11.108607\n",
      "Epoch: 327 \tTraining Loss: 11.108606\n",
      "Epoch: 328 \tTraining Loss: 11.108606\n",
      "Epoch: 329 \tTraining Loss: 11.108605\n",
      "Epoch: 330 \tTraining Loss: 11.108606\n",
      "Epoch: 331 \tTraining Loss: 11.108605\n",
      "Epoch: 332 \tTraining Loss: 11.108605\n",
      "Epoch: 333 \tTraining Loss: 11.108604\n",
      "Epoch: 334 \tTraining Loss: 11.108604\n",
      "Epoch: 335 \tTraining Loss: 11.108604\n",
      "Epoch: 336 \tTraining Loss: 11.108604\n",
      "Epoch: 337 \tTraining Loss: 11.108603\n",
      "Epoch: 338 \tTraining Loss: 11.108604\n",
      "Epoch: 339 \tTraining Loss: 11.108603\n",
      "Epoch: 340 \tTraining Loss: 11.108604\n",
      "Epoch: 341 \tTraining Loss: 11.108604\n",
      "Epoch: 342 \tTraining Loss: 11.108603\n",
      "Epoch: 343 \tTraining Loss: 11.108603\n",
      "Epoch: 344 \tTraining Loss: 11.108603\n",
      "Epoch: 345 \tTraining Loss: 11.108603\n",
      "Epoch: 346 \tTraining Loss: 11.108603\n",
      "Epoch: 347 \tTraining Loss: 11.108603\n",
      "Epoch: 348 \tTraining Loss: 11.108602\n",
      "Epoch: 349 \tTraining Loss: 11.108601\n",
      "Epoch: 350 \tTraining Loss: 11.108601\n",
      "Epoch: 351 \tTraining Loss: 11.108601\n",
      "Epoch: 352 \tTraining Loss: 11.108601\n",
      "Epoch: 353 \tTraining Loss: 11.108601\n",
      "Epoch: 354 \tTraining Loss: 11.108601\n",
      "Epoch: 355 \tTraining Loss: 11.108601\n",
      "Epoch: 356 \tTraining Loss: 11.108601\n",
      "Epoch: 357 \tTraining Loss: 11.108601\n",
      "Epoch: 358 \tTraining Loss: 11.108601\n",
      "Epoch: 359 \tTraining Loss: 11.108601\n",
      "Epoch: 360 \tTraining Loss: 11.108600\n",
      "Epoch: 361 \tTraining Loss: 11.108600\n",
      "Epoch: 362 \tTraining Loss: 11.108600\n",
      "Epoch: 363 \tTraining Loss: 11.108600\n",
      "Epoch: 364 \tTraining Loss: 11.108600\n",
      "Epoch: 365 \tTraining Loss: 11.108600\n",
      "Epoch: 366 \tTraining Loss: 11.108598\n",
      "Epoch: 367 \tTraining Loss: 11.108598\n",
      "Epoch: 368 \tTraining Loss: 11.108598\n",
      "Epoch: 369 \tTraining Loss: 11.108598\n",
      "Epoch: 370 \tTraining Loss: 11.108597\n",
      "Epoch: 371 \tTraining Loss: 11.108598\n",
      "Epoch: 372 \tTraining Loss: 11.108597\n",
      "Epoch: 373 \tTraining Loss: 11.108598\n",
      "Epoch: 374 \tTraining Loss: 11.108596\n",
      "Epoch: 375 \tTraining Loss: 11.108597\n",
      "Epoch: 376 \tTraining Loss: 11.108595\n",
      "Epoch: 377 \tTraining Loss: 11.108595\n",
      "Epoch: 378 \tTraining Loss: 11.108595\n",
      "Epoch: 379 \tTraining Loss: 11.108595\n",
      "Epoch: 380 \tTraining Loss: 11.108595\n",
      "Epoch: 381 \tTraining Loss: 11.108595\n",
      "Epoch: 382 \tTraining Loss: 11.108595\n",
      "Epoch: 383 \tTraining Loss: 11.108594\n",
      "Epoch: 384 \tTraining Loss: 11.108593\n",
      "Epoch: 385 \tTraining Loss: 11.108595\n",
      "Epoch: 386 \tTraining Loss: 11.108594\n",
      "Epoch: 387 \tTraining Loss: 11.108594\n",
      "Epoch: 388 \tTraining Loss: 11.108594\n",
      "Epoch: 389 \tTraining Loss: 11.108593\n",
      "Epoch: 390 \tTraining Loss: 11.108592\n",
      "Epoch: 391 \tTraining Loss: 11.108593\n",
      "Epoch: 392 \tTraining Loss: 11.108592\n",
      "Epoch: 393 \tTraining Loss: 11.108591\n",
      "Epoch: 394 \tTraining Loss: 11.108593\n",
      "Epoch: 395 \tTraining Loss: 11.108592\n",
      "Epoch: 396 \tTraining Loss: 11.108591\n",
      "Epoch: 397 \tTraining Loss: 11.108591\n",
      "Epoch: 398 \tTraining Loss: 11.108591\n",
      "Epoch: 399 \tTraining Loss: 11.108591\n",
      "Epoch: 400 \tTraining Loss: 11.108590\n",
      "Epoch: 401 \tTraining Loss: 11.108591\n",
      "Epoch: 402 \tTraining Loss: 11.108591\n",
      "Epoch: 403 \tTraining Loss: 11.108589\n",
      "Epoch: 404 \tTraining Loss: 11.108589\n",
      "Epoch: 405 \tTraining Loss: 11.108589\n",
      "Epoch: 406 \tTraining Loss: 11.108589\n",
      "Epoch: 407 \tTraining Loss: 11.108589\n",
      "Epoch: 408 \tTraining Loss: 11.108589\n",
      "Epoch: 409 \tTraining Loss: 11.108588\n",
      "Epoch: 410 \tTraining Loss: 11.108589\n",
      "Epoch: 411 \tTraining Loss: 11.108589\n",
      "Epoch: 412 \tTraining Loss: 11.108589\n",
      "Epoch: 413 \tTraining Loss: 11.108588\n",
      "Epoch: 414 \tTraining Loss: 11.108588\n",
      "Epoch: 415 \tTraining Loss: 11.108588\n",
      "Epoch: 416 \tTraining Loss: 11.108588\n",
      "Epoch: 417 \tTraining Loss: 11.108587\n",
      "Epoch: 418 \tTraining Loss: 11.108588\n",
      "Epoch: 419 \tTraining Loss: 11.108587\n",
      "Epoch: 420 \tTraining Loss: 11.108586\n",
      "Epoch: 421 \tTraining Loss: 11.108585\n",
      "Epoch: 422 \tTraining Loss: 11.108585\n",
      "Epoch: 423 \tTraining Loss: 11.108586\n",
      "Epoch: 424 \tTraining Loss: 11.108586\n",
      "Epoch: 425 \tTraining Loss: 11.108586\n",
      "Epoch: 426 \tTraining Loss: 11.108586\n",
      "Epoch: 427 \tTraining Loss: 11.108585\n",
      "Epoch: 428 \tTraining Loss: 11.108585\n",
      "Epoch: 429 \tTraining Loss: 11.108585\n",
      "Epoch: 430 \tTraining Loss: 11.108585\n",
      "Epoch: 431 \tTraining Loss: 11.108584\n",
      "Epoch: 432 \tTraining Loss: 11.108584\n",
      "Epoch: 433 \tTraining Loss: 11.108583\n",
      "Epoch: 434 \tTraining Loss: 11.108584\n",
      "Epoch: 435 \tTraining Loss: 11.108584\n",
      "Epoch: 436 \tTraining Loss: 11.108584\n",
      "Epoch: 437 \tTraining Loss: 11.108583\n",
      "Epoch: 438 \tTraining Loss: 11.108584\n",
      "Epoch: 439 \tTraining Loss: 11.108583\n",
      "Epoch: 440 \tTraining Loss: 11.108583\n",
      "Epoch: 441 \tTraining Loss: 11.108583\n",
      "Epoch: 442 \tTraining Loss: 11.108583\n",
      "Epoch: 443 \tTraining Loss: 11.108583\n",
      "Epoch: 444 \tTraining Loss: 11.108582\n",
      "Epoch: 445 \tTraining Loss: 11.108582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446 \tTraining Loss: 11.108582\n",
      "Epoch: 447 \tTraining Loss: 11.108582\n",
      "Epoch: 448 \tTraining Loss: 11.108582\n",
      "Epoch: 449 \tTraining Loss: 11.108582\n",
      "Epoch: 450 \tTraining Loss: 11.108582\n",
      "Epoch: 451 \tTraining Loss: 11.108581\n",
      "Epoch: 452 \tTraining Loss: 11.108581\n",
      "Epoch: 453 \tTraining Loss: 11.108581\n",
      "Epoch: 454 \tTraining Loss: 11.108580\n",
      "Epoch: 455 \tTraining Loss: 11.108582\n",
      "Epoch: 456 \tTraining Loss: 11.108580\n",
      "Epoch: 457 \tTraining Loss: 11.108580\n",
      "Epoch: 458 \tTraining Loss: 11.108578\n",
      "Epoch: 459 \tTraining Loss: 11.108580\n",
      "Epoch: 460 \tTraining Loss: 11.108578\n",
      "Epoch: 461 \tTraining Loss: 11.108578\n",
      "Epoch: 462 \tTraining Loss: 11.108578\n",
      "Epoch: 463 \tTraining Loss: 11.108578\n",
      "Epoch: 464 \tTraining Loss: 11.108578\n",
      "Epoch: 465 \tTraining Loss: 11.108577\n",
      "Epoch: 466 \tTraining Loss: 11.108577\n",
      "Epoch: 467 \tTraining Loss: 11.108578\n",
      "Epoch: 468 \tTraining Loss: 11.108577\n",
      "Epoch: 469 \tTraining Loss: 11.108576\n",
      "Epoch: 470 \tTraining Loss: 11.108576\n",
      "Epoch: 471 \tTraining Loss: 11.108576\n",
      "Epoch: 472 \tTraining Loss: 11.108576\n",
      "Epoch: 473 \tTraining Loss: 11.108576\n",
      "Epoch: 474 \tTraining Loss: 11.108576\n",
      "Epoch: 475 \tTraining Loss: 11.108576\n",
      "Epoch: 476 \tTraining Loss: 11.108575\n",
      "Epoch: 477 \tTraining Loss: 11.108575\n",
      "Epoch: 478 \tTraining Loss: 11.108575\n",
      "Epoch: 479 \tTraining Loss: 11.108574\n",
      "Epoch: 480 \tTraining Loss: 11.108574\n",
      "Epoch: 481 \tTraining Loss: 11.108575\n",
      "Epoch: 482 \tTraining Loss: 11.108574\n",
      "Epoch: 483 \tTraining Loss: 11.108574\n",
      "Epoch: 484 \tTraining Loss: 11.108574\n",
      "Epoch: 485 \tTraining Loss: 11.108573\n",
      "Epoch: 486 \tTraining Loss: 11.108573\n",
      "Epoch: 487 \tTraining Loss: 11.108572\n",
      "Epoch: 488 \tTraining Loss: 11.108572\n",
      "Epoch: 489 \tTraining Loss: 11.108572\n",
      "Epoch: 490 \tTraining Loss: 11.108572\n",
      "Epoch: 491 \tTraining Loss: 11.108572\n",
      "Epoch: 492 \tTraining Loss: 11.108572\n",
      "Epoch: 493 \tTraining Loss: 11.108572\n",
      "Epoch: 494 \tTraining Loss: 11.108570\n",
      "Epoch: 495 \tTraining Loss: 11.108571\n",
      "Epoch: 496 \tTraining Loss: 11.108570\n",
      "Epoch: 497 \tTraining Loss: 11.108570\n",
      "Epoch: 498 \tTraining Loss: 11.108571\n",
      "Epoch: 499 \tTraining Loss: 11.108569\n",
      "Epoch: 500 \tTraining Loss: 11.108569\n",
      "Epoch: 501 \tTraining Loss: 11.108569\n",
      "Epoch: 502 \tTraining Loss: 11.108569\n",
      "Epoch: 503 \tTraining Loss: 11.108570\n",
      "Epoch: 504 \tTraining Loss: 11.108569\n",
      "Epoch: 505 \tTraining Loss: 11.108569\n",
      "Epoch: 506 \tTraining Loss: 11.108569\n",
      "Epoch: 507 \tTraining Loss: 11.108568\n",
      "Epoch: 508 \tTraining Loss: 11.108568\n",
      "Epoch: 509 \tTraining Loss: 11.108568\n",
      "Epoch: 510 \tTraining Loss: 11.108568\n",
      "Epoch: 511 \tTraining Loss: 11.108567\n",
      "Epoch: 512 \tTraining Loss: 11.108568\n",
      "Epoch: 513 \tTraining Loss: 11.108566\n",
      "Epoch: 514 \tTraining Loss: 11.108566\n",
      "Epoch: 515 \tTraining Loss: 11.108567\n",
      "Epoch: 516 \tTraining Loss: 11.108567\n",
      "Epoch: 517 \tTraining Loss: 11.108566\n",
      "Epoch: 518 \tTraining Loss: 11.108567\n",
      "Epoch: 519 \tTraining Loss: 11.108567\n",
      "Epoch: 520 \tTraining Loss: 11.108566\n",
      "Epoch: 521 \tTraining Loss: 11.108566\n",
      "Epoch: 522 \tTraining Loss: 11.108565\n",
      "Epoch: 523 \tTraining Loss: 11.108565\n",
      "Epoch: 524 \tTraining Loss: 11.108565\n",
      "Epoch: 525 \tTraining Loss: 11.108565\n",
      "Epoch: 526 \tTraining Loss: 11.108565\n",
      "Epoch: 527 \tTraining Loss: 11.108565\n",
      "Epoch: 528 \tTraining Loss: 11.108565\n",
      "Epoch: 529 \tTraining Loss: 11.108565\n",
      "Epoch: 530 \tTraining Loss: 11.108565\n",
      "Epoch: 531 \tTraining Loss: 11.108565\n",
      "Epoch: 532 \tTraining Loss: 11.108565\n",
      "Epoch: 533 \tTraining Loss: 11.108565\n",
      "Epoch: 534 \tTraining Loss: 11.108565\n",
      "Epoch: 535 \tTraining Loss: 11.108565\n",
      "Epoch: 536 \tTraining Loss: 11.108562\n",
      "Epoch: 537 \tTraining Loss: 11.108563\n",
      "Epoch: 538 \tTraining Loss: 11.108564\n",
      "Epoch: 539 \tTraining Loss: 11.108562\n",
      "Epoch: 540 \tTraining Loss: 11.108562\n",
      "Epoch: 541 \tTraining Loss: 11.108562\n",
      "Epoch: 542 \tTraining Loss: 11.108562\n",
      "Epoch: 543 \tTraining Loss: 11.108562\n",
      "Epoch: 544 \tTraining Loss: 11.108561\n",
      "Epoch: 545 \tTraining Loss: 11.108562\n",
      "Epoch: 546 \tTraining Loss: 11.108561\n",
      "Epoch: 547 \tTraining Loss: 11.108562\n",
      "Epoch: 548 \tTraining Loss: 11.108562\n",
      "Epoch: 549 \tTraining Loss: 11.108562\n",
      "Epoch: 550 \tTraining Loss: 11.108562\n",
      "Epoch: 551 \tTraining Loss: 11.108562\n",
      "Epoch: 552 \tTraining Loss: 11.108560\n",
      "Epoch: 553 \tTraining Loss: 11.108561\n",
      "Epoch: 554 \tTraining Loss: 11.108561\n",
      "Epoch: 555 \tTraining Loss: 11.108560\n",
      "Epoch: 556 \tTraining Loss: 11.108561\n",
      "Epoch: 557 \tTraining Loss: 11.108559\n",
      "Epoch: 558 \tTraining Loss: 11.108559\n",
      "Epoch: 559 \tTraining Loss: 11.108559\n",
      "Epoch: 560 \tTraining Loss: 11.108560\n",
      "Epoch: 561 \tTraining Loss: 11.108559\n",
      "Epoch: 562 \tTraining Loss: 11.108559\n",
      "Epoch: 563 \tTraining Loss: 11.108557\n",
      "Epoch: 564 \tTraining Loss: 11.108558\n",
      "Epoch: 565 \tTraining Loss: 11.108559\n",
      "Epoch: 566 \tTraining Loss: 11.108559\n",
      "Epoch: 567 \tTraining Loss: 11.108559\n",
      "Epoch: 568 \tTraining Loss: 11.108557\n",
      "Epoch: 569 \tTraining Loss: 11.108557\n",
      "Epoch: 570 \tTraining Loss: 11.108557\n",
      "Epoch: 571 \tTraining Loss: 11.108557\n",
      "Epoch: 572 \tTraining Loss: 11.108558\n",
      "Epoch: 573 \tTraining Loss: 11.108558\n",
      "Epoch: 574 \tTraining Loss: 11.108557\n",
      "Epoch: 575 \tTraining Loss: 11.108556\n",
      "Epoch: 576 \tTraining Loss: 11.108557\n",
      "Epoch: 577 \tTraining Loss: 11.108556\n",
      "Epoch: 578 \tTraining Loss: 11.108554\n",
      "Epoch: 579 \tTraining Loss: 11.108555\n",
      "Epoch: 580 \tTraining Loss: 11.108555\n",
      "Epoch: 581 \tTraining Loss: 11.108555\n",
      "Epoch: 582 \tTraining Loss: 11.108554\n",
      "Epoch: 583 \tTraining Loss: 11.108555\n",
      "Epoch: 584 \tTraining Loss: 11.108554\n",
      "Epoch: 585 \tTraining Loss: 11.108555\n",
      "Epoch: 586 \tTraining Loss: 11.108555\n",
      "Epoch: 587 \tTraining Loss: 11.108554\n",
      "Epoch: 588 \tTraining Loss: 11.108554\n",
      "Epoch: 589 \tTraining Loss: 11.108554\n",
      "Epoch: 590 \tTraining Loss: 11.108554\n",
      "Epoch: 591 \tTraining Loss: 11.108554\n",
      "Epoch: 592 \tTraining Loss: 11.108553\n",
      "Epoch: 593 \tTraining Loss: 11.108553\n",
      "Epoch: 594 \tTraining Loss: 11.108553\n",
      "Epoch: 595 \tTraining Loss: 11.108553\n",
      "Epoch: 596 \tTraining Loss: 11.108553\n",
      "Epoch: 597 \tTraining Loss: 11.108552\n",
      "Epoch: 598 \tTraining Loss: 11.108552\n",
      "Epoch: 599 \tTraining Loss: 11.108551\n",
      "Epoch: 600 \tTraining Loss: 11.108552\n",
      "Epoch: 601 \tTraining Loss: 11.108552\n",
      "Epoch: 602 \tTraining Loss: 11.108551\n",
      "Epoch: 603 \tTraining Loss: 11.108551\n",
      "Epoch: 604 \tTraining Loss: 11.108552\n",
      "Epoch: 605 \tTraining Loss: 11.108549\n",
      "Epoch: 606 \tTraining Loss: 11.108549\n",
      "Epoch: 607 \tTraining Loss: 11.108549\n",
      "Epoch: 608 \tTraining Loss: 11.108549\n",
      "Epoch: 609 \tTraining Loss: 11.108548\n",
      "Epoch: 610 \tTraining Loss: 11.108548\n",
      "Epoch: 611 \tTraining Loss: 11.108548\n",
      "Epoch: 612 \tTraining Loss: 11.108548\n",
      "Epoch: 613 \tTraining Loss: 11.108548\n",
      "Epoch: 614 \tTraining Loss: 11.108548\n",
      "Epoch: 615 \tTraining Loss: 11.108547\n",
      "Epoch: 616 \tTraining Loss: 11.108548\n",
      "Epoch: 617 \tTraining Loss: 11.108547\n",
      "Epoch: 618 \tTraining Loss: 11.108547\n",
      "Epoch: 619 \tTraining Loss: 11.108547\n",
      "Epoch: 620 \tTraining Loss: 11.108547\n",
      "Epoch: 621 \tTraining Loss: 11.108546\n",
      "Epoch: 622 \tTraining Loss: 11.108545\n",
      "Epoch: 623 \tTraining Loss: 11.108544\n",
      "Epoch: 624 \tTraining Loss: 11.108544\n",
      "Epoch: 625 \tTraining Loss: 11.108544\n",
      "Epoch: 626 \tTraining Loss: 11.108544\n",
      "Epoch: 627 \tTraining Loss: 11.108544\n",
      "Epoch: 628 \tTraining Loss: 11.108544\n",
      "Epoch: 629 \tTraining Loss: 11.108544\n",
      "Epoch: 630 \tTraining Loss: 11.108544\n",
      "Epoch: 631 \tTraining Loss: 11.108545\n",
      "Epoch: 632 \tTraining Loss: 11.108544\n",
      "Epoch: 633 \tTraining Loss: 11.108543\n",
      "Epoch: 634 \tTraining Loss: 11.108543\n",
      "Epoch: 635 \tTraining Loss: 11.108543\n",
      "Epoch: 636 \tTraining Loss: 11.108543\n",
      "Epoch: 637 \tTraining Loss: 11.108543\n",
      "Epoch: 638 \tTraining Loss: 11.108543\n",
      "Epoch: 639 \tTraining Loss: 11.108543\n",
      "Epoch: 640 \tTraining Loss: 11.108543\n",
      "Epoch: 641 \tTraining Loss: 11.108543\n",
      "Epoch: 642 \tTraining Loss: 11.108542\n",
      "Epoch: 643 \tTraining Loss: 11.108542\n",
      "Epoch: 644 \tTraining Loss: 11.108541\n",
      "Epoch: 645 \tTraining Loss: 11.108541\n",
      "Epoch: 646 \tTraining Loss: 11.108541\n",
      "Epoch: 647 \tTraining Loss: 11.108541\n",
      "Epoch: 648 \tTraining Loss: 11.108541\n",
      "Epoch: 649 \tTraining Loss: 11.108541\n",
      "Epoch: 650 \tTraining Loss: 11.108541\n",
      "Epoch: 651 \tTraining Loss: 11.108541\n",
      "Epoch: 652 \tTraining Loss: 11.108541\n",
      "Epoch: 653 \tTraining Loss: 11.108541\n",
      "Epoch: 654 \tTraining Loss: 11.108541\n",
      "Epoch: 655 \tTraining Loss: 11.108541\n",
      "Epoch: 656 \tTraining Loss: 11.108541\n",
      "Epoch: 657 \tTraining Loss: 11.108541\n",
      "Epoch: 658 \tTraining Loss: 11.108539\n",
      "Epoch: 659 \tTraining Loss: 11.108539\n",
      "Epoch: 660 \tTraining Loss: 11.108539\n",
      "Epoch: 661 \tTraining Loss: 11.108539\n",
      "Epoch: 662 \tTraining Loss: 11.108539\n",
      "Epoch: 663 \tTraining Loss: 11.108539\n",
      "Epoch: 664 \tTraining Loss: 11.108538\n",
      "Epoch: 665 \tTraining Loss: 11.108537\n",
      "Epoch: 666 \tTraining Loss: 11.108537\n",
      "Epoch: 667 \tTraining Loss: 11.108537\n",
      "Epoch: 668 \tTraining Loss: 11.108537\n",
      "Epoch: 669 \tTraining Loss: 11.108537\n",
      "Epoch: 670 \tTraining Loss: 11.108537\n",
      "Epoch: 671 \tTraining Loss: 11.108537\n",
      "Epoch: 672 \tTraining Loss: 11.108536\n",
      "Epoch: 673 \tTraining Loss: 11.108536\n",
      "Epoch: 674 \tTraining Loss: 11.108536\n",
      "Epoch: 675 \tTraining Loss: 11.108536\n",
      "Epoch: 676 \tTraining Loss: 11.108537\n",
      "Epoch: 677 \tTraining Loss: 11.108535\n",
      "Epoch: 678 \tTraining Loss: 11.108536\n",
      "Epoch: 679 \tTraining Loss: 11.108535\n",
      "Epoch: 680 \tTraining Loss: 11.108534\n",
      "Epoch: 681 \tTraining Loss: 11.108535\n",
      "Epoch: 682 \tTraining Loss: 11.108534\n",
      "Epoch: 683 \tTraining Loss: 11.108534\n",
      "Epoch: 684 \tTraining Loss: 11.108534\n",
      "Epoch: 685 \tTraining Loss: 11.108534\n",
      "Epoch: 686 \tTraining Loss: 11.108533\n",
      "Epoch: 687 \tTraining Loss: 11.108533\n",
      "Epoch: 688 \tTraining Loss: 11.108533\n",
      "Epoch: 689 \tTraining Loss: 11.108532\n",
      "Epoch: 690 \tTraining Loss: 11.108531\n",
      "Epoch: 691 \tTraining Loss: 11.108531\n",
      "Epoch: 692 \tTraining Loss: 11.108531\n",
      "Epoch: 693 \tTraining Loss: 11.108531\n",
      "Epoch: 694 \tTraining Loss: 11.108531\n",
      "Epoch: 695 \tTraining Loss: 11.108531\n",
      "Epoch: 696 \tTraining Loss: 11.108531\n",
      "Epoch: 697 \tTraining Loss: 11.108531\n",
      "Epoch: 698 \tTraining Loss: 11.108531\n",
      "Epoch: 699 \tTraining Loss: 11.108531\n",
      "Epoch: 700 \tTraining Loss: 11.108531\n",
      "Epoch: 701 \tTraining Loss: 11.108531\n",
      "Epoch: 702 \tTraining Loss: 11.108531\n",
      "Epoch: 703 \tTraining Loss: 11.108531\n",
      "Epoch: 704 \tTraining Loss: 11.108531\n",
      "Epoch: 705 \tTraining Loss: 11.108530\n",
      "Epoch: 706 \tTraining Loss: 11.108530\n",
      "Epoch: 707 \tTraining Loss: 11.108530\n",
      "Epoch: 708 \tTraining Loss: 11.108531\n",
      "Epoch: 709 \tTraining Loss: 11.108530\n",
      "Epoch: 710 \tTraining Loss: 11.108529\n",
      "Epoch: 711 \tTraining Loss: 11.108529\n",
      "Epoch: 712 \tTraining Loss: 11.108529\n",
      "Epoch: 713 \tTraining Loss: 11.108528\n",
      "Epoch: 714 \tTraining Loss: 11.108528\n",
      "Epoch: 715 \tTraining Loss: 11.108527\n",
      "Epoch: 716 \tTraining Loss: 11.108527\n",
      "Epoch: 717 \tTraining Loss: 11.108527\n",
      "Epoch: 718 \tTraining Loss: 11.108525\n",
      "Epoch: 719 \tTraining Loss: 11.108525\n",
      "Epoch: 720 \tTraining Loss: 11.108526\n",
      "Epoch: 721 \tTraining Loss: 11.108526\n",
      "Epoch: 722 \tTraining Loss: 11.108526\n",
      "Epoch: 723 \tTraining Loss: 11.108526\n",
      "Epoch: 724 \tTraining Loss: 11.108525\n",
      "Epoch: 725 \tTraining Loss: 11.108524\n",
      "Epoch: 726 \tTraining Loss: 11.108524\n",
      "Epoch: 727 \tTraining Loss: 11.108524\n",
      "Epoch: 728 \tTraining Loss: 11.108524\n",
      "Epoch: 729 \tTraining Loss: 11.108524\n",
      "Epoch: 730 \tTraining Loss: 11.108524\n",
      "Epoch: 731 \tTraining Loss: 11.108523\n",
      "Epoch: 732 \tTraining Loss: 11.108524\n",
      "Epoch: 733 \tTraining Loss: 11.108524\n",
      "Epoch: 734 \tTraining Loss: 11.108522\n",
      "Epoch: 735 \tTraining Loss: 11.108522\n",
      "Epoch: 736 \tTraining Loss: 11.108521\n",
      "Epoch: 737 \tTraining Loss: 11.108521\n",
      "Epoch: 738 \tTraining Loss: 11.108522\n",
      "Epoch: 739 \tTraining Loss: 11.108522\n",
      "Epoch: 740 \tTraining Loss: 11.108522\n",
      "Epoch: 741 \tTraining Loss: 11.108521\n",
      "Epoch: 742 \tTraining Loss: 11.108521\n",
      "Epoch: 743 \tTraining Loss: 11.108521\n",
      "Epoch: 744 \tTraining Loss: 11.108521\n",
      "Epoch: 745 \tTraining Loss: 11.108521\n",
      "Epoch: 746 \tTraining Loss: 11.108521\n",
      "Epoch: 747 \tTraining Loss: 11.108521\n",
      "Epoch: 748 \tTraining Loss: 11.108521\n",
      "Epoch: 749 \tTraining Loss: 11.108520\n",
      "Epoch: 750 \tTraining Loss: 11.108521\n",
      "Epoch: 751 \tTraining Loss: 11.108520\n",
      "Epoch: 752 \tTraining Loss: 11.108520\n",
      "Epoch: 753 \tTraining Loss: 11.108519\n",
      "Epoch: 754 \tTraining Loss: 11.108519\n",
      "Epoch: 755 \tTraining Loss: 11.108519\n",
      "Epoch: 756 \tTraining Loss: 11.108519\n",
      "Epoch: 757 \tTraining Loss: 11.108518\n",
      "Epoch: 758 \tTraining Loss: 11.108518\n",
      "Epoch: 759 \tTraining Loss: 11.108518\n",
      "Epoch: 760 \tTraining Loss: 11.108518\n",
      "Epoch: 761 \tTraining Loss: 11.108517\n",
      "Epoch: 762 \tTraining Loss: 11.108517\n",
      "Epoch: 763 \tTraining Loss: 11.108518\n",
      "Epoch: 764 \tTraining Loss: 11.108518\n",
      "Epoch: 765 \tTraining Loss: 11.108517\n",
      "Epoch: 766 \tTraining Loss: 11.108517\n",
      "Epoch: 767 \tTraining Loss: 11.108517\n",
      "Epoch: 768 \tTraining Loss: 11.108517\n",
      "Epoch: 769 \tTraining Loss: 11.108515\n",
      "Epoch: 770 \tTraining Loss: 11.108517\n",
      "Epoch: 771 \tTraining Loss: 11.108515\n",
      "Epoch: 772 \tTraining Loss: 11.108515\n",
      "Epoch: 773 \tTraining Loss: 11.108515\n",
      "Epoch: 774 \tTraining Loss: 11.108514\n",
      "Epoch: 775 \tTraining Loss: 11.108514\n",
      "Epoch: 776 \tTraining Loss: 11.108513\n",
      "Epoch: 777 \tTraining Loss: 11.108513\n",
      "Epoch: 778 \tTraining Loss: 11.108513\n",
      "Epoch: 779 \tTraining Loss: 11.108513\n",
      "Epoch: 780 \tTraining Loss: 11.108513\n",
      "Epoch: 781 \tTraining Loss: 11.108513\n",
      "Epoch: 782 \tTraining Loss: 11.108513\n",
      "Epoch: 783 \tTraining Loss: 11.108512\n",
      "Epoch: 784 \tTraining Loss: 11.108512\n",
      "Epoch: 785 \tTraining Loss: 11.108512\n",
      "Epoch: 786 \tTraining Loss: 11.108511\n",
      "Epoch: 787 \tTraining Loss: 11.108512\n",
      "Epoch: 788 \tTraining Loss: 11.108512\n",
      "Epoch: 789 \tTraining Loss: 11.108511\n",
      "Epoch: 790 \tTraining Loss: 11.108511\n",
      "Epoch: 791 \tTraining Loss: 11.108510\n",
      "Epoch: 792 \tTraining Loss: 11.108510\n",
      "Epoch: 793 \tTraining Loss: 11.108510\n",
      "Epoch: 794 \tTraining Loss: 11.108509\n",
      "Epoch: 795 \tTraining Loss: 11.108510\n",
      "Epoch: 796 \tTraining Loss: 11.108509\n",
      "Epoch: 797 \tTraining Loss: 11.108509\n",
      "Epoch: 798 \tTraining Loss: 11.108509\n",
      "Epoch: 799 \tTraining Loss: 11.108509\n",
      "Epoch: 800 \tTraining Loss: 11.108509\n",
      "Epoch: 801 \tTraining Loss: 11.108507\n",
      "Epoch: 802 \tTraining Loss: 11.108509\n",
      "Epoch: 803 \tTraining Loss: 11.108508\n",
      "Epoch: 804 \tTraining Loss: 11.108508\n",
      "Epoch: 805 \tTraining Loss: 11.108507\n",
      "Epoch: 806 \tTraining Loss: 11.108507\n",
      "Epoch: 807 \tTraining Loss: 11.108507\n",
      "Epoch: 808 \tTraining Loss: 11.108507\n",
      "Epoch: 809 \tTraining Loss: 11.108507\n",
      "Epoch: 810 \tTraining Loss: 11.108507\n",
      "Epoch: 811 \tTraining Loss: 11.108507\n",
      "Epoch: 812 \tTraining Loss: 11.108506\n",
      "Epoch: 813 \tTraining Loss: 11.108506\n",
      "Epoch: 814 \tTraining Loss: 11.108506\n",
      "Epoch: 815 \tTraining Loss: 11.108507\n",
      "Epoch: 816 \tTraining Loss: 11.108506\n",
      "Epoch: 817 \tTraining Loss: 11.108506\n",
      "Epoch: 818 \tTraining Loss: 11.108506\n",
      "Epoch: 819 \tTraining Loss: 11.108505\n",
      "Epoch: 820 \tTraining Loss: 11.108504\n",
      "Epoch: 821 \tTraining Loss: 11.108504\n",
      "Epoch: 822 \tTraining Loss: 11.108504\n",
      "Epoch: 823 \tTraining Loss: 11.108504\n",
      "Epoch: 824 \tTraining Loss: 11.108504\n",
      "Epoch: 825 \tTraining Loss: 11.108504\n",
      "Epoch: 826 \tTraining Loss: 11.108504\n",
      "Epoch: 827 \tTraining Loss: 11.108502\n",
      "Epoch: 828 \tTraining Loss: 11.108502\n",
      "Epoch: 829 \tTraining Loss: 11.108502\n",
      "Epoch: 830 \tTraining Loss: 11.108502\n",
      "Epoch: 831 \tTraining Loss: 11.108502\n",
      "Epoch: 832 \tTraining Loss: 11.108501\n",
      "Epoch: 833 \tTraining Loss: 11.108502\n",
      "Epoch: 834 \tTraining Loss: 11.108501\n",
      "Epoch: 835 \tTraining Loss: 11.108501\n",
      "Epoch: 836 \tTraining Loss: 11.108501\n",
      "Epoch: 837 \tTraining Loss: 11.108501\n",
      "Epoch: 838 \tTraining Loss: 11.108501\n",
      "Epoch: 839 \tTraining Loss: 11.108501\n",
      "Epoch: 840 \tTraining Loss: 11.108501\n",
      "Epoch: 841 \tTraining Loss: 11.108500\n",
      "Epoch: 842 \tTraining Loss: 11.108500\n",
      "Epoch: 843 \tTraining Loss: 11.108500\n",
      "Epoch: 844 \tTraining Loss: 11.108501\n",
      "Epoch: 845 \tTraining Loss: 11.108500\n",
      "Epoch: 846 \tTraining Loss: 11.108500\n",
      "Epoch: 847 \tTraining Loss: 11.108500\n",
      "Epoch: 848 \tTraining Loss: 11.108500\n",
      "Epoch: 849 \tTraining Loss: 11.108499\n",
      "Epoch: 850 \tTraining Loss: 11.108499\n",
      "Epoch: 851 \tTraining Loss: 11.108499\n",
      "Epoch: 852 \tTraining Loss: 11.108499\n",
      "Epoch: 853 \tTraining Loss: 11.108499\n",
      "Epoch: 854 \tTraining Loss: 11.108499\n",
      "Epoch: 855 \tTraining Loss: 11.108499\n",
      "Epoch: 856 \tTraining Loss: 11.108499\n",
      "Epoch: 857 \tTraining Loss: 11.108498\n",
      "Epoch: 858 \tTraining Loss: 11.108499\n",
      "Epoch: 859 \tTraining Loss: 11.108500\n",
      "Epoch: 860 \tTraining Loss: 11.108498\n",
      "Epoch: 861 \tTraining Loss: 11.108497\n",
      "Epoch: 862 \tTraining Loss: 11.108498\n",
      "Epoch: 863 \tTraining Loss: 11.108498\n",
      "Epoch: 864 \tTraining Loss: 11.108498\n",
      "Epoch: 865 \tTraining Loss: 11.108498\n",
      "Epoch: 866 \tTraining Loss: 11.108496\n",
      "Epoch: 867 \tTraining Loss: 11.108496\n",
      "Epoch: 868 \tTraining Loss: 11.108496\n",
      "Epoch: 869 \tTraining Loss: 11.108496\n",
      "Epoch: 870 \tTraining Loss: 11.108496\n",
      "Epoch: 871 \tTraining Loss: 11.108496\n",
      "Epoch: 872 \tTraining Loss: 11.108496\n",
      "Epoch: 873 \tTraining Loss: 11.108496\n",
      "Epoch: 874 \tTraining Loss: 11.108495\n",
      "Epoch: 875 \tTraining Loss: 11.108496\n",
      "Epoch: 876 \tTraining Loss: 11.108495\n",
      "Epoch: 877 \tTraining Loss: 11.108495\n",
      "Epoch: 878 \tTraining Loss: 11.108494\n",
      "Epoch: 879 \tTraining Loss: 11.108495\n",
      "Epoch: 880 \tTraining Loss: 11.108495\n",
      "Epoch: 881 \tTraining Loss: 11.108495\n",
      "Epoch: 882 \tTraining Loss: 11.108493\n",
      "Epoch: 883 \tTraining Loss: 11.108493\n",
      "Epoch: 884 \tTraining Loss: 11.108493\n",
      "Epoch: 885 \tTraining Loss: 11.108493\n",
      "Epoch: 886 \tTraining Loss: 11.108491\n",
      "Epoch: 887 \tTraining Loss: 11.108491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888 \tTraining Loss: 11.108492\n",
      "Epoch: 889 \tTraining Loss: 11.108491\n",
      "Epoch: 890 \tTraining Loss: 11.108491\n",
      "Epoch: 891 \tTraining Loss: 11.108491\n",
      "Epoch: 892 \tTraining Loss: 11.108490\n",
      "Epoch: 893 \tTraining Loss: 11.108490\n",
      "Epoch: 894 \tTraining Loss: 11.108490\n",
      "Epoch: 895 \tTraining Loss: 11.108490\n",
      "Epoch: 896 \tTraining Loss: 11.108489\n",
      "Epoch: 897 \tTraining Loss: 11.108489\n",
      "Epoch: 898 \tTraining Loss: 11.108490\n",
      "Epoch: 899 \tTraining Loss: 11.108490\n",
      "Epoch: 900 \tTraining Loss: 11.108489\n",
      "Epoch: 901 \tTraining Loss: 11.108488\n",
      "Epoch: 902 \tTraining Loss: 11.108489\n",
      "Epoch: 903 \tTraining Loss: 11.108488\n",
      "Epoch: 904 \tTraining Loss: 11.108488\n",
      "Epoch: 905 \tTraining Loss: 11.108488\n",
      "Epoch: 906 \tTraining Loss: 11.108488\n",
      "Epoch: 907 \tTraining Loss: 11.108486\n",
      "Epoch: 908 \tTraining Loss: 11.108486\n",
      "Epoch: 909 \tTraining Loss: 11.108487\n",
      "Epoch: 910 \tTraining Loss: 11.108486\n",
      "Epoch: 911 \tTraining Loss: 11.108486\n",
      "Epoch: 912 \tTraining Loss: 11.108486\n",
      "Epoch: 913 \tTraining Loss: 11.108486\n",
      "Epoch: 914 \tTraining Loss: 11.108484\n",
      "Epoch: 915 \tTraining Loss: 11.108485\n",
      "Epoch: 916 \tTraining Loss: 11.108484\n",
      "Epoch: 917 \tTraining Loss: 11.108485\n",
      "Epoch: 918 \tTraining Loss: 11.108485\n",
      "Epoch: 919 \tTraining Loss: 11.108483\n",
      "Epoch: 920 \tTraining Loss: 11.108483\n",
      "Epoch: 921 \tTraining Loss: 11.108483\n",
      "Epoch: 922 \tTraining Loss: 11.108483\n",
      "Epoch: 923 \tTraining Loss: 11.108483\n",
      "Epoch: 924 \tTraining Loss: 11.108483\n",
      "Epoch: 925 \tTraining Loss: 11.108483\n",
      "Epoch: 926 \tTraining Loss: 11.108482\n",
      "Epoch: 927 \tTraining Loss: 11.108482\n",
      "Epoch: 928 \tTraining Loss: 11.108482\n",
      "Epoch: 929 \tTraining Loss: 11.108483\n",
      "Epoch: 930 \tTraining Loss: 11.108482\n",
      "Epoch: 931 \tTraining Loss: 11.108482\n",
      "Epoch: 932 \tTraining Loss: 11.108482\n",
      "Epoch: 933 \tTraining Loss: 11.108482\n",
      "Epoch: 934 \tTraining Loss: 11.108482\n",
      "Epoch: 935 \tTraining Loss: 11.108482\n",
      "Epoch: 936 \tTraining Loss: 11.108481\n",
      "Epoch: 937 \tTraining Loss: 11.108481\n",
      "Epoch: 938 \tTraining Loss: 11.108481\n",
      "Epoch: 939 \tTraining Loss: 11.108481\n",
      "Epoch: 940 \tTraining Loss: 11.108479\n",
      "Epoch: 941 \tTraining Loss: 11.108479\n",
      "Epoch: 942 \tTraining Loss: 11.108479\n",
      "Epoch: 943 \tTraining Loss: 11.108479\n",
      "Epoch: 944 \tTraining Loss: 11.108479\n",
      "Epoch: 945 \tTraining Loss: 11.108479\n",
      "Epoch: 946 \tTraining Loss: 11.108479\n",
      "Epoch: 947 \tTraining Loss: 11.108480\n",
      "Epoch: 948 \tTraining Loss: 11.108478\n",
      "Epoch: 949 \tTraining Loss: 11.108479\n",
      "Epoch: 950 \tTraining Loss: 11.108479\n",
      "Epoch: 951 \tTraining Loss: 11.108478\n",
      "Epoch: 952 \tTraining Loss: 11.108478\n",
      "Epoch: 953 \tTraining Loss: 11.108478\n",
      "Epoch: 954 \tTraining Loss: 11.108478\n",
      "Epoch: 955 \tTraining Loss: 11.108478\n",
      "Epoch: 956 \tTraining Loss: 11.108478\n",
      "Epoch: 957 \tTraining Loss: 11.108478\n",
      "Epoch: 958 \tTraining Loss: 11.108476\n",
      "Epoch: 959 \tTraining Loss: 11.108477\n",
      "Epoch: 960 \tTraining Loss: 11.108477\n",
      "Epoch: 961 \tTraining Loss: 11.108477\n",
      "Epoch: 962 \tTraining Loss: 11.108476\n",
      "Epoch: 963 \tTraining Loss: 11.108477\n",
      "Epoch: 964 \tTraining Loss: 11.108476\n",
      "Epoch: 965 \tTraining Loss: 11.108476\n",
      "Epoch: 966 \tTraining Loss: 11.108475\n",
      "Epoch: 967 \tTraining Loss: 11.108476\n",
      "Epoch: 968 \tTraining Loss: 11.108477\n",
      "Epoch: 969 \tTraining Loss: 11.108476\n",
      "Epoch: 970 \tTraining Loss: 11.108477\n",
      "Epoch: 971 \tTraining Loss: 11.108475\n",
      "Epoch: 972 \tTraining Loss: 11.108475\n",
      "Epoch: 973 \tTraining Loss: 11.108475\n",
      "Epoch: 974 \tTraining Loss: 11.108475\n",
      "Epoch: 975 \tTraining Loss: 11.108474\n",
      "Epoch: 976 \tTraining Loss: 11.108475\n",
      "Epoch: 977 \tTraining Loss: 11.108475\n",
      "Epoch: 978 \tTraining Loss: 11.108473\n",
      "Epoch: 979 \tTraining Loss: 11.108473\n",
      "Epoch: 980 \tTraining Loss: 11.108473\n",
      "Epoch: 981 \tTraining Loss: 11.108472\n",
      "Epoch: 982 \tTraining Loss: 11.108472\n",
      "Epoch: 983 \tTraining Loss: 11.108473\n",
      "Epoch: 984 \tTraining Loss: 11.108473\n",
      "Epoch: 985 \tTraining Loss: 11.108472\n",
      "Epoch: 986 \tTraining Loss: 11.108472\n",
      "Epoch: 987 \tTraining Loss: 11.108471\n",
      "Epoch: 988 \tTraining Loss: 11.108471\n",
      "Epoch: 989 \tTraining Loss: 11.108471\n",
      "Epoch: 990 \tTraining Loss: 11.108471\n",
      "Epoch: 991 \tTraining Loss: 11.108470\n",
      "Epoch: 992 \tTraining Loss: 11.108472\n",
      "Epoch: 993 \tTraining Loss: 11.108470\n",
      "Epoch: 994 \tTraining Loss: 11.108469\n",
      "Epoch: 995 \tTraining Loss: 11.108470\n",
      "Epoch: 996 \tTraining Loss: 11.108470\n",
      "Epoch: 997 \tTraining Loss: 11.108470\n",
      "Epoch: 998 \tTraining Loss: 11.108470\n",
      "Epoch: 999 \tTraining Loss: 11.108468\n",
      "Epoch: 1000 \tTraining Loss: 11.108469\n",
      "Epoch: 1001 \tTraining Loss: 11.108468\n",
      "Epoch: 1002 \tTraining Loss: 11.108469\n",
      "Epoch: 1003 \tTraining Loss: 11.108469\n",
      "Epoch: 1004 \tTraining Loss: 11.108468\n",
      "Epoch: 1005 \tTraining Loss: 11.108467\n",
      "Epoch: 1006 \tTraining Loss: 11.108468\n",
      "Epoch: 1007 \tTraining Loss: 11.108467\n",
      "Epoch: 1008 \tTraining Loss: 11.108467\n",
      "Epoch: 1009 \tTraining Loss: 11.108466\n",
      "Epoch: 1010 \tTraining Loss: 11.108466\n",
      "Epoch: 1011 \tTraining Loss: 11.108465\n",
      "Epoch: 1012 \tTraining Loss: 11.108465\n",
      "Epoch: 1013 \tTraining Loss: 11.108466\n",
      "Epoch: 1014 \tTraining Loss: 11.108465\n",
      "Epoch: 1015 \tTraining Loss: 11.108465\n",
      "Epoch: 1016 \tTraining Loss: 11.108465\n",
      "Epoch: 1017 \tTraining Loss: 11.108465\n",
      "Epoch: 1018 \tTraining Loss: 11.108463\n",
      "Epoch: 1019 \tTraining Loss: 11.108464\n",
      "Epoch: 1020 \tTraining Loss: 11.108465\n",
      "Epoch: 1021 \tTraining Loss: 11.108463\n",
      "Epoch: 1022 \tTraining Loss: 11.108463\n",
      "Epoch: 1023 \tTraining Loss: 11.108463\n",
      "Epoch: 1024 \tTraining Loss: 11.108463\n",
      "Epoch: 1025 \tTraining Loss: 11.108463\n",
      "Epoch: 1026 \tTraining Loss: 11.108463\n",
      "Epoch: 1027 \tTraining Loss: 11.108462\n",
      "Epoch: 1028 \tTraining Loss: 11.108462\n",
      "Epoch: 1029 \tTraining Loss: 11.108462\n",
      "Epoch: 1030 \tTraining Loss: 11.108461\n",
      "Epoch: 1031 \tTraining Loss: 11.108461\n",
      "Epoch: 1032 \tTraining Loss: 11.108462\n",
      "Epoch: 1033 \tTraining Loss: 11.108461\n",
      "Epoch: 1034 \tTraining Loss: 11.108462\n",
      "Epoch: 1035 \tTraining Loss: 11.108461\n",
      "Epoch: 1036 \tTraining Loss: 11.108461\n",
      "Epoch: 1037 \tTraining Loss: 11.108461\n",
      "Epoch: 1038 \tTraining Loss: 11.108461\n",
      "Epoch: 1039 \tTraining Loss: 11.108460\n",
      "Epoch: 1040 \tTraining Loss: 11.108460\n",
      "Epoch: 1041 \tTraining Loss: 11.108460\n",
      "Epoch: 1042 \tTraining Loss: 11.108460\n",
      "Epoch: 1043 \tTraining Loss: 11.108460\n",
      "Epoch: 1044 \tTraining Loss: 11.108460\n",
      "Epoch: 1045 \tTraining Loss: 11.108460\n",
      "Epoch: 1046 \tTraining Loss: 11.108460\n",
      "Epoch: 1047 \tTraining Loss: 11.108459\n",
      "Epoch: 1048 \tTraining Loss: 11.108459\n",
      "Epoch: 1049 \tTraining Loss: 11.108459\n",
      "Epoch: 1050 \tTraining Loss: 11.108459\n",
      "Epoch: 1051 \tTraining Loss: 11.108459\n",
      "Epoch: 1052 \tTraining Loss: 11.108459\n",
      "Epoch: 1053 \tTraining Loss: 11.108458\n",
      "Epoch: 1054 \tTraining Loss: 11.108458\n",
      "Epoch: 1055 \tTraining Loss: 11.108458\n",
      "Epoch: 1056 \tTraining Loss: 11.108458\n",
      "Epoch: 1057 \tTraining Loss: 11.108458\n",
      "Epoch: 1058 \tTraining Loss: 11.108456\n",
      "Epoch: 1059 \tTraining Loss: 11.108456\n",
      "Epoch: 1060 \tTraining Loss: 11.108456\n",
      "Epoch: 1061 \tTraining Loss: 11.108456\n",
      "Epoch: 1062 \tTraining Loss: 11.108456\n",
      "Epoch: 1063 \tTraining Loss: 11.108455\n",
      "Epoch: 1064 \tTraining Loss: 11.108455\n",
      "Epoch: 1065 \tTraining Loss: 11.108455\n",
      "Epoch: 1066 \tTraining Loss: 11.108455\n",
      "Epoch: 1067 \tTraining Loss: 11.108455\n",
      "Epoch: 1068 \tTraining Loss: 11.108455\n",
      "Epoch: 1069 \tTraining Loss: 11.108455\n",
      "Epoch: 1070 \tTraining Loss: 11.108454\n",
      "Epoch: 1071 \tTraining Loss: 11.108454\n",
      "Epoch: 1072 \tTraining Loss: 11.108454\n",
      "Epoch: 1073 \tTraining Loss: 11.108454\n",
      "Epoch: 1074 \tTraining Loss: 11.108455\n",
      "Epoch: 1075 \tTraining Loss: 11.108454\n",
      "Epoch: 1076 \tTraining Loss: 11.108452\n",
      "Epoch: 1077 \tTraining Loss: 11.108452\n",
      "Epoch: 1078 \tTraining Loss: 11.108454\n",
      "Epoch: 1079 \tTraining Loss: 11.108452\n",
      "Epoch: 1080 \tTraining Loss: 11.108452\n",
      "Epoch: 1081 \tTraining Loss: 11.108452\n",
      "Epoch: 1082 \tTraining Loss: 11.108452\n",
      "Epoch: 1083 \tTraining Loss: 11.108451\n",
      "Epoch: 1084 \tTraining Loss: 11.108451\n",
      "Epoch: 1085 \tTraining Loss: 11.108451\n",
      "Epoch: 1086 \tTraining Loss: 11.108451\n",
      "Epoch: 1087 \tTraining Loss: 11.108451\n",
      "Epoch: 1088 \tTraining Loss: 11.108451\n",
      "Epoch: 1089 \tTraining Loss: 11.108451\n",
      "Epoch: 1090 \tTraining Loss: 11.108450\n",
      "Epoch: 1091 \tTraining Loss: 11.108450\n",
      "Epoch: 1092 \tTraining Loss: 11.108450\n",
      "Epoch: 1093 \tTraining Loss: 11.108449\n",
      "Epoch: 1094 \tTraining Loss: 11.108449\n",
      "Epoch: 1095 \tTraining Loss: 11.108449\n",
      "Epoch: 1096 \tTraining Loss: 11.108449\n",
      "Epoch: 1097 \tTraining Loss: 11.108448\n",
      "Epoch: 1098 \tTraining Loss: 11.108448\n",
      "Epoch: 1099 \tTraining Loss: 11.108448\n",
      "Epoch: 1100 \tTraining Loss: 11.108448\n",
      "Epoch: 1101 \tTraining Loss: 11.108448\n",
      "Epoch: 1102 \tTraining Loss: 11.108448\n",
      "Epoch: 1103 \tTraining Loss: 11.108448\n",
      "Epoch: 1104 \tTraining Loss: 11.108448\n",
      "Epoch: 1105 \tTraining Loss: 11.108447\n",
      "Epoch: 1106 \tTraining Loss: 11.108447\n",
      "Epoch: 1107 \tTraining Loss: 11.108447\n",
      "Epoch: 1108 \tTraining Loss: 11.108446\n",
      "Epoch: 1109 \tTraining Loss: 11.108447\n",
      "Epoch: 1110 \tTraining Loss: 11.108446\n",
      "Epoch: 1111 \tTraining Loss: 11.108445\n",
      "Epoch: 1112 \tTraining Loss: 11.108446\n",
      "Epoch: 1113 \tTraining Loss: 11.108445\n",
      "Epoch: 1114 \tTraining Loss: 11.108445\n",
      "Epoch: 1115 \tTraining Loss: 11.108444\n",
      "Epoch: 1116 \tTraining Loss: 11.108443\n",
      "Epoch: 1117 \tTraining Loss: 11.108443\n",
      "Epoch: 1118 \tTraining Loss: 11.108443\n",
      "Epoch: 1119 \tTraining Loss: 11.108443\n",
      "Epoch: 1120 \tTraining Loss: 11.108443\n",
      "Epoch: 1121 \tTraining Loss: 11.108443\n",
      "Epoch: 1122 \tTraining Loss: 11.108442\n",
      "Epoch: 1123 \tTraining Loss: 11.108442\n",
      "Epoch: 1124 \tTraining Loss: 11.108443\n",
      "Epoch: 1125 \tTraining Loss: 11.108442\n",
      "Epoch: 1126 \tTraining Loss: 11.108442\n",
      "Epoch: 1127 \tTraining Loss: 11.108442\n",
      "Epoch: 1128 \tTraining Loss: 11.108442\n",
      "Epoch: 1129 \tTraining Loss: 11.108442\n",
      "Epoch: 1130 \tTraining Loss: 11.108442\n",
      "Epoch: 1131 \tTraining Loss: 11.108442\n",
      "Epoch: 1132 \tTraining Loss: 11.108442\n",
      "Epoch: 1133 \tTraining Loss: 11.108441\n",
      "Epoch: 1134 \tTraining Loss: 11.108440\n",
      "Epoch: 1135 \tTraining Loss: 11.108440\n",
      "Epoch: 1136 \tTraining Loss: 11.108441\n",
      "Epoch: 1137 \tTraining Loss: 11.108441\n",
      "Epoch: 1138 \tTraining Loss: 11.108439\n",
      "Epoch: 1139 \tTraining Loss: 11.108439\n",
      "Epoch: 1140 \tTraining Loss: 11.108439\n",
      "Epoch: 1141 \tTraining Loss: 11.108438\n",
      "Epoch: 1142 \tTraining Loss: 11.108438\n",
      "Epoch: 1143 \tTraining Loss: 11.108439\n",
      "Epoch: 1144 \tTraining Loss: 11.108438\n",
      "Epoch: 1145 \tTraining Loss: 11.108438\n",
      "Epoch: 1146 \tTraining Loss: 11.108438\n",
      "Epoch: 1147 \tTraining Loss: 11.108438\n",
      "Epoch: 1148 \tTraining Loss: 11.108438\n",
      "Epoch: 1149 \tTraining Loss: 11.108438\n",
      "Epoch: 1150 \tTraining Loss: 11.108438\n",
      "Epoch: 1151 \tTraining Loss: 11.108438\n",
      "Epoch: 1152 \tTraining Loss: 11.108437\n",
      "Epoch: 1153 \tTraining Loss: 11.108437\n",
      "Epoch: 1154 \tTraining Loss: 11.108437\n",
      "Epoch: 1155 \tTraining Loss: 11.108437\n",
      "Epoch: 1156 \tTraining Loss: 11.108437\n",
      "Epoch: 1157 \tTraining Loss: 11.108437\n",
      "Epoch: 1158 \tTraining Loss: 11.108437\n",
      "Epoch: 1159 \tTraining Loss: 11.108437\n",
      "Epoch: 1160 \tTraining Loss: 11.108436\n",
      "Epoch: 1161 \tTraining Loss: 11.108434\n",
      "Epoch: 1162 \tTraining Loss: 11.108436\n",
      "Epoch: 1163 \tTraining Loss: 11.108436\n",
      "Epoch: 1164 \tTraining Loss: 11.108436\n",
      "Epoch: 1165 \tTraining Loss: 11.108434\n",
      "Epoch: 1166 \tTraining Loss: 11.108435\n",
      "Epoch: 1167 \tTraining Loss: 11.108434\n",
      "Epoch: 1168 \tTraining Loss: 11.108434\n",
      "Epoch: 1169 \tTraining Loss: 11.108434\n",
      "Epoch: 1170 \tTraining Loss: 11.108434\n",
      "Epoch: 1171 \tTraining Loss: 11.108432\n",
      "Epoch: 1172 \tTraining Loss: 11.108434\n",
      "Epoch: 1173 \tTraining Loss: 11.108433\n",
      "Epoch: 1174 \tTraining Loss: 11.108432\n",
      "Epoch: 1175 \tTraining Loss: 11.108432\n",
      "Epoch: 1176 \tTraining Loss: 11.108432\n",
      "Epoch: 1177 \tTraining Loss: 11.108432\n",
      "Epoch: 1178 \tTraining Loss: 11.108432\n",
      "Epoch: 1179 \tTraining Loss: 11.108432\n",
      "Epoch: 1180 \tTraining Loss: 11.108432\n",
      "Epoch: 1181 \tTraining Loss: 11.108431\n",
      "Epoch: 1182 \tTraining Loss: 11.108430\n",
      "Epoch: 1183 \tTraining Loss: 11.108431\n",
      "Epoch: 1184 \tTraining Loss: 11.108431\n",
      "Epoch: 1185 \tTraining Loss: 11.108431\n",
      "Epoch: 1186 \tTraining Loss: 11.108431\n",
      "Epoch: 1187 \tTraining Loss: 11.108431\n",
      "Epoch: 1188 \tTraining Loss: 11.108430\n",
      "Epoch: 1189 \tTraining Loss: 11.108430\n",
      "Epoch: 1190 \tTraining Loss: 11.108429\n",
      "Epoch: 1191 \tTraining Loss: 11.108430\n",
      "Epoch: 1192 \tTraining Loss: 11.108429\n",
      "Epoch: 1193 \tTraining Loss: 11.108429\n",
      "Epoch: 1194 \tTraining Loss: 11.108429\n",
      "Epoch: 1195 \tTraining Loss: 11.108428\n",
      "Epoch: 1196 \tTraining Loss: 11.108428\n",
      "Epoch: 1197 \tTraining Loss: 11.108427\n",
      "Epoch: 1198 \tTraining Loss: 11.108427\n",
      "Epoch: 1199 \tTraining Loss: 11.108427\n",
      "Epoch: 1200 \tTraining Loss: 11.108427\n",
      "Epoch: 1201 \tTraining Loss: 11.108427\n",
      "Epoch: 1202 \tTraining Loss: 11.108427\n",
      "Epoch: 1203 \tTraining Loss: 11.108426\n",
      "Epoch: 1204 \tTraining Loss: 11.108425\n",
      "Epoch: 1205 \tTraining Loss: 11.108425\n",
      "Epoch: 1206 \tTraining Loss: 11.108425\n",
      "Epoch: 1207 \tTraining Loss: 11.108425\n",
      "Epoch: 1208 \tTraining Loss: 11.108423\n",
      "Epoch: 1209 \tTraining Loss: 11.108425\n",
      "Epoch: 1210 \tTraining Loss: 11.108423\n",
      "Epoch: 1211 \tTraining Loss: 11.108425\n",
      "Epoch: 1212 \tTraining Loss: 11.108424\n",
      "Epoch: 1213 \tTraining Loss: 11.108425\n",
      "Epoch: 1214 \tTraining Loss: 11.108423\n",
      "Epoch: 1215 \tTraining Loss: 11.108423\n",
      "Epoch: 1216 \tTraining Loss: 11.108423\n",
      "Epoch: 1217 \tTraining Loss: 11.108423\n",
      "Epoch: 1218 \tTraining Loss: 11.108423\n",
      "Epoch: 1219 \tTraining Loss: 11.108423\n",
      "Epoch: 1220 \tTraining Loss: 11.108423\n",
      "Epoch: 1221 \tTraining Loss: 11.108421\n",
      "Epoch: 1222 \tTraining Loss: 11.108421\n",
      "Epoch: 1223 \tTraining Loss: 11.108421\n",
      "Epoch: 1224 \tTraining Loss: 11.108421\n",
      "Epoch: 1225 \tTraining Loss: 11.108421\n",
      "Epoch: 1226 \tTraining Loss: 11.108421\n",
      "Epoch: 1227 \tTraining Loss: 11.108421\n",
      "Epoch: 1228 \tTraining Loss: 11.108419\n",
      "Epoch: 1229 \tTraining Loss: 11.108419\n",
      "Epoch: 1230 \tTraining Loss: 11.108419\n",
      "Epoch: 1231 \tTraining Loss: 11.108419\n",
      "Epoch: 1232 \tTraining Loss: 11.108419\n",
      "Epoch: 1233 \tTraining Loss: 11.108419\n",
      "Epoch: 1234 \tTraining Loss: 11.108419\n",
      "Epoch: 1235 \tTraining Loss: 11.108419\n",
      "Epoch: 1236 \tTraining Loss: 11.108418\n",
      "Epoch: 1237 \tTraining Loss: 11.108418\n",
      "Epoch: 1238 \tTraining Loss: 11.108418\n",
      "Epoch: 1239 \tTraining Loss: 11.108418\n",
      "Epoch: 1240 \tTraining Loss: 11.108418\n",
      "Epoch: 1241 \tTraining Loss: 11.108417\n",
      "Epoch: 1242 \tTraining Loss: 11.108417\n",
      "Epoch: 1243 \tTraining Loss: 11.108417\n",
      "Epoch: 1244 \tTraining Loss: 11.108417\n",
      "Epoch: 1245 \tTraining Loss: 11.108417\n",
      "Epoch: 1246 \tTraining Loss: 11.108417\n",
      "Epoch: 1247 \tTraining Loss: 11.108416\n",
      "Epoch: 1248 \tTraining Loss: 11.108417\n",
      "Epoch: 1249 \tTraining Loss: 11.108417\n",
      "Epoch: 1250 \tTraining Loss: 11.108415\n",
      "Epoch: 1251 \tTraining Loss: 11.108414\n",
      "Epoch: 1252 \tTraining Loss: 11.108414\n",
      "Epoch: 1253 \tTraining Loss: 11.108415\n",
      "Epoch: 1254 \tTraining Loss: 11.108414\n",
      "Epoch: 1255 \tTraining Loss: 11.108415\n",
      "Epoch: 1256 \tTraining Loss: 11.108415\n",
      "Epoch: 1257 \tTraining Loss: 11.108414\n",
      "Epoch: 1258 \tTraining Loss: 11.108414\n",
      "Epoch: 1259 \tTraining Loss: 11.108414\n",
      "Epoch: 1260 \tTraining Loss: 11.108414\n",
      "Epoch: 1261 \tTraining Loss: 11.108414\n",
      "Epoch: 1262 \tTraining Loss: 11.108414\n",
      "Epoch: 1263 \tTraining Loss: 11.108414\n",
      "Epoch: 1264 \tTraining Loss: 11.108414\n",
      "Epoch: 1265 \tTraining Loss: 11.108412\n",
      "Epoch: 1266 \tTraining Loss: 11.108412\n",
      "Epoch: 1267 \tTraining Loss: 11.108412\n",
      "Epoch: 1268 \tTraining Loss: 11.108412\n",
      "Epoch: 1269 \tTraining Loss: 11.108412\n",
      "Epoch: 1270 \tTraining Loss: 11.108412\n",
      "Epoch: 1271 \tTraining Loss: 11.108412\n",
      "Epoch: 1272 \tTraining Loss: 11.108411\n",
      "Epoch: 1273 \tTraining Loss: 11.108410\n",
      "Epoch: 1274 \tTraining Loss: 11.108410\n",
      "Epoch: 1275 \tTraining Loss: 11.108410\n",
      "Epoch: 1276 \tTraining Loss: 11.108410\n",
      "Epoch: 1277 \tTraining Loss: 11.108409\n",
      "Epoch: 1278 \tTraining Loss: 11.108409\n",
      "Epoch: 1279 \tTraining Loss: 11.108410\n",
      "Epoch: 1280 \tTraining Loss: 11.108409\n",
      "Epoch: 1281 \tTraining Loss: 11.108409\n",
      "Epoch: 1282 \tTraining Loss: 11.108407\n",
      "Epoch: 1283 \tTraining Loss: 11.108408\n",
      "Epoch: 1284 \tTraining Loss: 11.108408\n",
      "Epoch: 1285 \tTraining Loss: 11.108409\n",
      "Epoch: 1286 \tTraining Loss: 11.108409\n",
      "Epoch: 1287 \tTraining Loss: 11.108409\n",
      "Epoch: 1288 \tTraining Loss: 11.108409\n",
      "Epoch: 1289 \tTraining Loss: 11.108407\n",
      "Epoch: 1290 \tTraining Loss: 11.108407\n",
      "Epoch: 1291 \tTraining Loss: 11.108407\n",
      "Epoch: 1292 \tTraining Loss: 11.108407\n",
      "Epoch: 1293 \tTraining Loss: 11.108405\n",
      "Epoch: 1294 \tTraining Loss: 11.108406\n",
      "Epoch: 1295 \tTraining Loss: 11.108407\n",
      "Epoch: 1296 \tTraining Loss: 11.108406\n",
      "Epoch: 1297 \tTraining Loss: 11.108406\n",
      "Epoch: 1298 \tTraining Loss: 11.108404\n",
      "Epoch: 1299 \tTraining Loss: 11.108404\n",
      "Epoch: 1300 \tTraining Loss: 11.108405\n",
      "Epoch: 1301 \tTraining Loss: 11.108404\n",
      "Epoch: 1302 \tTraining Loss: 11.108405\n",
      "Epoch: 1303 \tTraining Loss: 11.108405\n",
      "Epoch: 1304 \tTraining Loss: 11.108404\n",
      "Epoch: 1305 \tTraining Loss: 11.108403\n",
      "Epoch: 1306 \tTraining Loss: 11.108404\n",
      "Epoch: 1307 \tTraining Loss: 11.108404\n",
      "Epoch: 1308 \tTraining Loss: 11.108403\n",
      "Epoch: 1309 \tTraining Loss: 11.108403\n",
      "Epoch: 1310 \tTraining Loss: 11.108403\n",
      "Epoch: 1311 \tTraining Loss: 11.108403\n",
      "Epoch: 1312 \tTraining Loss: 11.108403\n",
      "Epoch: 1313 \tTraining Loss: 11.108403\n",
      "Epoch: 1314 \tTraining Loss: 11.108403\n",
      "Epoch: 1315 \tTraining Loss: 11.108402\n",
      "Epoch: 1316 \tTraining Loss: 11.108402\n",
      "Epoch: 1317 \tTraining Loss: 11.108402\n",
      "Epoch: 1318 \tTraining Loss: 11.108401\n",
      "Epoch: 1319 \tTraining Loss: 11.108401\n",
      "Epoch: 1320 \tTraining Loss: 11.108401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1321 \tTraining Loss: 11.108400\n",
      "Epoch: 1322 \tTraining Loss: 11.108401\n",
      "Epoch: 1323 \tTraining Loss: 11.108400\n",
      "Epoch: 1324 \tTraining Loss: 11.108400\n",
      "Epoch: 1325 \tTraining Loss: 11.108400\n",
      "Epoch: 1326 \tTraining Loss: 11.108400\n",
      "Epoch: 1327 \tTraining Loss: 11.108399\n",
      "Epoch: 1328 \tTraining Loss: 11.108400\n",
      "Epoch: 1329 \tTraining Loss: 11.108399\n",
      "Epoch: 1330 \tTraining Loss: 11.108399\n",
      "Epoch: 1331 \tTraining Loss: 11.108398\n",
      "Epoch: 1332 \tTraining Loss: 11.108397\n",
      "Epoch: 1333 \tTraining Loss: 11.108398\n",
      "Epoch: 1334 \tTraining Loss: 11.108398\n",
      "Epoch: 1335 \tTraining Loss: 11.108397\n",
      "Epoch: 1336 \tTraining Loss: 11.108398\n",
      "Epoch: 1337 \tTraining Loss: 11.108397\n",
      "Epoch: 1338 \tTraining Loss: 11.108398\n",
      "Epoch: 1339 \tTraining Loss: 11.108397\n",
      "Epoch: 1340 \tTraining Loss: 11.108397\n",
      "Epoch: 1341 \tTraining Loss: 11.108397\n",
      "Epoch: 1342 \tTraining Loss: 11.108396\n",
      "Epoch: 1343 \tTraining Loss: 11.108396\n",
      "Epoch: 1344 \tTraining Loss: 11.108396\n",
      "Epoch: 1345 \tTraining Loss: 11.108396\n",
      "Epoch: 1346 \tTraining Loss: 11.108396\n",
      "Epoch: 1347 \tTraining Loss: 11.108396\n",
      "Epoch: 1348 \tTraining Loss: 11.108396\n",
      "Epoch: 1349 \tTraining Loss: 11.108396\n",
      "Epoch: 1350 \tTraining Loss: 11.108396\n",
      "Epoch: 1351 \tTraining Loss: 11.108396\n",
      "Epoch: 1352 \tTraining Loss: 11.108396\n",
      "Epoch: 1353 \tTraining Loss: 11.108395\n",
      "Epoch: 1354 \tTraining Loss: 11.108395\n",
      "Epoch: 1355 \tTraining Loss: 11.108395\n",
      "Epoch: 1356 \tTraining Loss: 11.108393\n",
      "Epoch: 1357 \tTraining Loss: 11.108393\n",
      "Epoch: 1358 \tTraining Loss: 11.108394\n",
      "Epoch: 1359 \tTraining Loss: 11.108393\n",
      "Epoch: 1360 \tTraining Loss: 11.108394\n",
      "Epoch: 1361 \tTraining Loss: 11.108393\n",
      "Epoch: 1362 \tTraining Loss: 11.108392\n",
      "Epoch: 1363 \tTraining Loss: 11.108390\n",
      "Epoch: 1364 \tTraining Loss: 11.108392\n",
      "Epoch: 1365 \tTraining Loss: 11.108391\n",
      "Epoch: 1366 \tTraining Loss: 11.108391\n",
      "Epoch: 1367 \tTraining Loss: 11.108390\n",
      "Epoch: 1368 \tTraining Loss: 11.108390\n",
      "Epoch: 1369 \tTraining Loss: 11.108390\n",
      "Epoch: 1370 \tTraining Loss: 11.108390\n",
      "Epoch: 1371 \tTraining Loss: 11.108390\n",
      "Epoch: 1372 \tTraining Loss: 11.108390\n",
      "Epoch: 1373 \tTraining Loss: 11.108390\n",
      "Epoch: 1374 \tTraining Loss: 11.108390\n",
      "Epoch: 1375 \tTraining Loss: 11.108390\n",
      "Epoch: 1376 \tTraining Loss: 11.108390\n",
      "Epoch: 1377 \tTraining Loss: 11.108390\n",
      "Epoch: 1378 \tTraining Loss: 11.108390\n",
      "Epoch: 1379 \tTraining Loss: 11.108389\n",
      "Epoch: 1380 \tTraining Loss: 11.108389\n",
      "Epoch: 1381 \tTraining Loss: 11.108388\n",
      "Epoch: 1382 \tTraining Loss: 11.108388\n",
      "Epoch: 1383 \tTraining Loss: 11.108388\n",
      "Epoch: 1384 \tTraining Loss: 11.108386\n",
      "Epoch: 1385 \tTraining Loss: 11.108387\n",
      "Epoch: 1386 \tTraining Loss: 11.108387\n",
      "Epoch: 1387 \tTraining Loss: 11.108387\n",
      "Epoch: 1388 \tTraining Loss: 11.108385\n",
      "Epoch: 1389 \tTraining Loss: 11.108386\n",
      "Epoch: 1390 \tTraining Loss: 11.108385\n",
      "Epoch: 1391 \tTraining Loss: 11.108385\n",
      "Epoch: 1392 \tTraining Loss: 11.108385\n",
      "Epoch: 1393 \tTraining Loss: 11.108385\n",
      "Epoch: 1394 \tTraining Loss: 11.108385\n",
      "Epoch: 1395 \tTraining Loss: 11.108385\n",
      "Epoch: 1396 \tTraining Loss: 11.108385\n",
      "Epoch: 1397 \tTraining Loss: 11.108384\n",
      "Epoch: 1398 \tTraining Loss: 11.108384\n",
      "Epoch: 1399 \tTraining Loss: 11.108384\n",
      "Epoch: 1400 \tTraining Loss: 11.108384\n",
      "Epoch: 1401 \tTraining Loss: 11.108384\n",
      "Epoch: 1402 \tTraining Loss: 11.108384\n",
      "Epoch: 1403 \tTraining Loss: 11.108384\n",
      "Epoch: 1404 \tTraining Loss: 11.108384\n",
      "Epoch: 1405 \tTraining Loss: 11.108383\n",
      "Epoch: 1406 \tTraining Loss: 11.108382\n",
      "Epoch: 1407 \tTraining Loss: 11.108382\n",
      "Epoch: 1408 \tTraining Loss: 11.108382\n",
      "Epoch: 1409 \tTraining Loss: 11.108381\n",
      "Epoch: 1410 \tTraining Loss: 11.108381\n",
      "Epoch: 1411 \tTraining Loss: 11.108380\n",
      "Epoch: 1412 \tTraining Loss: 11.108380\n",
      "Epoch: 1413 \tTraining Loss: 11.108379\n",
      "Epoch: 1414 \tTraining Loss: 11.108379\n",
      "Epoch: 1415 \tTraining Loss: 11.108379\n",
      "Epoch: 1416 \tTraining Loss: 11.108380\n",
      "Epoch: 1417 \tTraining Loss: 11.108378\n",
      "Epoch: 1418 \tTraining Loss: 11.108378\n",
      "Epoch: 1419 \tTraining Loss: 11.108378\n",
      "Epoch: 1420 \tTraining Loss: 11.108379\n",
      "Epoch: 1421 \tTraining Loss: 11.108378\n",
      "Epoch: 1422 \tTraining Loss: 11.108378\n",
      "Epoch: 1423 \tTraining Loss: 11.108378\n",
      "Epoch: 1424 \tTraining Loss: 11.108378\n",
      "Epoch: 1425 \tTraining Loss: 11.108378\n",
      "Epoch: 1426 \tTraining Loss: 11.108378\n",
      "Epoch: 1427 \tTraining Loss: 11.108377\n",
      "Epoch: 1428 \tTraining Loss: 11.108377\n",
      "Epoch: 1429 \tTraining Loss: 11.108377\n",
      "Epoch: 1430 \tTraining Loss: 11.108376\n",
      "Epoch: 1431 \tTraining Loss: 11.108376\n",
      "Epoch: 1432 \tTraining Loss: 11.108376\n",
      "Epoch: 1433 \tTraining Loss: 11.108375\n",
      "Epoch: 1434 \tTraining Loss: 11.108377\n",
      "Epoch: 1435 \tTraining Loss: 11.108375\n",
      "Epoch: 1436 \tTraining Loss: 11.108376\n",
      "Epoch: 1437 \tTraining Loss: 11.108375\n",
      "Epoch: 1438 \tTraining Loss: 11.108374\n",
      "Epoch: 1439 \tTraining Loss: 11.108375\n",
      "Epoch: 1440 \tTraining Loss: 11.108375\n",
      "Epoch: 1441 \tTraining Loss: 11.108375\n",
      "Epoch: 1442 \tTraining Loss: 11.108375\n",
      "Epoch: 1443 \tTraining Loss: 11.108374\n",
      "Epoch: 1444 \tTraining Loss: 11.108374\n",
      "Epoch: 1445 \tTraining Loss: 11.108374\n",
      "Epoch: 1446 \tTraining Loss: 11.108374\n",
      "Epoch: 1447 \tTraining Loss: 11.108374\n",
      "Epoch: 1448 \tTraining Loss: 11.108373\n",
      "Epoch: 1449 \tTraining Loss: 11.108373\n",
      "Epoch: 1450 \tTraining Loss: 11.108373\n",
      "Epoch: 1451 \tTraining Loss: 11.108372\n",
      "Epoch: 1452 \tTraining Loss: 11.108371\n",
      "Epoch: 1453 \tTraining Loss: 11.108372\n",
      "Epoch: 1454 \tTraining Loss: 11.108372\n",
      "Epoch: 1455 \tTraining Loss: 11.108372\n",
      "Epoch: 1456 \tTraining Loss: 11.108371\n",
      "Epoch: 1457 \tTraining Loss: 11.108371\n",
      "Epoch: 1458 \tTraining Loss: 11.108371\n",
      "Epoch: 1459 \tTraining Loss: 11.108370\n",
      "Epoch: 1460 \tTraining Loss: 11.108371\n",
      "Epoch: 1461 \tTraining Loss: 11.108371\n",
      "Epoch: 1462 \tTraining Loss: 11.108370\n",
      "Epoch: 1463 \tTraining Loss: 11.108370\n",
      "Epoch: 1464 \tTraining Loss: 11.108370\n",
      "Epoch: 1465 \tTraining Loss: 11.108370\n",
      "Epoch: 1466 \tTraining Loss: 11.108369\n",
      "Epoch: 1467 \tTraining Loss: 11.108370\n",
      "Epoch: 1468 \tTraining Loss: 11.108369\n",
      "Epoch: 1469 \tTraining Loss: 11.108369\n",
      "Epoch: 1470 \tTraining Loss: 11.108369\n",
      "Epoch: 1471 \tTraining Loss: 11.108369\n",
      "Epoch: 1472 \tTraining Loss: 11.108368\n",
      "Epoch: 1473 \tTraining Loss: 11.108367\n",
      "Epoch: 1474 \tTraining Loss: 11.108367\n",
      "Epoch: 1475 \tTraining Loss: 11.108366\n",
      "Epoch: 1476 \tTraining Loss: 11.108366\n",
      "Epoch: 1477 \tTraining Loss: 11.108366\n",
      "Epoch: 1478 \tTraining Loss: 11.108366\n",
      "Epoch: 1479 \tTraining Loss: 11.108366\n",
      "Epoch: 1480 \tTraining Loss: 11.108366\n",
      "Epoch: 1481 \tTraining Loss: 11.108366\n",
      "Epoch: 1482 \tTraining Loss: 11.108366\n",
      "Epoch: 1483 \tTraining Loss: 11.108366\n",
      "Epoch: 1484 \tTraining Loss: 11.108366\n",
      "Epoch: 1485 \tTraining Loss: 11.108366\n",
      "Epoch: 1486 \tTraining Loss: 11.108366\n",
      "Epoch: 1487 \tTraining Loss: 11.108366\n",
      "Epoch: 1488 \tTraining Loss: 11.108366\n",
      "Epoch: 1489 \tTraining Loss: 11.108366\n",
      "Epoch: 1490 \tTraining Loss: 11.108366\n",
      "Epoch: 1491 \tTraining Loss: 11.108366\n",
      "Epoch: 1492 \tTraining Loss: 11.108365\n",
      "Epoch: 1493 \tTraining Loss: 11.108364\n",
      "Epoch: 1494 \tTraining Loss: 11.108362\n",
      "Epoch: 1495 \tTraining Loss: 11.108363\n",
      "Epoch: 1496 \tTraining Loss: 11.108363\n",
      "Epoch: 1497 \tTraining Loss: 11.108363\n",
      "Epoch: 1498 \tTraining Loss: 11.108363\n",
      "Epoch: 1499 \tTraining Loss: 11.108363\n",
      "Epoch: 1500 \tTraining Loss: 11.108362\n",
      "Epoch: 1501 \tTraining Loss: 11.108362\n",
      "Epoch: 1502 \tTraining Loss: 11.108362\n",
      "Epoch: 1503 \tTraining Loss: 11.108360\n",
      "Epoch: 1504 \tTraining Loss: 11.108361\n",
      "Epoch: 1505 \tTraining Loss: 11.108360\n",
      "Epoch: 1506 \tTraining Loss: 11.108360\n",
      "Epoch: 1507 \tTraining Loss: 11.108361\n",
      "Epoch: 1508 \tTraining Loss: 11.108360\n",
      "Epoch: 1509 \tTraining Loss: 11.108360\n",
      "Epoch: 1510 \tTraining Loss: 11.108360\n",
      "Epoch: 1511 \tTraining Loss: 11.108360\n",
      "Epoch: 1512 \tTraining Loss: 11.108360\n",
      "Epoch: 1513 \tTraining Loss: 11.108359\n",
      "Epoch: 1514 \tTraining Loss: 11.108359\n",
      "Epoch: 1515 \tTraining Loss: 11.108357\n",
      "Epoch: 1516 \tTraining Loss: 11.108358\n",
      "Epoch: 1517 \tTraining Loss: 11.108358\n",
      "Epoch: 1518 \tTraining Loss: 11.108358\n",
      "Epoch: 1519 \tTraining Loss: 11.108358\n",
      "Epoch: 1520 \tTraining Loss: 11.108358\n",
      "Epoch: 1521 \tTraining Loss: 11.108356\n",
      "Epoch: 1522 \tTraining Loss: 11.108356\n",
      "Epoch: 1523 \tTraining Loss: 11.108356\n",
      "Epoch: 1524 \tTraining Loss: 11.108356\n",
      "Epoch: 1525 \tTraining Loss: 11.108356\n",
      "Epoch: 1526 \tTraining Loss: 11.108356\n",
      "Epoch: 1527 \tTraining Loss: 11.108356\n",
      "Epoch: 1528 \tTraining Loss: 11.108356\n",
      "Epoch: 1529 \tTraining Loss: 11.108355\n",
      "Epoch: 1530 \tTraining Loss: 11.108355\n",
      "Epoch: 1531 \tTraining Loss: 11.108355\n",
      "Epoch: 1532 \tTraining Loss: 11.108355\n",
      "Epoch: 1533 \tTraining Loss: 11.108355\n",
      "Epoch: 1534 \tTraining Loss: 11.108355\n",
      "Epoch: 1535 \tTraining Loss: 11.108355\n",
      "Epoch: 1536 \tTraining Loss: 11.108353\n",
      "Epoch: 1537 \tTraining Loss: 11.108353\n",
      "Epoch: 1538 \tTraining Loss: 11.108352\n",
      "Epoch: 1539 \tTraining Loss: 11.108352\n",
      "Epoch: 1540 \tTraining Loss: 11.108352\n",
      "Epoch: 1541 \tTraining Loss: 11.108352\n",
      "Epoch: 1542 \tTraining Loss: 11.108352\n",
      "Epoch: 1543 \tTraining Loss: 11.108352\n",
      "Epoch: 1544 \tTraining Loss: 11.108351\n",
      "Epoch: 1545 \tTraining Loss: 11.108352\n",
      "Epoch: 1546 \tTraining Loss: 11.108351\n",
      "Epoch: 1547 \tTraining Loss: 11.108351\n",
      "Epoch: 1548 \tTraining Loss: 11.108350\n",
      "Epoch: 1549 \tTraining Loss: 11.108350\n",
      "Epoch: 1550 \tTraining Loss: 11.108350\n",
      "Epoch: 1551 \tTraining Loss: 11.108350\n",
      "Epoch: 1552 \tTraining Loss: 11.108350\n",
      "Epoch: 1553 \tTraining Loss: 11.108350\n",
      "Epoch: 1554 \tTraining Loss: 11.108350\n",
      "Epoch: 1555 \tTraining Loss: 11.108350\n",
      "Epoch: 1556 \tTraining Loss: 11.108349\n",
      "Epoch: 1557 \tTraining Loss: 11.108349\n",
      "Epoch: 1558 \tTraining Loss: 11.108349\n",
      "Epoch: 1559 \tTraining Loss: 11.108348\n",
      "Epoch: 1560 \tTraining Loss: 11.108348\n",
      "Epoch: 1561 \tTraining Loss: 11.108348\n",
      "Epoch: 1562 \tTraining Loss: 11.108348\n",
      "Epoch: 1563 \tTraining Loss: 11.108348\n",
      "Epoch: 1564 \tTraining Loss: 11.108348\n",
      "Epoch: 1565 \tTraining Loss: 11.108348\n",
      "Epoch: 1566 \tTraining Loss: 11.108347\n",
      "Epoch: 1567 \tTraining Loss: 11.108347\n",
      "Epoch: 1568 \tTraining Loss: 11.108346\n",
      "Epoch: 1569 \tTraining Loss: 11.108346\n",
      "Epoch: 1570 \tTraining Loss: 11.108345\n",
      "Epoch: 1571 \tTraining Loss: 11.108345\n",
      "Epoch: 1572 \tTraining Loss: 11.108345\n",
      "Epoch: 1573 \tTraining Loss: 11.108345\n",
      "Epoch: 1574 \tTraining Loss: 11.108345\n",
      "Epoch: 1575 \tTraining Loss: 11.108345\n",
      "Epoch: 1576 \tTraining Loss: 11.108344\n",
      "Epoch: 1577 \tTraining Loss: 11.108344\n",
      "Epoch: 1578 \tTraining Loss: 11.108343\n",
      "Epoch: 1579 \tTraining Loss: 11.108343\n",
      "Epoch: 1580 \tTraining Loss: 11.108343\n",
      "Epoch: 1581 \tTraining Loss: 11.108343\n",
      "Epoch: 1582 \tTraining Loss: 11.108343\n",
      "Epoch: 1583 \tTraining Loss: 11.108343\n",
      "Epoch: 1584 \tTraining Loss: 11.108343\n",
      "Epoch: 1585 \tTraining Loss: 11.108343\n",
      "Epoch: 1586 \tTraining Loss: 11.108343\n",
      "Epoch: 1587 \tTraining Loss: 11.108342\n",
      "Epoch: 1588 \tTraining Loss: 11.108343\n",
      "Epoch: 1589 \tTraining Loss: 11.108342\n",
      "Epoch: 1590 \tTraining Loss: 11.108342\n",
      "Epoch: 1591 \tTraining Loss: 11.108342\n",
      "Epoch: 1592 \tTraining Loss: 11.108342\n",
      "Epoch: 1593 \tTraining Loss: 11.108342\n",
      "Epoch: 1594 \tTraining Loss: 11.108341\n",
      "Epoch: 1595 \tTraining Loss: 11.108342\n",
      "Epoch: 1596 \tTraining Loss: 11.108341\n",
      "Epoch: 1597 \tTraining Loss: 11.108341\n",
      "Epoch: 1598 \tTraining Loss: 11.108340\n",
      "Epoch: 1599 \tTraining Loss: 11.108339\n",
      "Epoch: 1600 \tTraining Loss: 11.108342\n",
      "Epoch: 1601 \tTraining Loss: 11.108339\n",
      "Epoch: 1602 \tTraining Loss: 11.108340\n",
      "Epoch: 1603 \tTraining Loss: 11.108340\n",
      "Epoch: 1604 \tTraining Loss: 11.108340\n",
      "Epoch: 1605 \tTraining Loss: 11.108340\n",
      "Epoch: 1606 \tTraining Loss: 11.108340\n",
      "Epoch: 1607 \tTraining Loss: 11.108340\n",
      "Epoch: 1608 \tTraining Loss: 11.108340\n",
      "Epoch: 1609 \tTraining Loss: 11.108338\n",
      "Epoch: 1610 \tTraining Loss: 11.108338\n",
      "Epoch: 1611 \tTraining Loss: 11.108338\n",
      "Epoch: 1612 \tTraining Loss: 11.108338\n",
      "Epoch: 1613 \tTraining Loss: 11.108338\n",
      "Epoch: 1614 \tTraining Loss: 11.108338\n",
      "Epoch: 1615 \tTraining Loss: 11.108338\n",
      "Epoch: 1616 \tTraining Loss: 11.108337\n",
      "Epoch: 1617 \tTraining Loss: 11.108338\n",
      "Epoch: 1618 \tTraining Loss: 11.108338\n",
      "Epoch: 1619 \tTraining Loss: 11.108337\n",
      "Epoch: 1620 \tTraining Loss: 11.108337\n",
      "Epoch: 1621 \tTraining Loss: 11.108336\n",
      "Epoch: 1622 \tTraining Loss: 11.108336\n",
      "Epoch: 1623 \tTraining Loss: 11.108336\n",
      "Epoch: 1624 \tTraining Loss: 11.108336\n",
      "Epoch: 1625 \tTraining Loss: 11.108336\n",
      "Epoch: 1626 \tTraining Loss: 11.108335\n",
      "Epoch: 1627 \tTraining Loss: 11.108336\n",
      "Epoch: 1628 \tTraining Loss: 11.108335\n",
      "Epoch: 1629 \tTraining Loss: 11.108335\n",
      "Epoch: 1630 \tTraining Loss: 11.108335\n",
      "Epoch: 1631 \tTraining Loss: 11.108335\n",
      "Epoch: 1632 \tTraining Loss: 11.108335\n",
      "Epoch: 1633 \tTraining Loss: 11.108335\n",
      "Epoch: 1634 \tTraining Loss: 11.108335\n",
      "Epoch: 1635 \tTraining Loss: 11.108334\n",
      "Epoch: 1636 \tTraining Loss: 11.108333\n",
      "Epoch: 1637 \tTraining Loss: 11.108333\n",
      "Epoch: 1638 \tTraining Loss: 11.108333\n",
      "Epoch: 1639 \tTraining Loss: 11.108334\n",
      "Epoch: 1640 \tTraining Loss: 11.108333\n",
      "Epoch: 1641 \tTraining Loss: 11.108333\n",
      "Epoch: 1642 \tTraining Loss: 11.108333\n",
      "Epoch: 1643 \tTraining Loss: 11.108331\n",
      "Epoch: 1644 \tTraining Loss: 11.108331\n",
      "Epoch: 1645 \tTraining Loss: 11.108331\n",
      "Epoch: 1646 \tTraining Loss: 11.108331\n",
      "Epoch: 1647 \tTraining Loss: 11.108331\n",
      "Epoch: 1648 \tTraining Loss: 11.108331\n",
      "Epoch: 1649 \tTraining Loss: 11.108331\n",
      "Epoch: 1650 \tTraining Loss: 11.108331\n",
      "Epoch: 1651 \tTraining Loss: 11.108331\n",
      "Epoch: 1652 \tTraining Loss: 11.108331\n",
      "Epoch: 1653 \tTraining Loss: 11.108331\n",
      "Epoch: 1654 \tTraining Loss: 11.108331\n",
      "Epoch: 1655 \tTraining Loss: 11.108330\n",
      "Epoch: 1656 \tTraining Loss: 11.108330\n",
      "Epoch: 1657 \tTraining Loss: 11.108330\n",
      "Epoch: 1658 \tTraining Loss: 11.108330\n",
      "Epoch: 1659 \tTraining Loss: 11.108329\n",
      "Epoch: 1660 \tTraining Loss: 11.108329\n",
      "Epoch: 1661 \tTraining Loss: 11.108328\n",
      "Epoch: 1662 \tTraining Loss: 11.108328\n",
      "Epoch: 1663 \tTraining Loss: 11.108328\n",
      "Epoch: 1664 \tTraining Loss: 11.108328\n",
      "Epoch: 1665 \tTraining Loss: 11.108327\n",
      "Epoch: 1666 \tTraining Loss: 11.108326\n",
      "Epoch: 1667 \tTraining Loss: 11.108326\n",
      "Epoch: 1668 \tTraining Loss: 11.108326\n",
      "Epoch: 1669 \tTraining Loss: 11.108326\n",
      "Epoch: 1670 \tTraining Loss: 11.108325\n",
      "Epoch: 1671 \tTraining Loss: 11.108325\n",
      "Epoch: 1672 \tTraining Loss: 11.108325\n",
      "Epoch: 1673 \tTraining Loss: 11.108325\n",
      "Epoch: 1674 \tTraining Loss: 11.108325\n",
      "Epoch: 1675 \tTraining Loss: 11.108325\n",
      "Epoch: 1676 \tTraining Loss: 11.108325\n",
      "Epoch: 1677 \tTraining Loss: 11.108325\n",
      "Epoch: 1678 \tTraining Loss: 11.108324\n",
      "Epoch: 1679 \tTraining Loss: 11.108322\n",
      "Epoch: 1680 \tTraining Loss: 11.108324\n",
      "Epoch: 1681 \tTraining Loss: 11.108322\n",
      "Epoch: 1682 \tTraining Loss: 11.108322\n",
      "Epoch: 1683 \tTraining Loss: 11.108322\n",
      "Epoch: 1684 \tTraining Loss: 11.108322\n",
      "Epoch: 1685 \tTraining Loss: 11.108322\n",
      "Epoch: 1686 \tTraining Loss: 11.108322\n",
      "Epoch: 1687 \tTraining Loss: 11.108321\n",
      "Epoch: 1688 \tTraining Loss: 11.108321\n",
      "Epoch: 1689 \tTraining Loss: 11.108321\n",
      "Epoch: 1690 \tTraining Loss: 11.108320\n",
      "Epoch: 1691 \tTraining Loss: 11.108320\n",
      "Epoch: 1692 \tTraining Loss: 11.108319\n",
      "Epoch: 1693 \tTraining Loss: 11.108319\n",
      "Epoch: 1694 \tTraining Loss: 11.108319\n",
      "Epoch: 1695 \tTraining Loss: 11.108319\n",
      "Epoch: 1696 \tTraining Loss: 11.108319\n",
      "Epoch: 1697 \tTraining Loss: 11.108319\n",
      "Epoch: 1698 \tTraining Loss: 11.108318\n",
      "Epoch: 1699 \tTraining Loss: 11.108318\n",
      "Epoch: 1700 \tTraining Loss: 11.108318\n",
      "Epoch: 1701 \tTraining Loss: 11.108318\n",
      "Epoch: 1702 \tTraining Loss: 11.108318\n",
      "Epoch: 1703 \tTraining Loss: 11.108317\n",
      "Epoch: 1704 \tTraining Loss: 11.108316\n",
      "Epoch: 1705 \tTraining Loss: 11.108316\n",
      "Epoch: 1706 \tTraining Loss: 11.108316\n",
      "Epoch: 1707 \tTraining Loss: 11.108316\n",
      "Epoch: 1708 \tTraining Loss: 11.108315\n",
      "Epoch: 1709 \tTraining Loss: 11.108315\n",
      "Epoch: 1710 \tTraining Loss: 11.108315\n",
      "Epoch: 1711 \tTraining Loss: 11.108315\n",
      "Epoch: 1712 \tTraining Loss: 11.108315\n",
      "Epoch: 1713 \tTraining Loss: 11.108315\n",
      "Epoch: 1714 \tTraining Loss: 11.108315\n",
      "Epoch: 1715 \tTraining Loss: 11.108315\n",
      "Epoch: 1716 \tTraining Loss: 11.108315\n",
      "Epoch: 1717 \tTraining Loss: 11.108315\n",
      "Epoch: 1718 \tTraining Loss: 11.108315\n",
      "Epoch: 1719 \tTraining Loss: 11.108315\n",
      "Epoch: 1720 \tTraining Loss: 11.108315\n",
      "Epoch: 1721 \tTraining Loss: 11.108315\n",
      "Epoch: 1722 \tTraining Loss: 11.108314\n",
      "Epoch: 1723 \tTraining Loss: 11.108314\n",
      "Epoch: 1724 \tTraining Loss: 11.108314\n",
      "Epoch: 1725 \tTraining Loss: 11.108315\n",
      "Epoch: 1726 \tTraining Loss: 11.108314\n",
      "Epoch: 1727 \tTraining Loss: 11.108314\n",
      "Epoch: 1728 \tTraining Loss: 11.108314\n",
      "Epoch: 1729 \tTraining Loss: 11.108314\n",
      "Epoch: 1730 \tTraining Loss: 11.108314\n",
      "Epoch: 1731 \tTraining Loss: 11.108314\n",
      "Epoch: 1732 \tTraining Loss: 11.108311\n",
      "Epoch: 1733 \tTraining Loss: 11.108313\n",
      "Epoch: 1734 \tTraining Loss: 11.108310\n",
      "Epoch: 1735 \tTraining Loss: 11.108311\n",
      "Epoch: 1736 \tTraining Loss: 11.108311\n",
      "Epoch: 1737 \tTraining Loss: 11.108311\n",
      "Epoch: 1738 \tTraining Loss: 11.108311\n",
      "Epoch: 1739 \tTraining Loss: 11.108311\n",
      "Epoch: 1740 \tTraining Loss: 11.108310\n",
      "Epoch: 1741 \tTraining Loss: 11.108309\n",
      "Epoch: 1742 \tTraining Loss: 11.108309\n",
      "Epoch: 1743 \tTraining Loss: 11.108309\n",
      "Epoch: 1744 \tTraining Loss: 11.108309\n",
      "Epoch: 1745 \tTraining Loss: 11.108309\n",
      "Epoch: 1746 \tTraining Loss: 11.108309\n",
      "Epoch: 1747 \tTraining Loss: 11.108308\n",
      "Epoch: 1748 \tTraining Loss: 11.108309\n",
      "Epoch: 1749 \tTraining Loss: 11.108308\n",
      "Epoch: 1750 \tTraining Loss: 11.108308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1751 \tTraining Loss: 11.108308\n",
      "Epoch: 1752 \tTraining Loss: 11.108308\n",
      "Epoch: 1753 \tTraining Loss: 11.108307\n",
      "Epoch: 1754 \tTraining Loss: 11.108307\n",
      "Epoch: 1755 \tTraining Loss: 11.108307\n",
      "Epoch: 1756 \tTraining Loss: 11.108307\n",
      "Epoch: 1757 \tTraining Loss: 11.108306\n",
      "Epoch: 1758 \tTraining Loss: 11.108306\n",
      "Epoch: 1759 \tTraining Loss: 11.108306\n",
      "Epoch: 1760 \tTraining Loss: 11.108306\n",
      "Epoch: 1761 \tTraining Loss: 11.108306\n",
      "Epoch: 1762 \tTraining Loss: 11.108305\n",
      "Epoch: 1763 \tTraining Loss: 11.108304\n",
      "Epoch: 1764 \tTraining Loss: 11.108304\n",
      "Epoch: 1765 \tTraining Loss: 11.108304\n",
      "Epoch: 1766 \tTraining Loss: 11.108305\n",
      "Epoch: 1767 \tTraining Loss: 11.108305\n",
      "Epoch: 1768 \tTraining Loss: 11.108305\n",
      "Epoch: 1769 \tTraining Loss: 11.108304\n",
      "Epoch: 1770 \tTraining Loss: 11.108304\n",
      "Epoch: 1771 \tTraining Loss: 11.108303\n",
      "Epoch: 1772 \tTraining Loss: 11.108303\n",
      "Epoch: 1773 \tTraining Loss: 11.108303\n",
      "Epoch: 1774 \tTraining Loss: 11.108303\n",
      "Epoch: 1775 \tTraining Loss: 11.108302\n",
      "Epoch: 1776 \tTraining Loss: 11.108302\n",
      "Epoch: 1777 \tTraining Loss: 11.108302\n",
      "Epoch: 1778 \tTraining Loss: 11.108302\n",
      "Epoch: 1779 \tTraining Loss: 11.108302\n",
      "Epoch: 1780 \tTraining Loss: 11.108302\n",
      "Epoch: 1781 \tTraining Loss: 11.108301\n",
      "Epoch: 1782 \tTraining Loss: 11.108301\n",
      "Epoch: 1783 \tTraining Loss: 11.108301\n",
      "Epoch: 1784 \tTraining Loss: 11.108301\n",
      "Epoch: 1785 \tTraining Loss: 11.108301\n",
      "Epoch: 1786 \tTraining Loss: 11.108300\n",
      "Epoch: 1787 \tTraining Loss: 11.108301\n",
      "Epoch: 1788 \tTraining Loss: 11.108300\n",
      "Epoch: 1789 \tTraining Loss: 11.108300\n",
      "Epoch: 1790 \tTraining Loss: 11.108300\n",
      "Epoch: 1791 \tTraining Loss: 11.108299\n",
      "Epoch: 1792 \tTraining Loss: 11.108299\n",
      "Epoch: 1793 \tTraining Loss: 11.108298\n",
      "Epoch: 1794 \tTraining Loss: 11.108298\n",
      "Epoch: 1795 \tTraining Loss: 11.108298\n",
      "Epoch: 1796 \tTraining Loss: 11.108298\n",
      "Epoch: 1797 \tTraining Loss: 11.108298\n",
      "Epoch: 1798 \tTraining Loss: 11.108297\n",
      "Epoch: 1799 \tTraining Loss: 11.108296\n",
      "Epoch: 1800 \tTraining Loss: 11.108296\n",
      "Epoch: 1801 \tTraining Loss: 11.108296\n",
      "Epoch: 1802 \tTraining Loss: 11.108296\n",
      "Epoch: 1803 \tTraining Loss: 11.108295\n",
      "Epoch: 1804 \tTraining Loss: 11.108296\n",
      "Epoch: 1805 \tTraining Loss: 11.108295\n",
      "Epoch: 1806 \tTraining Loss: 11.108295\n",
      "Epoch: 1807 \tTraining Loss: 11.108295\n",
      "Epoch: 1808 \tTraining Loss: 11.108295\n",
      "Epoch: 1809 \tTraining Loss: 11.108295\n",
      "Epoch: 1810 \tTraining Loss: 11.108295\n",
      "Epoch: 1811 \tTraining Loss: 11.108296\n",
      "Epoch: 1812 \tTraining Loss: 11.108294\n",
      "Epoch: 1813 \tTraining Loss: 11.108294\n",
      "Epoch: 1814 \tTraining Loss: 11.108294\n",
      "Epoch: 1815 \tTraining Loss: 11.108294\n",
      "Epoch: 1816 \tTraining Loss: 11.108292\n",
      "Epoch: 1817 \tTraining Loss: 11.108294\n",
      "Epoch: 1818 \tTraining Loss: 11.108292\n",
      "Epoch: 1819 \tTraining Loss: 11.108292\n",
      "Epoch: 1820 \tTraining Loss: 11.108291\n",
      "Epoch: 1821 \tTraining Loss: 11.108291\n",
      "Epoch: 1822 \tTraining Loss: 11.108291\n",
      "Epoch: 1823 \tTraining Loss: 11.108291\n",
      "Epoch: 1824 \tTraining Loss: 11.108291\n",
      "Epoch: 1825 \tTraining Loss: 11.108291\n",
      "Epoch: 1826 \tTraining Loss: 11.108292\n",
      "Epoch: 1827 \tTraining Loss: 11.108290\n",
      "Epoch: 1828 \tTraining Loss: 11.108291\n",
      "Epoch: 1829 \tTraining Loss: 11.108290\n",
      "Epoch: 1830 \tTraining Loss: 11.108290\n",
      "Epoch: 1831 \tTraining Loss: 11.108291\n",
      "Epoch: 1832 \tTraining Loss: 11.108290\n",
      "Epoch: 1833 \tTraining Loss: 11.108290\n",
      "Epoch: 1834 \tTraining Loss: 11.108289\n",
      "Epoch: 1835 \tTraining Loss: 11.108288\n",
      "Epoch: 1836 \tTraining Loss: 11.108290\n",
      "Epoch: 1837 \tTraining Loss: 11.108289\n",
      "Epoch: 1838 \tTraining Loss: 11.108288\n",
      "Epoch: 1839 \tTraining Loss: 11.108288\n",
      "Epoch: 1840 \tTraining Loss: 11.108287\n",
      "Epoch: 1841 \tTraining Loss: 11.108287\n",
      "Epoch: 1842 \tTraining Loss: 11.108287\n",
      "Epoch: 1843 \tTraining Loss: 11.108287\n",
      "Epoch: 1844 \tTraining Loss: 11.108288\n",
      "Epoch: 1845 \tTraining Loss: 11.108287\n",
      "Epoch: 1846 \tTraining Loss: 11.108287\n",
      "Epoch: 1847 \tTraining Loss: 11.108287\n",
      "Epoch: 1848 \tTraining Loss: 11.108287\n",
      "Epoch: 1849 \tTraining Loss: 11.108286\n",
      "Epoch: 1850 \tTraining Loss: 11.108286\n",
      "Epoch: 1851 \tTraining Loss: 11.108286\n",
      "Epoch: 1852 \tTraining Loss: 11.108285\n",
      "Epoch: 1853 \tTraining Loss: 11.108286\n",
      "Epoch: 1854 \tTraining Loss: 11.108284\n",
      "Epoch: 1855 \tTraining Loss: 11.108284\n",
      "Epoch: 1856 \tTraining Loss: 11.108284\n",
      "Epoch: 1857 \tTraining Loss: 11.108284\n",
      "Epoch: 1858 \tTraining Loss: 11.108284\n",
      "Epoch: 1859 \tTraining Loss: 11.108284\n",
      "Epoch: 1860 \tTraining Loss: 11.108284\n",
      "Epoch: 1861 \tTraining Loss: 11.108284\n",
      "Epoch: 1862 \tTraining Loss: 11.108284\n",
      "Epoch: 1863 \tTraining Loss: 11.108283\n",
      "Epoch: 1864 \tTraining Loss: 11.108284\n",
      "Epoch: 1865 \tTraining Loss: 11.108282\n",
      "Epoch: 1866 \tTraining Loss: 11.108284\n",
      "Epoch: 1867 \tTraining Loss: 11.108283\n",
      "Epoch: 1868 \tTraining Loss: 11.108283\n",
      "Epoch: 1869 \tTraining Loss: 11.108282\n",
      "Epoch: 1870 \tTraining Loss: 11.108283\n",
      "Epoch: 1871 \tTraining Loss: 11.108283\n",
      "Epoch: 1872 \tTraining Loss: 11.108282\n",
      "Epoch: 1873 \tTraining Loss: 11.108282\n",
      "Epoch: 1874 \tTraining Loss: 11.108283\n",
      "Epoch: 1875 \tTraining Loss: 11.108282\n",
      "Epoch: 1876 \tTraining Loss: 11.108282\n",
      "Epoch: 1877 \tTraining Loss: 11.108282\n",
      "Epoch: 1878 \tTraining Loss: 11.108282\n",
      "Epoch: 1879 \tTraining Loss: 11.108282\n",
      "Epoch: 1880 \tTraining Loss: 11.108281\n",
      "Epoch: 1881 \tTraining Loss: 11.108280\n",
      "Epoch: 1882 \tTraining Loss: 11.108280\n",
      "Epoch: 1883 \tTraining Loss: 11.108279\n",
      "Epoch: 1884 \tTraining Loss: 11.108278\n",
      "Epoch: 1885 \tTraining Loss: 11.108278\n",
      "Epoch: 1886 \tTraining Loss: 11.108278\n",
      "Epoch: 1887 \tTraining Loss: 11.108278\n",
      "Epoch: 1888 \tTraining Loss: 11.108278\n",
      "Epoch: 1889 \tTraining Loss: 11.108278\n",
      "Epoch: 1890 \tTraining Loss: 11.108278\n",
      "Epoch: 1891 \tTraining Loss: 11.108278\n",
      "Epoch: 1892 \tTraining Loss: 11.108278\n",
      "Epoch: 1893 \tTraining Loss: 11.108278\n",
      "Epoch: 1894 \tTraining Loss: 11.108278\n",
      "Epoch: 1895 \tTraining Loss: 11.108277\n",
      "Epoch: 1896 \tTraining Loss: 11.108277\n",
      "Epoch: 1897 \tTraining Loss: 11.108277\n",
      "Epoch: 1898 \tTraining Loss: 11.108277\n",
      "Epoch: 1899 \tTraining Loss: 11.108276\n",
      "Epoch: 1900 \tTraining Loss: 11.108276\n",
      "Epoch: 1901 \tTraining Loss: 11.108275\n",
      "Epoch: 1902 \tTraining Loss: 11.108275\n",
      "Epoch: 1903 \tTraining Loss: 11.108274\n",
      "Epoch: 1904 \tTraining Loss: 11.108274\n",
      "Epoch: 1905 \tTraining Loss: 11.108274\n",
      "Epoch: 1906 \tTraining Loss: 11.108274\n",
      "Epoch: 1907 \tTraining Loss: 11.108274\n",
      "Epoch: 1908 \tTraining Loss: 11.108274\n",
      "Epoch: 1909 \tTraining Loss: 11.108274\n",
      "Epoch: 1910 \tTraining Loss: 11.108274\n",
      "Epoch: 1911 \tTraining Loss: 11.108273\n",
      "Epoch: 1912 \tTraining Loss: 11.108273\n",
      "Epoch: 1913 \tTraining Loss: 11.108273\n",
      "Epoch: 1914 \tTraining Loss: 11.108273\n",
      "Epoch: 1915 \tTraining Loss: 11.108273\n",
      "Epoch: 1916 \tTraining Loss: 11.108273\n",
      "Epoch: 1917 \tTraining Loss: 11.108273\n",
      "Epoch: 1918 \tTraining Loss: 11.108273\n",
      "Epoch: 1919 \tTraining Loss: 11.108273\n",
      "Epoch: 1920 \tTraining Loss: 11.108273\n",
      "Epoch: 1921 \tTraining Loss: 11.108272\n",
      "Epoch: 1922 \tTraining Loss: 11.108271\n",
      "Epoch: 1923 \tTraining Loss: 11.108271\n",
      "Epoch: 1924 \tTraining Loss: 11.108271\n",
      "Epoch: 1925 \tTraining Loss: 11.108270\n",
      "Epoch: 1926 \tTraining Loss: 11.108270\n",
      "Epoch: 1927 \tTraining Loss: 11.108270\n",
      "Epoch: 1928 \tTraining Loss: 11.108270\n",
      "Epoch: 1929 \tTraining Loss: 11.108271\n",
      "Epoch: 1930 \tTraining Loss: 11.108270\n",
      "Epoch: 1931 \tTraining Loss: 11.108270\n",
      "Epoch: 1932 \tTraining Loss: 11.108269\n",
      "Epoch: 1933 \tTraining Loss: 11.108269\n",
      "Epoch: 1934 \tTraining Loss: 11.108269\n",
      "Epoch: 1935 \tTraining Loss: 11.108269\n",
      "Epoch: 1936 \tTraining Loss: 11.108269\n",
      "Epoch: 1937 \tTraining Loss: 11.108269\n",
      "Epoch: 1938 \tTraining Loss: 11.108269\n",
      "Epoch: 1939 \tTraining Loss: 11.108268\n",
      "Epoch: 1940 \tTraining Loss: 11.108268\n",
      "Epoch: 1941 \tTraining Loss: 11.108267\n",
      "Epoch: 1942 \tTraining Loss: 11.108267\n",
      "Epoch: 1943 \tTraining Loss: 11.108267\n",
      "Epoch: 1944 \tTraining Loss: 11.108267\n",
      "Epoch: 1945 \tTraining Loss: 11.108267\n",
      "Epoch: 1946 \tTraining Loss: 11.108266\n",
      "Epoch: 1947 \tTraining Loss: 11.108266\n",
      "Epoch: 1948 \tTraining Loss: 11.108266\n",
      "Epoch: 1949 \tTraining Loss: 11.108265\n",
      "Epoch: 1950 \tTraining Loss: 11.108264\n",
      "Epoch: 1951 \tTraining Loss: 11.108265\n",
      "Epoch: 1952 \tTraining Loss: 11.108264\n",
      "Epoch: 1953 \tTraining Loss: 11.108264\n",
      "Epoch: 1954 \tTraining Loss: 11.108264\n",
      "Epoch: 1955 \tTraining Loss: 11.108265\n",
      "Epoch: 1956 \tTraining Loss: 11.108265\n",
      "Epoch: 1957 \tTraining Loss: 11.108263\n",
      "Epoch: 1958 \tTraining Loss: 11.108263\n",
      "Epoch: 1959 \tTraining Loss: 11.108263\n",
      "Epoch: 1960 \tTraining Loss: 11.108263\n",
      "Epoch: 1961 \tTraining Loss: 11.108262\n",
      "Epoch: 1962 \tTraining Loss: 11.108261\n",
      "Epoch: 1963 \tTraining Loss: 11.108261\n",
      "Epoch: 1964 \tTraining Loss: 11.108261\n",
      "Epoch: 1965 \tTraining Loss: 11.108261\n",
      "Epoch: 1966 \tTraining Loss: 11.108261\n",
      "Epoch: 1967 \tTraining Loss: 11.108259\n",
      "Epoch: 1968 \tTraining Loss: 11.108259\n",
      "Epoch: 1969 \tTraining Loss: 11.108259\n",
      "Epoch: 1970 \tTraining Loss: 11.108259\n",
      "Epoch: 1971 \tTraining Loss: 11.108259\n",
      "Epoch: 1972 \tTraining Loss: 11.108258\n",
      "Epoch: 1973 \tTraining Loss: 11.108258\n",
      "Epoch: 1974 \tTraining Loss: 11.108258\n",
      "Epoch: 1975 \tTraining Loss: 11.108257\n",
      "Epoch: 1976 \tTraining Loss: 11.108257\n",
      "Epoch: 1977 \tTraining Loss: 11.108257\n",
      "Epoch: 1978 \tTraining Loss: 11.108257\n",
      "Epoch: 1979 \tTraining Loss: 11.108257\n",
      "Epoch: 1980 \tTraining Loss: 11.108257\n",
      "Epoch: 1981 \tTraining Loss: 11.108257\n",
      "Epoch: 1982 \tTraining Loss: 11.108257\n",
      "Epoch: 1983 \tTraining Loss: 11.108257\n",
      "Epoch: 1984 \tTraining Loss: 11.108257\n",
      "Epoch: 1985 \tTraining Loss: 11.108257\n",
      "Epoch: 1986 \tTraining Loss: 11.108255\n",
      "Epoch: 1987 \tTraining Loss: 11.108255\n",
      "Epoch: 1988 \tTraining Loss: 11.108255\n",
      "Epoch: 1989 \tTraining Loss: 11.108255\n",
      "Epoch: 1990 \tTraining Loss: 11.108255\n",
      "Epoch: 1991 \tTraining Loss: 11.108255\n",
      "Epoch: 1992 \tTraining Loss: 11.108255\n",
      "Epoch: 1993 \tTraining Loss: 11.108254\n",
      "Epoch: 1994 \tTraining Loss: 11.108254\n",
      "Epoch: 1995 \tTraining Loss: 11.108254\n",
      "Epoch: 1996 \tTraining Loss: 11.108253\n",
      "Epoch: 1997 \tTraining Loss: 11.108254\n",
      "Epoch: 1998 \tTraining Loss: 11.108254\n",
      "Epoch: 1999 \tTraining Loss: 11.108254\n",
      "Epoch: 2000 \tTraining Loss: 11.108254\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000 # the number of epochs can be tuned for better performance\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(teacher.parameters(), lr=0.01)\n",
    "teacher.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # train the model \n",
    "    for idx, (data, labels) in enumerate(train_loader):\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = teacher(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss\n",
    "\n",
    "        # ------------------\n",
    "\n",
    "    # print the mean squared loss of the training dataset normalized by the mean square of the training dataset labels\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss / torch.mean(torch.pow(y_train, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of the trained model\n",
    "\n",
    "teacher.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "for idx, (data, labels) in enumerate(test_loader):\n",
    "    # forward pass\n",
    "    output = teacher(data)\n",
    "    test_loss += criterion(output, labels).item()\n",
    "\n",
    "# print the mean squared loss of the test dataset normalized by the mean square of the test  labels\n",
    "print('Average mean squared error {:.6f}'.format(test_loss / torch.mean(torch.pow(y_test, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Vary the width parameter, and plot the test error for different widths (5 points)\n",
    "\n",
    "1. How does the test error vary as we change the width? In particular, consider varying the width of the student network from 1 to 20.\n",
    "2. [Bonus] What happens if we vary the sample size?\n",
    "3. [Bonus] How about adding a small amount of noise to the labels of the training dataset?\n",
    "\n",
    "Report what you found and include the results in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000 # the number of epochs can be tuned for better performance\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_net(w):\n",
    "    std_net = Net(d_input,w)\n",
    "    std_net.train() # prep model for training\n",
    "    optimizer = torch.optim.SGD(std_net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # train the model \n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = std_net(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output,labels)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss\n",
    "            \n",
    "            t_loss = train_loss / torch.mean(torch.pow(y_train, 2))\n",
    "\n",
    "            # ------------------\n",
    "        \n",
    "    # Test the performance of the trained model\n",
    "    std_net.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for idx, (data, labels) in enumerate(test_loader):\n",
    "        # forward pass\n",
    "        output = std_net(data)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "    # print the mean squared loss of the test dataset normalized by the mean square of the test  labels\n",
    "    print('Average mean squared error {:.6f}'.format(test_loss / torch.mean(torch.pow(y_test, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width: 1\n",
      "Average mean squared error 10.930061\n",
      "width: 2\n",
      "Average mean squared error 10.916376\n",
      "width: 3\n",
      "Average mean squared error 10.914468\n",
      "width: 4\n",
      "Average mean squared error 10.910889\n",
      "width: 5\n",
      "Average mean squared error 10.913124\n",
      "width: 6\n",
      "Average mean squared error 10.910731\n",
      "width: 7\n",
      "Average mean squared error 10.910593\n",
      "width: 8\n",
      "Average mean squared error 10.910300\n",
      "width: 9\n",
      "Average mean squared error 10.910397\n",
      "width: 10\n",
      "Average mean squared error 10.910455\n",
      "width: 11\n",
      "Average mean squared error 10.910155\n",
      "width: 12\n",
      "Average mean squared error 10.910148\n",
      "width: 13\n",
      "Average mean squared error 10.909992\n",
      "width: 14\n",
      "Average mean squared error 10.909336\n",
      "width: 15\n",
      "Average mean squared error 10.910934\n",
      "width: 16\n",
      "Average mean squared error 10.909252\n",
      "width: 17\n",
      "Average mean squared error 10.909606\n",
      "width: 18\n",
      "Average mean squared error 10.909761\n",
      "width: 19\n",
      "Average mean squared error 10.908634\n",
      "width: 20\n",
      "Average mean squared error 10.908721\n"
     ]
    }
   ],
   "source": [
    "test_error = []\n",
    "for w in range(1,21):\n",
    "    print(\"width:\",w)\n",
    "    test_error.append(student_net(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Varying Sample size\n",
    "Having width as 20 and changing the Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "N1 = 5 * 20 * d_input\n",
    "\n",
    "# random data from standard normal distribution\n",
    "x_train = torch.randn(N1, d_input)\n",
    "x_test = torch.randn(N1, d_input)\n",
    "\n",
    "# teacher network with random weights\n",
    "teacher = Net(d_input, width)\n",
    "\n",
    "# generate labels using the teacher network\n",
    "y_train = torch.FloatTensor([teacher.forward(x) for x in x_train])\n",
    "y_test = torch.FloatTensor([teacher.forward(x) for x in x_test])\n",
    "\n",
    "# combine the data and labels into pytorch friendly format\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_net(w):\n",
    "    std_net = Net(d_input,w)\n",
    "    std_net.train() # prep model for training\n",
    "    optimizer = torch.optim.SGD(std_net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # train the model \n",
    "        for idx, (data, labels) in enumerate(train_loader):\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = std_net(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output,labels)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss\n",
    "            \n",
    "            t_loss = train_loss / torch.mean(torch.pow(y_train, 2))\n",
    "\n",
    "            # ------------------\n",
    "        \n",
    "    # Test the performance of the trained model\n",
    "    std_net.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    for idx, (data, labels) in enumerate(test_loader):\n",
    "        # forward pass\n",
    "        output = std_net(data)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "    # print the mean squared loss of the test dataset normalized by the mean square of the test  labels\n",
    "    print('Average mean squared error {:.6f}'.format(test_loss / torch.mean(torch.pow(y_test, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean squared error 87.164070\n"
     ]
    }
   ],
   "source": [
    "test_error = []\n",
    "test_error.append(student_net(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying sample size is increasing the *mean squared error*. However we can try for different vlaues of *Width* and *d_input* to experiment what actually happens for smaller to larger values of *N(Sample Size)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "N = 5 * width * d_input\n",
    "\n",
    "# random data from standard normal distribution\n",
    "x_train = torch.randn(N, d_input)\n",
    "x_test = torch.randn(N, d_input)\n",
    "\n",
    "# teacher network with random weights\n",
    "teacher = Net(d_input, width)\n",
    "\n",
    "# generate labels using the teacher network\n",
    "y_train = torch.FloatTensor([teacher.forward(x) for x in x_train])\n",
    "y_test = torch.FloatTensor([teacher.forward(x) for x in x_test])\n",
    "\n",
    "# combine the data and labels into pytorch friendly format\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    AddGaussianNoise(0., 1.)\n",
    "])\n",
    "\n",
    "\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 47.371262\n",
      "Epoch: 2 \tTraining Loss: 43.498249\n",
      "Epoch: 3 \tTraining Loss: 40.663643\n",
      "Epoch: 4 \tTraining Loss: 38.483887\n",
      "Epoch: 5 \tTraining Loss: 36.756279\n",
      "Epoch: 6 \tTraining Loss: 35.356884\n",
      "Epoch: 7 \tTraining Loss: 34.207561\n",
      "Epoch: 8 \tTraining Loss: 33.250862\n",
      "Epoch: 9 \tTraining Loss: 32.443417\n",
      "Epoch: 10 \tTraining Loss: 31.755939\n",
      "Epoch: 11 \tTraining Loss: 31.163343\n",
      "Epoch: 12 \tTraining Loss: 30.647829\n",
      "Epoch: 13 \tTraining Loss: 30.195847\n",
      "Epoch: 14 \tTraining Loss: 29.796682\n",
      "Epoch: 15 \tTraining Loss: 29.442566\n",
      "Epoch: 16 \tTraining Loss: 29.126474\n",
      "Epoch: 17 \tTraining Loss: 28.843119\n",
      "Epoch: 18 \tTraining Loss: 28.588062\n",
      "Epoch: 19 \tTraining Loss: 28.357546\n",
      "Epoch: 20 \tTraining Loss: 28.148333\n",
      "Epoch: 21 \tTraining Loss: 27.957737\n",
      "Epoch: 22 \tTraining Loss: 27.783419\n",
      "Epoch: 23 \tTraining Loss: 27.623901\n",
      "Epoch: 24 \tTraining Loss: 27.477463\n",
      "Epoch: 25 \tTraining Loss: 27.342577\n",
      "Epoch: 26 \tTraining Loss: 27.218096\n",
      "Epoch: 27 \tTraining Loss: 27.102879\n",
      "Epoch: 28 \tTraining Loss: 26.995913\n",
      "Epoch: 29 \tTraining Loss: 26.896387\n",
      "Epoch: 30 \tTraining Loss: 26.803530\n",
      "Epoch: 31 \tTraining Loss: 26.716892\n",
      "Epoch: 32 \tTraining Loss: 26.635967\n",
      "Epoch: 33 \tTraining Loss: 26.560268\n",
      "Epoch: 34 \tTraining Loss: 26.489351\n",
      "Epoch: 35 \tTraining Loss: 26.422768\n",
      "Epoch: 36 \tTraining Loss: 26.360207\n",
      "Epoch: 37 \tTraining Loss: 26.301355\n",
      "Epoch: 38 \tTraining Loss: 26.245964\n",
      "Epoch: 39 \tTraining Loss: 26.193777\n",
      "Epoch: 40 \tTraining Loss: 26.144487\n",
      "Epoch: 41 \tTraining Loss: 26.097919\n",
      "Epoch: 42 \tTraining Loss: 26.053911\n",
      "Epoch: 43 \tTraining Loss: 26.012234\n",
      "Epoch: 44 \tTraining Loss: 25.972738\n",
      "Epoch: 45 \tTraining Loss: 25.935265\n",
      "Epoch: 46 \tTraining Loss: 25.899687\n",
      "Epoch: 47 \tTraining Loss: 25.865877\n",
      "Epoch: 48 \tTraining Loss: 25.833731\n",
      "Epoch: 49 \tTraining Loss: 25.803123\n",
      "Epoch: 50 \tTraining Loss: 25.773951\n",
      "Epoch: 51 \tTraining Loss: 25.746130\n",
      "Epoch: 52 \tTraining Loss: 25.719576\n",
      "Epoch: 53 \tTraining Loss: 25.694220\n",
      "Epoch: 54 \tTraining Loss: 25.669987\n",
      "Epoch: 55 \tTraining Loss: 25.646818\n",
      "Epoch: 56 \tTraining Loss: 25.624640\n",
      "Epoch: 57 \tTraining Loss: 25.603416\n",
      "Epoch: 58 \tTraining Loss: 25.583088\n",
      "Epoch: 59 \tTraining Loss: 25.563614\n",
      "Epoch: 60 \tTraining Loss: 25.544943\n",
      "Epoch: 61 \tTraining Loss: 25.527035\n",
      "Epoch: 62 \tTraining Loss: 25.509848\n",
      "Epoch: 63 \tTraining Loss: 25.493351\n",
      "Epoch: 64 \tTraining Loss: 25.477514\n",
      "Epoch: 65 \tTraining Loss: 25.462305\n",
      "Epoch: 66 \tTraining Loss: 25.447683\n",
      "Epoch: 67 \tTraining Loss: 25.433622\n",
      "Epoch: 68 \tTraining Loss: 25.420095\n",
      "Epoch: 69 \tTraining Loss: 25.407080\n",
      "Epoch: 70 \tTraining Loss: 25.394545\n",
      "Epoch: 71 \tTraining Loss: 25.382473\n",
      "Epoch: 72 \tTraining Loss: 25.370840\n",
      "Epoch: 73 \tTraining Loss: 25.359625\n",
      "Epoch: 74 \tTraining Loss: 25.348814\n",
      "Epoch: 75 \tTraining Loss: 25.338385\n",
      "Epoch: 76 \tTraining Loss: 25.328327\n",
      "Epoch: 77 \tTraining Loss: 25.318611\n",
      "Epoch: 78 \tTraining Loss: 25.309231\n",
      "Epoch: 79 \tTraining Loss: 25.300169\n",
      "Epoch: 80 \tTraining Loss: 25.291410\n",
      "Epoch: 81 \tTraining Loss: 25.282949\n",
      "Epoch: 82 \tTraining Loss: 25.274765\n",
      "Epoch: 83 \tTraining Loss: 25.266846\n",
      "Epoch: 84 \tTraining Loss: 25.259193\n",
      "Epoch: 85 \tTraining Loss: 25.251780\n",
      "Epoch: 86 \tTraining Loss: 25.244612\n",
      "Epoch: 87 \tTraining Loss: 25.237667\n",
      "Epoch: 88 \tTraining Loss: 25.230940\n",
      "Epoch: 89 \tTraining Loss: 25.224428\n",
      "Epoch: 90 \tTraining Loss: 25.218121\n",
      "Epoch: 91 \tTraining Loss: 25.212002\n",
      "Epoch: 92 \tTraining Loss: 25.206076\n",
      "Epoch: 93 \tTraining Loss: 25.200329\n",
      "Epoch: 94 \tTraining Loss: 25.194757\n",
      "Epoch: 95 \tTraining Loss: 25.189358\n",
      "Epoch: 96 \tTraining Loss: 25.184111\n",
      "Epoch: 97 \tTraining Loss: 25.179026\n",
      "Epoch: 98 \tTraining Loss: 25.174093\n",
      "Epoch: 99 \tTraining Loss: 25.169298\n",
      "Epoch: 100 \tTraining Loss: 25.164644\n",
      "Epoch: 101 \tTraining Loss: 25.160131\n",
      "Epoch: 102 \tTraining Loss: 25.155737\n",
      "Epoch: 103 \tTraining Loss: 25.151474\n",
      "Epoch: 104 \tTraining Loss: 25.147331\n",
      "Epoch: 105 \tTraining Loss: 25.143307\n",
      "Epoch: 106 \tTraining Loss: 25.139395\n",
      "Epoch: 107 \tTraining Loss: 25.135592\n",
      "Epoch: 108 \tTraining Loss: 25.131891\n",
      "Epoch: 109 \tTraining Loss: 25.128290\n",
      "Epoch: 110 \tTraining Loss: 25.124798\n",
      "Epoch: 111 \tTraining Loss: 25.121393\n",
      "Epoch: 112 \tTraining Loss: 25.118082\n",
      "Epoch: 113 \tTraining Loss: 25.114864\n",
      "Epoch: 114 \tTraining Loss: 25.111732\n",
      "Epoch: 115 \tTraining Loss: 25.108686\n",
      "Epoch: 116 \tTraining Loss: 25.105715\n",
      "Epoch: 117 \tTraining Loss: 25.102819\n",
      "Epoch: 118 \tTraining Loss: 25.100010\n",
      "Epoch: 119 \tTraining Loss: 25.097267\n",
      "Epoch: 120 \tTraining Loss: 25.094601\n",
      "Epoch: 121 \tTraining Loss: 25.092003\n",
      "Epoch: 122 \tTraining Loss: 25.089470\n",
      "Epoch: 123 \tTraining Loss: 25.087004\n",
      "Epoch: 124 \tTraining Loss: 25.084602\n",
      "Epoch: 125 \tTraining Loss: 25.082254\n",
      "Epoch: 126 \tTraining Loss: 25.079975\n",
      "Epoch: 127 \tTraining Loss: 25.077755\n",
      "Epoch: 128 \tTraining Loss: 25.075583\n",
      "Epoch: 129 \tTraining Loss: 25.073471\n",
      "Epoch: 130 \tTraining Loss: 25.071409\n",
      "Epoch: 131 \tTraining Loss: 25.069393\n",
      "Epoch: 132 \tTraining Loss: 25.067440\n",
      "Epoch: 133 \tTraining Loss: 25.065527\n",
      "Epoch: 134 \tTraining Loss: 25.063662\n",
      "Epoch: 135 \tTraining Loss: 25.061844\n",
      "Epoch: 136 \tTraining Loss: 25.060072\n",
      "Epoch: 137 \tTraining Loss: 25.058334\n",
      "Epoch: 138 \tTraining Loss: 25.056648\n",
      "Epoch: 139 \tTraining Loss: 25.055002\n",
      "Epoch: 140 \tTraining Loss: 25.053394\n",
      "Epoch: 141 \tTraining Loss: 25.051823\n",
      "Epoch: 142 \tTraining Loss: 25.050291\n",
      "Epoch: 143 \tTraining Loss: 25.048792\n",
      "Epoch: 144 \tTraining Loss: 25.047338\n",
      "Epoch: 145 \tTraining Loss: 25.045914\n",
      "Epoch: 146 \tTraining Loss: 25.044521\n",
      "Epoch: 147 \tTraining Loss: 25.043159\n",
      "Epoch: 148 \tTraining Loss: 25.041832\n",
      "Epoch: 149 \tTraining Loss: 25.040539\n",
      "Epoch: 150 \tTraining Loss: 25.039270\n",
      "Epoch: 151 \tTraining Loss: 25.038033\n",
      "Epoch: 152 \tTraining Loss: 25.036829\n",
      "Epoch: 153 \tTraining Loss: 25.035645\n",
      "Epoch: 154 \tTraining Loss: 25.034492\n",
      "Epoch: 155 \tTraining Loss: 25.033373\n",
      "Epoch: 156 \tTraining Loss: 25.032270\n",
      "Epoch: 157 \tTraining Loss: 25.031195\n",
      "Epoch: 158 \tTraining Loss: 25.030144\n",
      "Epoch: 159 \tTraining Loss: 25.029118\n",
      "Epoch: 160 \tTraining Loss: 25.028114\n",
      "Epoch: 161 \tTraining Loss: 25.027136\n",
      "Epoch: 162 \tTraining Loss: 25.026178\n",
      "Epoch: 163 \tTraining Loss: 25.025240\n",
      "Epoch: 164 \tTraining Loss: 25.024323\n",
      "Epoch: 165 \tTraining Loss: 25.023430\n",
      "Epoch: 166 \tTraining Loss: 25.022556\n",
      "Epoch: 167 \tTraining Loss: 25.021700\n",
      "Epoch: 168 \tTraining Loss: 25.020868\n",
      "Epoch: 169 \tTraining Loss: 25.020050\n",
      "Epoch: 170 \tTraining Loss: 25.019251\n",
      "Epoch: 171 \tTraining Loss: 25.018469\n",
      "Epoch: 172 \tTraining Loss: 25.017706\n",
      "Epoch: 173 \tTraining Loss: 25.016960\n",
      "Epoch: 174 \tTraining Loss: 25.016228\n",
      "Epoch: 175 \tTraining Loss: 25.015511\n",
      "Epoch: 176 \tTraining Loss: 25.014814\n",
      "Epoch: 177 \tTraining Loss: 25.014133\n",
      "Epoch: 178 \tTraining Loss: 25.013462\n",
      "Epoch: 179 \tTraining Loss: 25.012808\n",
      "Epoch: 180 \tTraining Loss: 25.012169\n",
      "Epoch: 181 \tTraining Loss: 25.011543\n",
      "Epoch: 182 \tTraining Loss: 25.010933\n",
      "Epoch: 183 \tTraining Loss: 25.010334\n",
      "Epoch: 184 \tTraining Loss: 25.009747\n",
      "Epoch: 185 \tTraining Loss: 25.009172\n",
      "Epoch: 186 \tTraining Loss: 25.008610\n",
      "Epoch: 187 \tTraining Loss: 25.008064\n",
      "Epoch: 188 \tTraining Loss: 25.007530\n",
      "Epoch: 189 \tTraining Loss: 25.007002\n",
      "Epoch: 190 \tTraining Loss: 25.006487\n",
      "Epoch: 191 \tTraining Loss: 25.005985\n",
      "Epoch: 192 \tTraining Loss: 25.005491\n",
      "Epoch: 193 \tTraining Loss: 25.005014\n",
      "Epoch: 194 \tTraining Loss: 25.004543\n",
      "Epoch: 195 \tTraining Loss: 25.004082\n",
      "Epoch: 196 \tTraining Loss: 25.003628\n",
      "Epoch: 197 \tTraining Loss: 25.003185\n",
      "Epoch: 198 \tTraining Loss: 25.002752\n",
      "Epoch: 199 \tTraining Loss: 25.002329\n",
      "Epoch: 200 \tTraining Loss: 25.001917\n",
      "Epoch: 201 \tTraining Loss: 25.001511\n",
      "Epoch: 202 \tTraining Loss: 25.001110\n",
      "Epoch: 203 \tTraining Loss: 25.000725\n",
      "Epoch: 204 \tTraining Loss: 25.000343\n",
      "Epoch: 205 \tTraining Loss: 24.999971\n",
      "Epoch: 206 \tTraining Loss: 24.999607\n",
      "Epoch: 207 \tTraining Loss: 24.999250\n",
      "Epoch: 208 \tTraining Loss: 24.998901\n",
      "Epoch: 209 \tTraining Loss: 24.998558\n",
      "Epoch: 210 \tTraining Loss: 24.998220\n",
      "Epoch: 211 \tTraining Loss: 24.997896\n",
      "Epoch: 212 \tTraining Loss: 24.997576\n",
      "Epoch: 213 \tTraining Loss: 24.997259\n",
      "Epoch: 214 \tTraining Loss: 24.996952\n",
      "Epoch: 215 \tTraining Loss: 24.996647\n",
      "Epoch: 216 \tTraining Loss: 24.996355\n",
      "Epoch: 217 \tTraining Loss: 24.996065\n",
      "Epoch: 218 \tTraining Loss: 24.995781\n",
      "Epoch: 219 \tTraining Loss: 24.995504\n",
      "Epoch: 220 \tTraining Loss: 24.995234\n",
      "Epoch: 221 \tTraining Loss: 24.994967\n",
      "Epoch: 222 \tTraining Loss: 24.994709\n",
      "Epoch: 223 \tTraining Loss: 24.994453\n",
      "Epoch: 224 \tTraining Loss: 24.994200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 \tTraining Loss: 24.993959\n",
      "Epoch: 226 \tTraining Loss: 24.993719\n",
      "Epoch: 227 \tTraining Loss: 24.993484\n",
      "Epoch: 228 \tTraining Loss: 24.993254\n",
      "Epoch: 229 \tTraining Loss: 24.993029\n",
      "Epoch: 230 \tTraining Loss: 24.992807\n",
      "Epoch: 231 \tTraining Loss: 24.992594\n",
      "Epoch: 232 \tTraining Loss: 24.992376\n",
      "Epoch: 233 \tTraining Loss: 24.992174\n",
      "Epoch: 234 \tTraining Loss: 24.991972\n",
      "Epoch: 235 \tTraining Loss: 24.991772\n",
      "Epoch: 236 \tTraining Loss: 24.991575\n",
      "Epoch: 237 \tTraining Loss: 24.991386\n",
      "Epoch: 238 \tTraining Loss: 24.991199\n",
      "Epoch: 239 \tTraining Loss: 24.991013\n",
      "Epoch: 240 \tTraining Loss: 24.990837\n",
      "Epoch: 241 \tTraining Loss: 24.990662\n",
      "Epoch: 242 \tTraining Loss: 24.990490\n",
      "Epoch: 243 \tTraining Loss: 24.990324\n",
      "Epoch: 244 \tTraining Loss: 24.990154\n",
      "Epoch: 245 \tTraining Loss: 24.989992\n",
      "Epoch: 246 \tTraining Loss: 24.989836\n",
      "Epoch: 247 \tTraining Loss: 24.989676\n",
      "Epoch: 248 \tTraining Loss: 24.989532\n",
      "Epoch: 249 \tTraining Loss: 24.989382\n",
      "Epoch: 250 \tTraining Loss: 24.989235\n",
      "Epoch: 251 \tTraining Loss: 24.989084\n",
      "Epoch: 252 \tTraining Loss: 24.988951\n",
      "Epoch: 253 \tTraining Loss: 24.988817\n",
      "Epoch: 254 \tTraining Loss: 24.988680\n",
      "Epoch: 255 \tTraining Loss: 24.988546\n",
      "Epoch: 256 \tTraining Loss: 24.988419\n",
      "Epoch: 257 \tTraining Loss: 24.988295\n",
      "Epoch: 258 \tTraining Loss: 24.988165\n",
      "Epoch: 259 \tTraining Loss: 24.988045\n",
      "Epoch: 260 \tTraining Loss: 24.987926\n",
      "Epoch: 261 \tTraining Loss: 24.987810\n",
      "Epoch: 262 \tTraining Loss: 24.987692\n",
      "Epoch: 263 \tTraining Loss: 24.987585\n",
      "Epoch: 264 \tTraining Loss: 24.987474\n",
      "Epoch: 265 \tTraining Loss: 24.987368\n",
      "Epoch: 266 \tTraining Loss: 24.987265\n",
      "Epoch: 267 \tTraining Loss: 24.987160\n",
      "Epoch: 268 \tTraining Loss: 24.987057\n",
      "Epoch: 269 \tTraining Loss: 24.986958\n",
      "Epoch: 270 \tTraining Loss: 24.986860\n",
      "Epoch: 271 \tTraining Loss: 24.986769\n",
      "Epoch: 272 \tTraining Loss: 24.986673\n",
      "Epoch: 273 \tTraining Loss: 24.986582\n",
      "Epoch: 274 \tTraining Loss: 24.986492\n",
      "Epoch: 275 \tTraining Loss: 24.986406\n",
      "Epoch: 276 \tTraining Loss: 24.986322\n",
      "Epoch: 277 \tTraining Loss: 24.986233\n",
      "Epoch: 278 \tTraining Loss: 24.986151\n",
      "Epoch: 279 \tTraining Loss: 24.986073\n",
      "Epoch: 280 \tTraining Loss: 24.985989\n",
      "Epoch: 281 \tTraining Loss: 24.985912\n",
      "Epoch: 282 \tTraining Loss: 24.985838\n",
      "Epoch: 283 \tTraining Loss: 24.985762\n",
      "Epoch: 284 \tTraining Loss: 24.985689\n",
      "Epoch: 285 \tTraining Loss: 24.985619\n",
      "Epoch: 286 \tTraining Loss: 24.985550\n",
      "Epoch: 287 \tTraining Loss: 24.985483\n",
      "Epoch: 288 \tTraining Loss: 24.985413\n",
      "Epoch: 289 \tTraining Loss: 24.985348\n",
      "Epoch: 290 \tTraining Loss: 24.985279\n",
      "Epoch: 291 \tTraining Loss: 24.985218\n",
      "Epoch: 292 \tTraining Loss: 24.985155\n",
      "Epoch: 293 \tTraining Loss: 24.985090\n",
      "Epoch: 294 \tTraining Loss: 24.985031\n",
      "Epoch: 295 \tTraining Loss: 24.984976\n",
      "Epoch: 296 \tTraining Loss: 24.984917\n",
      "Epoch: 297 \tTraining Loss: 24.984858\n",
      "Epoch: 298 \tTraining Loss: 24.984804\n",
      "Epoch: 299 \tTraining Loss: 24.984753\n",
      "Epoch: 300 \tTraining Loss: 24.984701\n",
      "Epoch: 301 \tTraining Loss: 24.984648\n",
      "Epoch: 302 \tTraining Loss: 24.984592\n",
      "Epoch: 303 \tTraining Loss: 24.984547\n",
      "Epoch: 304 \tTraining Loss: 24.984499\n",
      "Epoch: 305 \tTraining Loss: 24.984446\n",
      "Epoch: 306 \tTraining Loss: 24.984404\n",
      "Epoch: 307 \tTraining Loss: 24.984354\n",
      "Epoch: 308 \tTraining Loss: 24.984310\n",
      "Epoch: 309 \tTraining Loss: 24.984264\n",
      "Epoch: 310 \tTraining Loss: 24.984222\n",
      "Epoch: 311 \tTraining Loss: 24.984179\n",
      "Epoch: 312 \tTraining Loss: 24.984137\n",
      "Epoch: 313 \tTraining Loss: 24.984098\n",
      "Epoch: 314 \tTraining Loss: 24.984055\n",
      "Epoch: 315 \tTraining Loss: 24.984015\n",
      "Epoch: 316 \tTraining Loss: 24.983978\n",
      "Epoch: 317 \tTraining Loss: 24.983942\n",
      "Epoch: 318 \tTraining Loss: 24.983908\n",
      "Epoch: 319 \tTraining Loss: 24.983871\n",
      "Epoch: 320 \tTraining Loss: 24.983833\n",
      "Epoch: 321 \tTraining Loss: 24.983797\n",
      "Epoch: 322 \tTraining Loss: 24.983763\n",
      "Epoch: 323 \tTraining Loss: 24.983728\n",
      "Epoch: 324 \tTraining Loss: 24.983698\n",
      "Epoch: 325 \tTraining Loss: 24.983665\n",
      "Epoch: 326 \tTraining Loss: 24.983635\n",
      "Epoch: 327 \tTraining Loss: 24.983603\n",
      "Epoch: 328 \tTraining Loss: 24.983574\n",
      "Epoch: 329 \tTraining Loss: 24.983545\n",
      "Epoch: 330 \tTraining Loss: 24.983513\n",
      "Epoch: 331 \tTraining Loss: 24.983488\n",
      "Epoch: 332 \tTraining Loss: 24.983456\n",
      "Epoch: 333 \tTraining Loss: 24.983427\n",
      "Epoch: 334 \tTraining Loss: 24.983406\n",
      "Epoch: 335 \tTraining Loss: 24.983374\n",
      "Epoch: 336 \tTraining Loss: 24.983353\n",
      "Epoch: 337 \tTraining Loss: 24.983328\n",
      "Epoch: 338 \tTraining Loss: 24.983303\n",
      "Epoch: 339 \tTraining Loss: 24.983276\n",
      "Epoch: 340 \tTraining Loss: 24.983253\n",
      "Epoch: 341 \tTraining Loss: 24.983225\n",
      "Epoch: 342 \tTraining Loss: 24.983204\n",
      "Epoch: 343 \tTraining Loss: 24.983179\n",
      "Epoch: 344 \tTraining Loss: 24.983164\n",
      "Epoch: 345 \tTraining Loss: 24.983143\n",
      "Epoch: 346 \tTraining Loss: 24.983116\n",
      "Epoch: 347 \tTraining Loss: 24.983099\n",
      "Epoch: 348 \tTraining Loss: 24.983072\n",
      "Epoch: 349 \tTraining Loss: 24.983057\n",
      "Epoch: 350 \tTraining Loss: 24.983038\n",
      "Epoch: 351 \tTraining Loss: 24.983017\n",
      "Epoch: 352 \tTraining Loss: 24.982998\n",
      "Epoch: 353 \tTraining Loss: 24.982986\n",
      "Epoch: 354 \tTraining Loss: 24.982965\n",
      "Epoch: 355 \tTraining Loss: 24.982946\n",
      "Epoch: 356 \tTraining Loss: 24.982927\n",
      "Epoch: 357 \tTraining Loss: 24.982912\n",
      "Epoch: 358 \tTraining Loss: 24.982891\n",
      "Epoch: 359 \tTraining Loss: 24.982872\n",
      "Epoch: 360 \tTraining Loss: 24.982862\n",
      "Epoch: 361 \tTraining Loss: 24.982847\n",
      "Epoch: 362 \tTraining Loss: 24.982826\n",
      "Epoch: 363 \tTraining Loss: 24.982815\n",
      "Epoch: 364 \tTraining Loss: 24.982801\n",
      "Epoch: 365 \tTraining Loss: 24.982786\n",
      "Epoch: 366 \tTraining Loss: 24.982773\n",
      "Epoch: 367 \tTraining Loss: 24.982759\n",
      "Epoch: 368 \tTraining Loss: 24.982746\n",
      "Epoch: 369 \tTraining Loss: 24.982729\n",
      "Epoch: 370 \tTraining Loss: 24.982718\n",
      "Epoch: 371 \tTraining Loss: 24.982702\n",
      "Epoch: 372 \tTraining Loss: 24.982691\n",
      "Epoch: 373 \tTraining Loss: 24.982677\n",
      "Epoch: 374 \tTraining Loss: 24.982666\n",
      "Epoch: 375 \tTraining Loss: 24.982656\n",
      "Epoch: 376 \tTraining Loss: 24.982641\n",
      "Epoch: 377 \tTraining Loss: 24.982628\n",
      "Epoch: 378 \tTraining Loss: 24.982624\n",
      "Epoch: 379 \tTraining Loss: 24.982607\n",
      "Epoch: 380 \tTraining Loss: 24.982594\n",
      "Epoch: 381 \tTraining Loss: 24.982586\n",
      "Epoch: 382 \tTraining Loss: 24.982573\n",
      "Epoch: 383 \tTraining Loss: 24.982565\n",
      "Epoch: 384 \tTraining Loss: 24.982557\n",
      "Epoch: 385 \tTraining Loss: 24.982544\n",
      "Epoch: 386 \tTraining Loss: 24.982536\n",
      "Epoch: 387 \tTraining Loss: 24.982527\n",
      "Epoch: 388 \tTraining Loss: 24.982517\n",
      "Epoch: 389 \tTraining Loss: 24.982510\n",
      "Epoch: 390 \tTraining Loss: 24.982496\n",
      "Epoch: 391 \tTraining Loss: 24.982489\n",
      "Epoch: 392 \tTraining Loss: 24.982479\n",
      "Epoch: 393 \tTraining Loss: 24.982475\n",
      "Epoch: 394 \tTraining Loss: 24.982466\n",
      "Epoch: 395 \tTraining Loss: 24.982458\n",
      "Epoch: 396 \tTraining Loss: 24.982450\n",
      "Epoch: 397 \tTraining Loss: 24.982439\n",
      "Epoch: 398 \tTraining Loss: 24.982433\n",
      "Epoch: 399 \tTraining Loss: 24.982426\n",
      "Epoch: 400 \tTraining Loss: 24.982418\n",
      "Epoch: 401 \tTraining Loss: 24.982412\n",
      "Epoch: 402 \tTraining Loss: 24.982399\n",
      "Epoch: 403 \tTraining Loss: 24.982395\n",
      "Epoch: 404 \tTraining Loss: 24.982389\n",
      "Epoch: 405 \tTraining Loss: 24.982384\n",
      "Epoch: 406 \tTraining Loss: 24.982374\n",
      "Epoch: 407 \tTraining Loss: 24.982368\n",
      "Epoch: 408 \tTraining Loss: 24.982363\n",
      "Epoch: 409 \tTraining Loss: 24.982355\n",
      "Epoch: 410 \tTraining Loss: 24.982353\n",
      "Epoch: 411 \tTraining Loss: 24.982347\n",
      "Epoch: 412 \tTraining Loss: 24.982336\n",
      "Epoch: 413 \tTraining Loss: 24.982332\n",
      "Epoch: 414 \tTraining Loss: 24.982327\n",
      "Epoch: 415 \tTraining Loss: 24.982325\n",
      "Epoch: 416 \tTraining Loss: 24.982315\n",
      "Epoch: 417 \tTraining Loss: 24.982313\n",
      "Epoch: 418 \tTraining Loss: 24.982306\n",
      "Epoch: 419 \tTraining Loss: 24.982302\n",
      "Epoch: 420 \tTraining Loss: 24.982292\n",
      "Epoch: 421 \tTraining Loss: 24.982290\n",
      "Epoch: 422 \tTraining Loss: 24.982285\n",
      "Epoch: 423 \tTraining Loss: 24.982279\n",
      "Epoch: 424 \tTraining Loss: 24.982273\n",
      "Epoch: 425 \tTraining Loss: 24.982271\n",
      "Epoch: 426 \tTraining Loss: 24.982269\n",
      "Epoch: 427 \tTraining Loss: 24.982260\n",
      "Epoch: 428 \tTraining Loss: 24.982258\n",
      "Epoch: 429 \tTraining Loss: 24.982248\n",
      "Epoch: 430 \tTraining Loss: 24.982244\n",
      "Epoch: 431 \tTraining Loss: 24.982248\n",
      "Epoch: 432 \tTraining Loss: 24.982239\n",
      "Epoch: 433 \tTraining Loss: 24.982239\n",
      "Epoch: 434 \tTraining Loss: 24.982229\n",
      "Epoch: 435 \tTraining Loss: 24.982224\n",
      "Epoch: 436 \tTraining Loss: 24.982224\n",
      "Epoch: 437 \tTraining Loss: 24.982218\n",
      "Epoch: 438 \tTraining Loss: 24.982214\n",
      "Epoch: 439 \tTraining Loss: 24.982208\n",
      "Epoch: 440 \tTraining Loss: 24.982210\n",
      "Epoch: 441 \tTraining Loss: 24.982208\n",
      "Epoch: 442 \tTraining Loss: 24.982203\n",
      "Epoch: 443 \tTraining Loss: 24.982197\n",
      "Epoch: 444 \tTraining Loss: 24.982195\n",
      "Epoch: 445 \tTraining Loss: 24.982189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446 \tTraining Loss: 24.982189\n",
      "Epoch: 447 \tTraining Loss: 24.982187\n",
      "Epoch: 448 \tTraining Loss: 24.982180\n",
      "Epoch: 449 \tTraining Loss: 24.982180\n",
      "Epoch: 450 \tTraining Loss: 24.982174\n",
      "Epoch: 451 \tTraining Loss: 24.982168\n",
      "Epoch: 452 \tTraining Loss: 24.982172\n",
      "Epoch: 453 \tTraining Loss: 24.982168\n",
      "Epoch: 454 \tTraining Loss: 24.982162\n",
      "Epoch: 455 \tTraining Loss: 24.982161\n",
      "Epoch: 456 \tTraining Loss: 24.982159\n",
      "Epoch: 457 \tTraining Loss: 24.982155\n",
      "Epoch: 458 \tTraining Loss: 24.982155\n",
      "Epoch: 459 \tTraining Loss: 24.982155\n",
      "Epoch: 460 \tTraining Loss: 24.982151\n",
      "Epoch: 461 \tTraining Loss: 24.982145\n",
      "Epoch: 462 \tTraining Loss: 24.982141\n",
      "Epoch: 463 \tTraining Loss: 24.982140\n",
      "Epoch: 464 \tTraining Loss: 24.982140\n",
      "Epoch: 465 \tTraining Loss: 24.982138\n",
      "Epoch: 466 \tTraining Loss: 24.982134\n",
      "Epoch: 467 \tTraining Loss: 24.982134\n",
      "Epoch: 468 \tTraining Loss: 24.982134\n",
      "Epoch: 469 \tTraining Loss: 24.982124\n",
      "Epoch: 470 \tTraining Loss: 24.982121\n",
      "Epoch: 471 \tTraining Loss: 24.982121\n",
      "Epoch: 472 \tTraining Loss: 24.982119\n",
      "Epoch: 473 \tTraining Loss: 24.982121\n",
      "Epoch: 474 \tTraining Loss: 24.982119\n",
      "Epoch: 475 \tTraining Loss: 24.982113\n",
      "Epoch: 476 \tTraining Loss: 24.982113\n",
      "Epoch: 477 \tTraining Loss: 24.982109\n",
      "Epoch: 478 \tTraining Loss: 24.982109\n",
      "Epoch: 479 \tTraining Loss: 24.982109\n",
      "Epoch: 480 \tTraining Loss: 24.982100\n",
      "Epoch: 481 \tTraining Loss: 24.982100\n",
      "Epoch: 482 \tTraining Loss: 24.982100\n",
      "Epoch: 483 \tTraining Loss: 24.982103\n",
      "Epoch: 484 \tTraining Loss: 24.982098\n",
      "Epoch: 485 \tTraining Loss: 24.982098\n",
      "Epoch: 486 \tTraining Loss: 24.982092\n",
      "Epoch: 487 \tTraining Loss: 24.982092\n",
      "Epoch: 488 \tTraining Loss: 24.982090\n",
      "Epoch: 489 \tTraining Loss: 24.982092\n",
      "Epoch: 490 \tTraining Loss: 24.982092\n",
      "Epoch: 491 \tTraining Loss: 24.982092\n",
      "Epoch: 492 \tTraining Loss: 24.982088\n",
      "Epoch: 493 \tTraining Loss: 24.982088\n",
      "Epoch: 494 \tTraining Loss: 24.982079\n",
      "Epoch: 495 \tTraining Loss: 24.982082\n",
      "Epoch: 496 \tTraining Loss: 24.982077\n",
      "Epoch: 497 \tTraining Loss: 24.982077\n",
      "Epoch: 498 \tTraining Loss: 24.982071\n",
      "Epoch: 499 \tTraining Loss: 24.982071\n",
      "Epoch: 500 \tTraining Loss: 24.982071\n",
      "Epoch: 501 \tTraining Loss: 24.982071\n",
      "Epoch: 502 \tTraining Loss: 24.982075\n",
      "Epoch: 503 \tTraining Loss: 24.982071\n",
      "Epoch: 504 \tTraining Loss: 24.982069\n",
      "Epoch: 505 \tTraining Loss: 24.982067\n",
      "Epoch: 506 \tTraining Loss: 24.982063\n",
      "Epoch: 507 \tTraining Loss: 24.982063\n",
      "Epoch: 508 \tTraining Loss: 24.982063\n",
      "Epoch: 509 \tTraining Loss: 24.982061\n",
      "Epoch: 510 \tTraining Loss: 24.982061\n",
      "Epoch: 511 \tTraining Loss: 24.982061\n",
      "Epoch: 512 \tTraining Loss: 24.982058\n",
      "Epoch: 513 \tTraining Loss: 24.982056\n",
      "Epoch: 514 \tTraining Loss: 24.982056\n",
      "Epoch: 515 \tTraining Loss: 24.982056\n",
      "Epoch: 516 \tTraining Loss: 24.982054\n",
      "Epoch: 517 \tTraining Loss: 24.982054\n",
      "Epoch: 518 \tTraining Loss: 24.982050\n",
      "Epoch: 519 \tTraining Loss: 24.982050\n",
      "Epoch: 520 \tTraining Loss: 24.982050\n",
      "Epoch: 521 \tTraining Loss: 24.982048\n",
      "Epoch: 522 \tTraining Loss: 24.982048\n",
      "Epoch: 523 \tTraining Loss: 24.982046\n",
      "Epoch: 524 \tTraining Loss: 24.982046\n",
      "Epoch: 525 \tTraining Loss: 24.982042\n",
      "Epoch: 526 \tTraining Loss: 24.982040\n",
      "Epoch: 527 \tTraining Loss: 24.982040\n",
      "Epoch: 528 \tTraining Loss: 24.982040\n",
      "Epoch: 529 \tTraining Loss: 24.982042\n",
      "Epoch: 530 \tTraining Loss: 24.982040\n",
      "Epoch: 531 \tTraining Loss: 24.982037\n",
      "Epoch: 532 \tTraining Loss: 24.982037\n",
      "Epoch: 533 \tTraining Loss: 24.982033\n",
      "Epoch: 534 \tTraining Loss: 24.982035\n",
      "Epoch: 535 \tTraining Loss: 24.982033\n",
      "Epoch: 536 \tTraining Loss: 24.982033\n",
      "Epoch: 537 \tTraining Loss: 24.982033\n",
      "Epoch: 538 \tTraining Loss: 24.982033\n",
      "Epoch: 539 \tTraining Loss: 24.982029\n",
      "Epoch: 540 \tTraining Loss: 24.982029\n",
      "Epoch: 541 \tTraining Loss: 24.982029\n",
      "Epoch: 542 \tTraining Loss: 24.982029\n",
      "Epoch: 543 \tTraining Loss: 24.982027\n",
      "Epoch: 544 \tTraining Loss: 24.982027\n",
      "Epoch: 545 \tTraining Loss: 24.982025\n",
      "Epoch: 546 \tTraining Loss: 24.982021\n",
      "Epoch: 547 \tTraining Loss: 24.982021\n",
      "Epoch: 548 \tTraining Loss: 24.982025\n",
      "Epoch: 549 \tTraining Loss: 24.982025\n",
      "Epoch: 550 \tTraining Loss: 24.982019\n",
      "Epoch: 551 \tTraining Loss: 24.982025\n",
      "Epoch: 552 \tTraining Loss: 24.982021\n",
      "Epoch: 553 \tTraining Loss: 24.982019\n",
      "Epoch: 554 \tTraining Loss: 24.982014\n",
      "Epoch: 555 \tTraining Loss: 24.982014\n",
      "Epoch: 556 \tTraining Loss: 24.982012\n",
      "Epoch: 557 \tTraining Loss: 24.982008\n",
      "Epoch: 558 \tTraining Loss: 24.982008\n",
      "Epoch: 559 \tTraining Loss: 24.982008\n",
      "Epoch: 560 \tTraining Loss: 24.982012\n",
      "Epoch: 561 \tTraining Loss: 24.982012\n",
      "Epoch: 562 \tTraining Loss: 24.982012\n",
      "Epoch: 563 \tTraining Loss: 24.982008\n",
      "Epoch: 564 \tTraining Loss: 24.982008\n",
      "Epoch: 565 \tTraining Loss: 24.982008\n",
      "Epoch: 566 \tTraining Loss: 24.982006\n",
      "Epoch: 567 \tTraining Loss: 24.982002\n",
      "Epoch: 568 \tTraining Loss: 24.982006\n",
      "Epoch: 569 \tTraining Loss: 24.982002\n",
      "Epoch: 570 \tTraining Loss: 24.982002\n",
      "Epoch: 571 \tTraining Loss: 24.982006\n",
      "Epoch: 572 \tTraining Loss: 24.982000\n",
      "Epoch: 573 \tTraining Loss: 24.981998\n",
      "Epoch: 574 \tTraining Loss: 24.981998\n",
      "Epoch: 575 \tTraining Loss: 24.982000\n",
      "Epoch: 576 \tTraining Loss: 24.982000\n",
      "Epoch: 577 \tTraining Loss: 24.982000\n",
      "Epoch: 578 \tTraining Loss: 24.981998\n",
      "Epoch: 579 \tTraining Loss: 24.981998\n",
      "Epoch: 580 \tTraining Loss: 24.981998\n",
      "Epoch: 581 \tTraining Loss: 24.981998\n",
      "Epoch: 582 \tTraining Loss: 24.981995\n",
      "Epoch: 583 \tTraining Loss: 24.981995\n",
      "Epoch: 584 \tTraining Loss: 24.981995\n",
      "Epoch: 585 \tTraining Loss: 24.981993\n",
      "Epoch: 586 \tTraining Loss: 24.981995\n",
      "Epoch: 587 \tTraining Loss: 24.981991\n",
      "Epoch: 588 \tTraining Loss: 24.981993\n",
      "Epoch: 589 \tTraining Loss: 24.981991\n",
      "Epoch: 590 \tTraining Loss: 24.981987\n",
      "Epoch: 591 \tTraining Loss: 24.981985\n",
      "Epoch: 592 \tTraining Loss: 24.981987\n",
      "Epoch: 593 \tTraining Loss: 24.981987\n",
      "Epoch: 594 \tTraining Loss: 24.981987\n",
      "Epoch: 595 \tTraining Loss: 24.981985\n",
      "Epoch: 596 \tTraining Loss: 24.981985\n",
      "Epoch: 597 \tTraining Loss: 24.981985\n",
      "Epoch: 598 \tTraining Loss: 24.981985\n",
      "Epoch: 599 \tTraining Loss: 24.981985\n",
      "Epoch: 600 \tTraining Loss: 24.981985\n",
      "Epoch: 601 \tTraining Loss: 24.981985\n",
      "Epoch: 602 \tTraining Loss: 24.981981\n",
      "Epoch: 603 \tTraining Loss: 24.981981\n",
      "Epoch: 604 \tTraining Loss: 24.981979\n",
      "Epoch: 605 \tTraining Loss: 24.981979\n",
      "Epoch: 606 \tTraining Loss: 24.981979\n",
      "Epoch: 607 \tTraining Loss: 24.981979\n",
      "Epoch: 608 \tTraining Loss: 24.981979\n",
      "Epoch: 609 \tTraining Loss: 24.981979\n",
      "Epoch: 610 \tTraining Loss: 24.981979\n",
      "Epoch: 611 \tTraining Loss: 24.981979\n",
      "Epoch: 612 \tTraining Loss: 24.981977\n",
      "Epoch: 613 \tTraining Loss: 24.981979\n",
      "Epoch: 614 \tTraining Loss: 24.981979\n",
      "Epoch: 615 \tTraining Loss: 24.981979\n",
      "Epoch: 616 \tTraining Loss: 24.981977\n",
      "Epoch: 617 \tTraining Loss: 24.981974\n",
      "Epoch: 618 \tTraining Loss: 24.981974\n",
      "Epoch: 619 \tTraining Loss: 24.981974\n",
      "Epoch: 620 \tTraining Loss: 24.981972\n",
      "Epoch: 621 \tTraining Loss: 24.981972\n",
      "Epoch: 622 \tTraining Loss: 24.981970\n",
      "Epoch: 623 \tTraining Loss: 24.981970\n",
      "Epoch: 624 \tTraining Loss: 24.981966\n",
      "Epoch: 625 \tTraining Loss: 24.981966\n",
      "Epoch: 626 \tTraining Loss: 24.981966\n",
      "Epoch: 627 \tTraining Loss: 24.981966\n",
      "Epoch: 628 \tTraining Loss: 24.981964\n",
      "Epoch: 629 \tTraining Loss: 24.981964\n",
      "Epoch: 630 \tTraining Loss: 24.981964\n",
      "Epoch: 631 \tTraining Loss: 24.981964\n",
      "Epoch: 632 \tTraining Loss: 24.981964\n",
      "Epoch: 633 \tTraining Loss: 24.981964\n",
      "Epoch: 634 \tTraining Loss: 24.981964\n",
      "Epoch: 635 \tTraining Loss: 24.981960\n",
      "Epoch: 636 \tTraining Loss: 24.981960\n",
      "Epoch: 637 \tTraining Loss: 24.981960\n",
      "Epoch: 638 \tTraining Loss: 24.981958\n",
      "Epoch: 639 \tTraining Loss: 24.981958\n",
      "Epoch: 640 \tTraining Loss: 24.981956\n",
      "Epoch: 641 \tTraining Loss: 24.981956\n",
      "Epoch: 642 \tTraining Loss: 24.981956\n",
      "Epoch: 643 \tTraining Loss: 24.981956\n",
      "Epoch: 644 \tTraining Loss: 24.981956\n",
      "Epoch: 645 \tTraining Loss: 24.981953\n",
      "Epoch: 646 \tTraining Loss: 24.981956\n",
      "Epoch: 647 \tTraining Loss: 24.981953\n",
      "Epoch: 648 \tTraining Loss: 24.981953\n",
      "Epoch: 649 \tTraining Loss: 24.981953\n",
      "Epoch: 650 \tTraining Loss: 24.981953\n",
      "Epoch: 651 \tTraining Loss: 24.981953\n",
      "Epoch: 652 \tTraining Loss: 24.981953\n",
      "Epoch: 653 \tTraining Loss: 24.981951\n",
      "Epoch: 654 \tTraining Loss: 24.981953\n",
      "Epoch: 655 \tTraining Loss: 24.981953\n",
      "Epoch: 656 \tTraining Loss: 24.981953\n",
      "Epoch: 657 \tTraining Loss: 24.981953\n",
      "Epoch: 658 \tTraining Loss: 24.981949\n",
      "Epoch: 659 \tTraining Loss: 24.981949\n",
      "Epoch: 660 \tTraining Loss: 24.981949\n",
      "Epoch: 661 \tTraining Loss: 24.981949\n",
      "Epoch: 662 \tTraining Loss: 24.981949\n",
      "Epoch: 663 \tTraining Loss: 24.981945\n",
      "Epoch: 664 \tTraining Loss: 24.981949\n",
      "Epoch: 665 \tTraining Loss: 24.981945\n",
      "Epoch: 666 \tTraining Loss: 24.981943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 667 \tTraining Loss: 24.981943\n",
      "Epoch: 668 \tTraining Loss: 24.981943\n",
      "Epoch: 669 \tTraining Loss: 24.981939\n",
      "Epoch: 670 \tTraining Loss: 24.981943\n",
      "Epoch: 671 \tTraining Loss: 24.981939\n",
      "Epoch: 672 \tTraining Loss: 24.981943\n",
      "Epoch: 673 \tTraining Loss: 24.981939\n",
      "Epoch: 674 \tTraining Loss: 24.981939\n",
      "Epoch: 675 \tTraining Loss: 24.981943\n",
      "Epoch: 676 \tTraining Loss: 24.981937\n",
      "Epoch: 677 \tTraining Loss: 24.981937\n",
      "Epoch: 678 \tTraining Loss: 24.981936\n",
      "Epoch: 679 \tTraining Loss: 24.981936\n",
      "Epoch: 680 \tTraining Loss: 24.981932\n",
      "Epoch: 681 \tTraining Loss: 24.981932\n",
      "Epoch: 682 \tTraining Loss: 24.981932\n",
      "Epoch: 683 \tTraining Loss: 24.981932\n",
      "Epoch: 684 \tTraining Loss: 24.981932\n",
      "Epoch: 685 \tTraining Loss: 24.981936\n",
      "Epoch: 686 \tTraining Loss: 24.981930\n",
      "Epoch: 687 \tTraining Loss: 24.981930\n",
      "Epoch: 688 \tTraining Loss: 24.981932\n",
      "Epoch: 689 \tTraining Loss: 24.981932\n",
      "Epoch: 690 \tTraining Loss: 24.981932\n",
      "Epoch: 691 \tTraining Loss: 24.981932\n",
      "Epoch: 692 \tTraining Loss: 24.981930\n",
      "Epoch: 693 \tTraining Loss: 24.981928\n",
      "Epoch: 694 \tTraining Loss: 24.981928\n",
      "Epoch: 695 \tTraining Loss: 24.981928\n",
      "Epoch: 696 \tTraining Loss: 24.981928\n",
      "Epoch: 697 \tTraining Loss: 24.981928\n",
      "Epoch: 698 \tTraining Loss: 24.981928\n",
      "Epoch: 699 \tTraining Loss: 24.981928\n",
      "Epoch: 700 \tTraining Loss: 24.981928\n",
      "Epoch: 701 \tTraining Loss: 24.981928\n",
      "Epoch: 702 \tTraining Loss: 24.981928\n",
      "Epoch: 703 \tTraining Loss: 24.981928\n",
      "Epoch: 704 \tTraining Loss: 24.981924\n",
      "Epoch: 705 \tTraining Loss: 24.981928\n",
      "Epoch: 706 \tTraining Loss: 24.981924\n",
      "Epoch: 707 \tTraining Loss: 24.981922\n",
      "Epoch: 708 \tTraining Loss: 24.981922\n",
      "Epoch: 709 \tTraining Loss: 24.981922\n",
      "Epoch: 710 \tTraining Loss: 24.981918\n",
      "Epoch: 711 \tTraining Loss: 24.981922\n",
      "Epoch: 712 \tTraining Loss: 24.981918\n",
      "Epoch: 713 \tTraining Loss: 24.981918\n",
      "Epoch: 714 \tTraining Loss: 24.981918\n",
      "Epoch: 715 \tTraining Loss: 24.981918\n",
      "Epoch: 716 \tTraining Loss: 24.981918\n",
      "Epoch: 717 \tTraining Loss: 24.981916\n",
      "Epoch: 718 \tTraining Loss: 24.981916\n",
      "Epoch: 719 \tTraining Loss: 24.981916\n",
      "Epoch: 720 \tTraining Loss: 24.981916\n",
      "Epoch: 721 \tTraining Loss: 24.981916\n",
      "Epoch: 722 \tTraining Loss: 24.981916\n",
      "Epoch: 723 \tTraining Loss: 24.981916\n",
      "Epoch: 724 \tTraining Loss: 24.981915\n",
      "Epoch: 725 \tTraining Loss: 24.981915\n",
      "Epoch: 726 \tTraining Loss: 24.981915\n",
      "Epoch: 727 \tTraining Loss: 24.981911\n",
      "Epoch: 728 \tTraining Loss: 24.981915\n",
      "Epoch: 729 \tTraining Loss: 24.981911\n",
      "Epoch: 730 \tTraining Loss: 24.981911\n",
      "Epoch: 731 \tTraining Loss: 24.981911\n",
      "Epoch: 732 \tTraining Loss: 24.981911\n",
      "Epoch: 733 \tTraining Loss: 24.981909\n",
      "Epoch: 734 \tTraining Loss: 24.981909\n",
      "Epoch: 735 \tTraining Loss: 24.981909\n",
      "Epoch: 736 \tTraining Loss: 24.981909\n",
      "Epoch: 737 \tTraining Loss: 24.981909\n",
      "Epoch: 738 \tTraining Loss: 24.981907\n",
      "Epoch: 739 \tTraining Loss: 24.981907\n",
      "Epoch: 740 \tTraining Loss: 24.981907\n",
      "Epoch: 741 \tTraining Loss: 24.981907\n",
      "Epoch: 742 \tTraining Loss: 24.981907\n",
      "Epoch: 743 \tTraining Loss: 24.981903\n",
      "Epoch: 744 \tTraining Loss: 24.981903\n",
      "Epoch: 745 \tTraining Loss: 24.981901\n",
      "Epoch: 746 \tTraining Loss: 24.981901\n",
      "Epoch: 747 \tTraining Loss: 24.981901\n",
      "Epoch: 748 \tTraining Loss: 24.981897\n",
      "Epoch: 749 \tTraining Loss: 24.981897\n",
      "Epoch: 750 \tTraining Loss: 24.981901\n",
      "Epoch: 751 \tTraining Loss: 24.981897\n",
      "Epoch: 752 \tTraining Loss: 24.981897\n",
      "Epoch: 753 \tTraining Loss: 24.981897\n",
      "Epoch: 754 \tTraining Loss: 24.981894\n",
      "Epoch: 755 \tTraining Loss: 24.981894\n",
      "Epoch: 756 \tTraining Loss: 24.981894\n",
      "Epoch: 757 \tTraining Loss: 24.981895\n",
      "Epoch: 758 \tTraining Loss: 24.981894\n",
      "Epoch: 759 \tTraining Loss: 24.981894\n",
      "Epoch: 760 \tTraining Loss: 24.981894\n",
      "Epoch: 761 \tTraining Loss: 24.981894\n",
      "Epoch: 762 \tTraining Loss: 24.981890\n",
      "Epoch: 763 \tTraining Loss: 24.981890\n",
      "Epoch: 764 \tTraining Loss: 24.981890\n",
      "Epoch: 765 \tTraining Loss: 24.981890\n",
      "Epoch: 766 \tTraining Loss: 24.981890\n",
      "Epoch: 767 \tTraining Loss: 24.981890\n",
      "Epoch: 768 \tTraining Loss: 24.981888\n",
      "Epoch: 769 \tTraining Loss: 24.981888\n",
      "Epoch: 770 \tTraining Loss: 24.981888\n",
      "Epoch: 771 \tTraining Loss: 24.981888\n",
      "Epoch: 772 \tTraining Loss: 24.981886\n",
      "Epoch: 773 \tTraining Loss: 24.981886\n",
      "Epoch: 774 \tTraining Loss: 24.981882\n",
      "Epoch: 775 \tTraining Loss: 24.981886\n",
      "Epoch: 776 \tTraining Loss: 24.981886\n",
      "Epoch: 777 \tTraining Loss: 24.981882\n",
      "Epoch: 778 \tTraining Loss: 24.981882\n",
      "Epoch: 779 \tTraining Loss: 24.981882\n",
      "Epoch: 780 \tTraining Loss: 24.981880\n",
      "Epoch: 781 \tTraining Loss: 24.981876\n",
      "Epoch: 782 \tTraining Loss: 24.981876\n",
      "Epoch: 783 \tTraining Loss: 24.981876\n",
      "Epoch: 784 \tTraining Loss: 24.981876\n",
      "Epoch: 785 \tTraining Loss: 24.981876\n",
      "Epoch: 786 \tTraining Loss: 24.981876\n",
      "Epoch: 787 \tTraining Loss: 24.981876\n",
      "Epoch: 788 \tTraining Loss: 24.981876\n",
      "Epoch: 789 \tTraining Loss: 24.981876\n",
      "Epoch: 790 \tTraining Loss: 24.981876\n",
      "Epoch: 791 \tTraining Loss: 24.981874\n",
      "Epoch: 792 \tTraining Loss: 24.981876\n",
      "Epoch: 793 \tTraining Loss: 24.981873\n",
      "Epoch: 794 \tTraining Loss: 24.981873\n",
      "Epoch: 795 \tTraining Loss: 24.981869\n",
      "Epoch: 796 \tTraining Loss: 24.981869\n",
      "Epoch: 797 \tTraining Loss: 24.981869\n",
      "Epoch: 798 \tTraining Loss: 24.981867\n",
      "Epoch: 799 \tTraining Loss: 24.981867\n",
      "Epoch: 800 \tTraining Loss: 24.981867\n",
      "Epoch: 801 \tTraining Loss: 24.981867\n",
      "Epoch: 802 \tTraining Loss: 24.981867\n",
      "Epoch: 803 \tTraining Loss: 24.981867\n",
      "Epoch: 804 \tTraining Loss: 24.981867\n",
      "Epoch: 805 \tTraining Loss: 24.981867\n",
      "Epoch: 806 \tTraining Loss: 24.981867\n",
      "Epoch: 807 \tTraining Loss: 24.981867\n",
      "Epoch: 808 \tTraining Loss: 24.981867\n",
      "Epoch: 809 \tTraining Loss: 24.981861\n",
      "Epoch: 810 \tTraining Loss: 24.981859\n",
      "Epoch: 811 \tTraining Loss: 24.981859\n",
      "Epoch: 812 \tTraining Loss: 24.981859\n",
      "Epoch: 813 \tTraining Loss: 24.981859\n",
      "Epoch: 814 \tTraining Loss: 24.981859\n",
      "Epoch: 815 \tTraining Loss: 24.981859\n",
      "Epoch: 816 \tTraining Loss: 24.981859\n",
      "Epoch: 817 \tTraining Loss: 24.981855\n",
      "Epoch: 818 \tTraining Loss: 24.981855\n",
      "Epoch: 819 \tTraining Loss: 24.981855\n",
      "Epoch: 820 \tTraining Loss: 24.981855\n",
      "Epoch: 821 \tTraining Loss: 24.981855\n",
      "Epoch: 822 \tTraining Loss: 24.981855\n",
      "Epoch: 823 \tTraining Loss: 24.981853\n",
      "Epoch: 824 \tTraining Loss: 24.981853\n",
      "Epoch: 825 \tTraining Loss: 24.981853\n",
      "Epoch: 826 \tTraining Loss: 24.981853\n",
      "Epoch: 827 \tTraining Loss: 24.981853\n",
      "Epoch: 828 \tTraining Loss: 24.981853\n",
      "Epoch: 829 \tTraining Loss: 24.981853\n",
      "Epoch: 830 \tTraining Loss: 24.981852\n",
      "Epoch: 831 \tTraining Loss: 24.981852\n",
      "Epoch: 832 \tTraining Loss: 24.981852\n",
      "Epoch: 833 \tTraining Loss: 24.981852\n",
      "Epoch: 834 \tTraining Loss: 24.981852\n",
      "Epoch: 835 \tTraining Loss: 24.981848\n",
      "Epoch: 836 \tTraining Loss: 24.981848\n",
      "Epoch: 837 \tTraining Loss: 24.981846\n",
      "Epoch: 838 \tTraining Loss: 24.981846\n",
      "Epoch: 839 \tTraining Loss: 24.981846\n",
      "Epoch: 840 \tTraining Loss: 24.981846\n",
      "Epoch: 841 \tTraining Loss: 24.981846\n",
      "Epoch: 842 \tTraining Loss: 24.981842\n",
      "Epoch: 843 \tTraining Loss: 24.981842\n",
      "Epoch: 844 \tTraining Loss: 24.981842\n",
      "Epoch: 845 \tTraining Loss: 24.981840\n",
      "Epoch: 846 \tTraining Loss: 24.981842\n",
      "Epoch: 847 \tTraining Loss: 24.981842\n",
      "Epoch: 848 \tTraining Loss: 24.981840\n",
      "Epoch: 849 \tTraining Loss: 24.981840\n",
      "Epoch: 850 \tTraining Loss: 24.981840\n",
      "Epoch: 851 \tTraining Loss: 24.981840\n",
      "Epoch: 852 \tTraining Loss: 24.981840\n",
      "Epoch: 853 \tTraining Loss: 24.981840\n",
      "Epoch: 854 \tTraining Loss: 24.981838\n",
      "Epoch: 855 \tTraining Loss: 24.981838\n",
      "Epoch: 856 \tTraining Loss: 24.981834\n",
      "Epoch: 857 \tTraining Loss: 24.981834\n",
      "Epoch: 858 \tTraining Loss: 24.981834\n",
      "Epoch: 859 \tTraining Loss: 24.981834\n",
      "Epoch: 860 \tTraining Loss: 24.981833\n",
      "Epoch: 861 \tTraining Loss: 24.981833\n",
      "Epoch: 862 \tTraining Loss: 24.981833\n",
      "Epoch: 863 \tTraining Loss: 24.981833\n",
      "Epoch: 864 \tTraining Loss: 24.981831\n",
      "Epoch: 865 \tTraining Loss: 24.981831\n",
      "Epoch: 866 \tTraining Loss: 24.981831\n",
      "Epoch: 867 \tTraining Loss: 24.981827\n",
      "Epoch: 868 \tTraining Loss: 24.981831\n",
      "Epoch: 869 \tTraining Loss: 24.981831\n",
      "Epoch: 870 \tTraining Loss: 24.981831\n",
      "Epoch: 871 \tTraining Loss: 24.981827\n",
      "Epoch: 872 \tTraining Loss: 24.981827\n",
      "Epoch: 873 \tTraining Loss: 24.981825\n",
      "Epoch: 874 \tTraining Loss: 24.981825\n",
      "Epoch: 875 \tTraining Loss: 24.981821\n",
      "Epoch: 876 \tTraining Loss: 24.981821\n",
      "Epoch: 877 \tTraining Loss: 24.981825\n",
      "Epoch: 878 \tTraining Loss: 24.981821\n",
      "Epoch: 879 \tTraining Loss: 24.981821\n",
      "Epoch: 880 \tTraining Loss: 24.981821\n",
      "Epoch: 881 \tTraining Loss: 24.981821\n",
      "Epoch: 882 \tTraining Loss: 24.981821\n",
      "Epoch: 883 \tTraining Loss: 24.981821\n",
      "Epoch: 884 \tTraining Loss: 24.981819\n",
      "Epoch: 885 \tTraining Loss: 24.981819\n",
      "Epoch: 886 \tTraining Loss: 24.981819\n",
      "Epoch: 887 \tTraining Loss: 24.981817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 888 \tTraining Loss: 24.981817\n",
      "Epoch: 889 \tTraining Loss: 24.981817\n",
      "Epoch: 890 \tTraining Loss: 24.981817\n",
      "Epoch: 891 \tTraining Loss: 24.981813\n",
      "Epoch: 892 \tTraining Loss: 24.981813\n",
      "Epoch: 893 \tTraining Loss: 24.981813\n",
      "Epoch: 894 \tTraining Loss: 24.981813\n",
      "Epoch: 895 \tTraining Loss: 24.981817\n",
      "Epoch: 896 \tTraining Loss: 24.981812\n",
      "Epoch: 897 \tTraining Loss: 24.981812\n",
      "Epoch: 898 \tTraining Loss: 24.981812\n",
      "Epoch: 899 \tTraining Loss: 24.981812\n",
      "Epoch: 900 \tTraining Loss: 24.981810\n",
      "Epoch: 901 \tTraining Loss: 24.981812\n",
      "Epoch: 902 \tTraining Loss: 24.981810\n",
      "Epoch: 903 \tTraining Loss: 24.981806\n",
      "Epoch: 904 \tTraining Loss: 24.981810\n",
      "Epoch: 905 \tTraining Loss: 24.981806\n",
      "Epoch: 906 \tTraining Loss: 24.981806\n",
      "Epoch: 907 \tTraining Loss: 24.981806\n",
      "Epoch: 908 \tTraining Loss: 24.981806\n",
      "Epoch: 909 \tTraining Loss: 24.981806\n",
      "Epoch: 910 \tTraining Loss: 24.981806\n",
      "Epoch: 911 \tTraining Loss: 24.981806\n",
      "Epoch: 912 \tTraining Loss: 24.981806\n",
      "Epoch: 913 \tTraining Loss: 24.981804\n",
      "Epoch: 914 \tTraining Loss: 24.981804\n",
      "Epoch: 915 \tTraining Loss: 24.981804\n",
      "Epoch: 916 \tTraining Loss: 24.981800\n",
      "Epoch: 917 \tTraining Loss: 24.981800\n",
      "Epoch: 918 \tTraining Loss: 24.981800\n",
      "Epoch: 919 \tTraining Loss: 24.981800\n",
      "Epoch: 920 \tTraining Loss: 24.981800\n",
      "Epoch: 921 \tTraining Loss: 24.981800\n",
      "Epoch: 922 \tTraining Loss: 24.981798\n",
      "Epoch: 923 \tTraining Loss: 24.981798\n",
      "Epoch: 924 \tTraining Loss: 24.981798\n",
      "Epoch: 925 \tTraining Loss: 24.981796\n",
      "Epoch: 926 \tTraining Loss: 24.981796\n",
      "Epoch: 927 \tTraining Loss: 24.981796\n",
      "Epoch: 928 \tTraining Loss: 24.981792\n",
      "Epoch: 929 \tTraining Loss: 24.981792\n",
      "Epoch: 930 \tTraining Loss: 24.981791\n",
      "Epoch: 931 \tTraining Loss: 24.981792\n",
      "Epoch: 932 \tTraining Loss: 24.981791\n",
      "Epoch: 933 \tTraining Loss: 24.981791\n",
      "Epoch: 934 \tTraining Loss: 24.981791\n",
      "Epoch: 935 \tTraining Loss: 24.981791\n",
      "Epoch: 936 \tTraining Loss: 24.981791\n",
      "Epoch: 937 \tTraining Loss: 24.981789\n",
      "Epoch: 938 \tTraining Loss: 24.981789\n",
      "Epoch: 939 \tTraining Loss: 24.981789\n",
      "Epoch: 940 \tTraining Loss: 24.981789\n",
      "Epoch: 941 \tTraining Loss: 24.981789\n",
      "Epoch: 942 \tTraining Loss: 24.981789\n",
      "Epoch: 943 \tTraining Loss: 24.981789\n",
      "Epoch: 944 \tTraining Loss: 24.981785\n",
      "Epoch: 945 \tTraining Loss: 24.981785\n",
      "Epoch: 946 \tTraining Loss: 24.981785\n",
      "Epoch: 947 \tTraining Loss: 24.981783\n",
      "Epoch: 948 \tTraining Loss: 24.981783\n",
      "Epoch: 949 \tTraining Loss: 24.981779\n",
      "Epoch: 950 \tTraining Loss: 24.981779\n",
      "Epoch: 951 \tTraining Loss: 24.981779\n",
      "Epoch: 952 \tTraining Loss: 24.981779\n",
      "Epoch: 953 \tTraining Loss: 24.981779\n",
      "Epoch: 954 \tTraining Loss: 24.981779\n",
      "Epoch: 955 \tTraining Loss: 24.981779\n",
      "Epoch: 956 \tTraining Loss: 24.981779\n",
      "Epoch: 957 \tTraining Loss: 24.981777\n",
      "Epoch: 958 \tTraining Loss: 24.981777\n",
      "Epoch: 959 \tTraining Loss: 24.981779\n",
      "Epoch: 960 \tTraining Loss: 24.981777\n",
      "Epoch: 961 \tTraining Loss: 24.981777\n",
      "Epoch: 962 \tTraining Loss: 24.981775\n",
      "Epoch: 963 \tTraining Loss: 24.981775\n",
      "Epoch: 964 \tTraining Loss: 24.981775\n",
      "Epoch: 965 \tTraining Loss: 24.981771\n",
      "Epoch: 966 \tTraining Loss: 24.981771\n",
      "Epoch: 967 \tTraining Loss: 24.981771\n",
      "Epoch: 968 \tTraining Loss: 24.981771\n",
      "Epoch: 969 \tTraining Loss: 24.981770\n",
      "Epoch: 970 \tTraining Loss: 24.981770\n",
      "Epoch: 971 \tTraining Loss: 24.981770\n",
      "Epoch: 972 \tTraining Loss: 24.981770\n",
      "Epoch: 973 \tTraining Loss: 24.981770\n",
      "Epoch: 974 \tTraining Loss: 24.981770\n",
      "Epoch: 975 \tTraining Loss: 24.981770\n",
      "Epoch: 976 \tTraining Loss: 24.981768\n",
      "Epoch: 977 \tTraining Loss: 24.981768\n",
      "Epoch: 978 \tTraining Loss: 24.981768\n",
      "Epoch: 979 \tTraining Loss: 24.981764\n",
      "Epoch: 980 \tTraining Loss: 24.981764\n",
      "Epoch: 981 \tTraining Loss: 24.981764\n",
      "Epoch: 982 \tTraining Loss: 24.981762\n",
      "Epoch: 983 \tTraining Loss: 24.981762\n",
      "Epoch: 984 \tTraining Loss: 24.981762\n",
      "Epoch: 985 \tTraining Loss: 24.981762\n",
      "Epoch: 986 \tTraining Loss: 24.981762\n",
      "Epoch: 987 \tTraining Loss: 24.981762\n",
      "Epoch: 988 \tTraining Loss: 24.981758\n",
      "Epoch: 989 \tTraining Loss: 24.981758\n",
      "Epoch: 990 \tTraining Loss: 24.981758\n",
      "Epoch: 991 \tTraining Loss: 24.981758\n",
      "Epoch: 992 \tTraining Loss: 24.981756\n",
      "Epoch: 993 \tTraining Loss: 24.981756\n",
      "Epoch: 994 \tTraining Loss: 24.981756\n",
      "Epoch: 995 \tTraining Loss: 24.981754\n",
      "Epoch: 996 \tTraining Loss: 24.981754\n",
      "Epoch: 997 \tTraining Loss: 24.981754\n",
      "Epoch: 998 \tTraining Loss: 24.981750\n",
      "Epoch: 999 \tTraining Loss: 24.981750\n",
      "Epoch: 1000 \tTraining Loss: 24.981750\n",
      "Epoch: 1001 \tTraining Loss: 24.981749\n",
      "Epoch: 1002 \tTraining Loss: 24.981749\n",
      "Epoch: 1003 \tTraining Loss: 24.981749\n",
      "Epoch: 1004 \tTraining Loss: 24.981749\n",
      "Epoch: 1005 \tTraining Loss: 24.981749\n",
      "Epoch: 1006 \tTraining Loss: 24.981749\n",
      "Epoch: 1007 \tTraining Loss: 24.981749\n",
      "Epoch: 1008 \tTraining Loss: 24.981749\n",
      "Epoch: 1009 \tTraining Loss: 24.981743\n",
      "Epoch: 1010 \tTraining Loss: 24.981749\n",
      "Epoch: 1011 \tTraining Loss: 24.981743\n",
      "Epoch: 1012 \tTraining Loss: 24.981743\n",
      "Epoch: 1013 \tTraining Loss: 24.981743\n",
      "Epoch: 1014 \tTraining Loss: 24.981743\n",
      "Epoch: 1015 \tTraining Loss: 24.981743\n",
      "Epoch: 1016 \tTraining Loss: 24.981741\n",
      "Epoch: 1017 \tTraining Loss: 24.981741\n",
      "Epoch: 1018 \tTraining Loss: 24.981743\n",
      "Epoch: 1019 \tTraining Loss: 24.981741\n",
      "Epoch: 1020 \tTraining Loss: 24.981741\n",
      "Epoch: 1021 \tTraining Loss: 24.981741\n",
      "Epoch: 1022 \tTraining Loss: 24.981741\n",
      "Epoch: 1023 \tTraining Loss: 24.981741\n",
      "Epoch: 1024 \tTraining Loss: 24.981737\n",
      "Epoch: 1025 \tTraining Loss: 24.981737\n",
      "Epoch: 1026 \tTraining Loss: 24.981737\n",
      "Epoch: 1027 \tTraining Loss: 24.981735\n",
      "Epoch: 1028 \tTraining Loss: 24.981735\n",
      "Epoch: 1029 \tTraining Loss: 24.981733\n",
      "Epoch: 1030 \tTraining Loss: 24.981735\n",
      "Epoch: 1031 \tTraining Loss: 24.981733\n",
      "Epoch: 1032 \tTraining Loss: 24.981733\n",
      "Epoch: 1033 \tTraining Loss: 24.981730\n",
      "Epoch: 1034 \tTraining Loss: 24.981730\n",
      "Epoch: 1035 \tTraining Loss: 24.981730\n",
      "Epoch: 1036 \tTraining Loss: 24.981730\n",
      "Epoch: 1037 \tTraining Loss: 24.981730\n",
      "Epoch: 1038 \tTraining Loss: 24.981733\n",
      "Epoch: 1039 \tTraining Loss: 24.981730\n",
      "Epoch: 1040 \tTraining Loss: 24.981730\n",
      "Epoch: 1041 \tTraining Loss: 24.981728\n",
      "Epoch: 1042 \tTraining Loss: 24.981726\n",
      "Epoch: 1043 \tTraining Loss: 24.981726\n",
      "Epoch: 1044 \tTraining Loss: 24.981726\n",
      "Epoch: 1045 \tTraining Loss: 24.981726\n",
      "Epoch: 1046 \tTraining Loss: 24.981726\n",
      "Epoch: 1047 \tTraining Loss: 24.981722\n",
      "Epoch: 1048 \tTraining Loss: 24.981726\n",
      "Epoch: 1049 \tTraining Loss: 24.981726\n",
      "Epoch: 1050 \tTraining Loss: 24.981726\n",
      "Epoch: 1051 \tTraining Loss: 24.981726\n",
      "Epoch: 1052 \tTraining Loss: 24.981722\n",
      "Epoch: 1053 \tTraining Loss: 24.981722\n",
      "Epoch: 1054 \tTraining Loss: 24.981720\n",
      "Epoch: 1055 \tTraining Loss: 24.981720\n",
      "Epoch: 1056 \tTraining Loss: 24.981716\n",
      "Epoch: 1057 \tTraining Loss: 24.981716\n",
      "Epoch: 1058 \tTraining Loss: 24.981716\n",
      "Epoch: 1059 \tTraining Loss: 24.981716\n",
      "Epoch: 1060 \tTraining Loss: 24.981716\n",
      "Epoch: 1061 \tTraining Loss: 24.981716\n",
      "Epoch: 1062 \tTraining Loss: 24.981716\n",
      "Epoch: 1063 \tTraining Loss: 24.981714\n",
      "Epoch: 1064 \tTraining Loss: 24.981714\n",
      "Epoch: 1065 \tTraining Loss: 24.981714\n",
      "Epoch: 1066 \tTraining Loss: 24.981712\n",
      "Epoch: 1067 \tTraining Loss: 24.981712\n",
      "Epoch: 1068 \tTraining Loss: 24.981709\n",
      "Epoch: 1069 \tTraining Loss: 24.981709\n",
      "Epoch: 1070 \tTraining Loss: 24.981707\n",
      "Epoch: 1071 \tTraining Loss: 24.981707\n",
      "Epoch: 1072 \tTraining Loss: 24.981707\n",
      "Epoch: 1073 \tTraining Loss: 24.981707\n",
      "Epoch: 1074 \tTraining Loss: 24.981707\n",
      "Epoch: 1075 \tTraining Loss: 24.981707\n",
      "Epoch: 1076 \tTraining Loss: 24.981707\n",
      "Epoch: 1077 \tTraining Loss: 24.981707\n",
      "Epoch: 1078 \tTraining Loss: 24.981701\n",
      "Epoch: 1079 \tTraining Loss: 24.981701\n",
      "Epoch: 1080 \tTraining Loss: 24.981701\n",
      "Epoch: 1081 \tTraining Loss: 24.981701\n",
      "Epoch: 1082 \tTraining Loss: 24.981701\n",
      "Epoch: 1083 \tTraining Loss: 24.981701\n",
      "Epoch: 1084 \tTraining Loss: 24.981699\n",
      "Epoch: 1085 \tTraining Loss: 24.981699\n",
      "Epoch: 1086 \tTraining Loss: 24.981699\n",
      "Epoch: 1087 \tTraining Loss: 24.981699\n",
      "Epoch: 1088 \tTraining Loss: 24.981699\n",
      "Epoch: 1089 \tTraining Loss: 24.981699\n",
      "Epoch: 1090 \tTraining Loss: 24.981695\n",
      "Epoch: 1091 \tTraining Loss: 24.981695\n",
      "Epoch: 1092 \tTraining Loss: 24.981695\n",
      "Epoch: 1093 \tTraining Loss: 24.981695\n",
      "Epoch: 1094 \tTraining Loss: 24.981695\n",
      "Epoch: 1095 \tTraining Loss: 24.981693\n",
      "Epoch: 1096 \tTraining Loss: 24.981693\n",
      "Epoch: 1097 \tTraining Loss: 24.981695\n",
      "Epoch: 1098 \tTraining Loss: 24.981693\n",
      "Epoch: 1099 \tTraining Loss: 24.981693\n",
      "Epoch: 1100 \tTraining Loss: 24.981693\n",
      "Epoch: 1101 \tTraining Loss: 24.981695\n",
      "Epoch: 1102 \tTraining Loss: 24.981693\n",
      "Epoch: 1103 \tTraining Loss: 24.981691\n",
      "Epoch: 1104 \tTraining Loss: 24.981691\n",
      "Epoch: 1105 \tTraining Loss: 24.981691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1106 \tTraining Loss: 24.981691\n",
      "Epoch: 1107 \tTraining Loss: 24.981688\n",
      "Epoch: 1108 \tTraining Loss: 24.981688\n",
      "Epoch: 1109 \tTraining Loss: 24.981691\n",
      "Epoch: 1110 \tTraining Loss: 24.981691\n",
      "Epoch: 1111 \tTraining Loss: 24.981688\n",
      "Epoch: 1112 \tTraining Loss: 24.981688\n",
      "Epoch: 1113 \tTraining Loss: 24.981688\n",
      "Epoch: 1114 \tTraining Loss: 24.981686\n",
      "Epoch: 1115 \tTraining Loss: 24.981686\n",
      "Epoch: 1116 \tTraining Loss: 24.981682\n",
      "Epoch: 1117 \tTraining Loss: 24.981682\n",
      "Epoch: 1118 \tTraining Loss: 24.981682\n",
      "Epoch: 1119 \tTraining Loss: 24.981686\n",
      "Epoch: 1120 \tTraining Loss: 24.981680\n",
      "Epoch: 1121 \tTraining Loss: 24.981682\n",
      "Epoch: 1122 \tTraining Loss: 24.981682\n",
      "Epoch: 1123 \tTraining Loss: 24.981678\n",
      "Epoch: 1124 \tTraining Loss: 24.981678\n",
      "Epoch: 1125 \tTraining Loss: 24.981678\n",
      "Epoch: 1126 \tTraining Loss: 24.981678\n",
      "Epoch: 1127 \tTraining Loss: 24.981674\n",
      "Epoch: 1128 \tTraining Loss: 24.981674\n",
      "Epoch: 1129 \tTraining Loss: 24.981674\n",
      "Epoch: 1130 \tTraining Loss: 24.981678\n",
      "Epoch: 1131 \tTraining Loss: 24.981674\n",
      "Epoch: 1132 \tTraining Loss: 24.981674\n",
      "Epoch: 1133 \tTraining Loss: 24.981674\n",
      "Epoch: 1134 \tTraining Loss: 24.981672\n",
      "Epoch: 1135 \tTraining Loss: 24.981672\n",
      "Epoch: 1136 \tTraining Loss: 24.981672\n",
      "Epoch: 1137 \tTraining Loss: 24.981670\n",
      "Epoch: 1138 \tTraining Loss: 24.981670\n",
      "Epoch: 1139 \tTraining Loss: 24.981670\n",
      "Epoch: 1140 \tTraining Loss: 24.981670\n",
      "Epoch: 1141 \tTraining Loss: 24.981667\n",
      "Epoch: 1142 \tTraining Loss: 24.981667\n",
      "Epoch: 1143 \tTraining Loss: 24.981670\n",
      "Epoch: 1144 \tTraining Loss: 24.981667\n",
      "Epoch: 1145 \tTraining Loss: 24.981665\n",
      "Epoch: 1146 \tTraining Loss: 24.981665\n",
      "Epoch: 1147 \tTraining Loss: 24.981661\n",
      "Epoch: 1148 \tTraining Loss: 24.981661\n",
      "Epoch: 1149 \tTraining Loss: 24.981661\n",
      "Epoch: 1150 \tTraining Loss: 24.981659\n",
      "Epoch: 1151 \tTraining Loss: 24.981661\n",
      "Epoch: 1152 \tTraining Loss: 24.981665\n",
      "Epoch: 1153 \tTraining Loss: 24.981659\n",
      "Epoch: 1154 \tTraining Loss: 24.981659\n",
      "Epoch: 1155 \tTraining Loss: 24.981659\n",
      "Epoch: 1156 \tTraining Loss: 24.981659\n",
      "Epoch: 1157 \tTraining Loss: 24.981659\n",
      "Epoch: 1158 \tTraining Loss: 24.981657\n",
      "Epoch: 1159 \tTraining Loss: 24.981659\n",
      "Epoch: 1160 \tTraining Loss: 24.981657\n",
      "Epoch: 1161 \tTraining Loss: 24.981659\n",
      "Epoch: 1162 \tTraining Loss: 24.981657\n",
      "Epoch: 1163 \tTraining Loss: 24.981657\n",
      "Epoch: 1164 \tTraining Loss: 24.981657\n",
      "Epoch: 1165 \tTraining Loss: 24.981657\n",
      "Epoch: 1166 \tTraining Loss: 24.981653\n",
      "Epoch: 1167 \tTraining Loss: 24.981653\n",
      "Epoch: 1168 \tTraining Loss: 24.981653\n",
      "Epoch: 1169 \tTraining Loss: 24.981651\n",
      "Epoch: 1170 \tTraining Loss: 24.981651\n",
      "Epoch: 1171 \tTraining Loss: 24.981651\n",
      "Epoch: 1172 \tTraining Loss: 24.981649\n",
      "Epoch: 1173 \tTraining Loss: 24.981649\n",
      "Epoch: 1174 \tTraining Loss: 24.981649\n",
      "Epoch: 1175 \tTraining Loss: 24.981649\n",
      "Epoch: 1176 \tTraining Loss: 24.981646\n",
      "Epoch: 1177 \tTraining Loss: 24.981646\n",
      "Epoch: 1178 \tTraining Loss: 24.981646\n",
      "Epoch: 1179 \tTraining Loss: 24.981646\n",
      "Epoch: 1180 \tTraining Loss: 24.981646\n",
      "Epoch: 1181 \tTraining Loss: 24.981644\n",
      "Epoch: 1182 \tTraining Loss: 24.981644\n",
      "Epoch: 1183 \tTraining Loss: 24.981640\n",
      "Epoch: 1184 \tTraining Loss: 24.981640\n",
      "Epoch: 1185 \tTraining Loss: 24.981638\n",
      "Epoch: 1186 \tTraining Loss: 24.981638\n",
      "Epoch: 1187 \tTraining Loss: 24.981638\n",
      "Epoch: 1188 \tTraining Loss: 24.981638\n",
      "Epoch: 1189 \tTraining Loss: 24.981636\n",
      "Epoch: 1190 \tTraining Loss: 24.981636\n",
      "Epoch: 1191 \tTraining Loss: 24.981636\n",
      "Epoch: 1192 \tTraining Loss: 24.981638\n",
      "Epoch: 1193 \tTraining Loss: 24.981632\n",
      "Epoch: 1194 \tTraining Loss: 24.981632\n",
      "Epoch: 1195 \tTraining Loss: 24.981636\n",
      "Epoch: 1196 \tTraining Loss: 24.981632\n",
      "Epoch: 1197 \tTraining Loss: 24.981632\n",
      "Epoch: 1198 \tTraining Loss: 24.981636\n",
      "Epoch: 1199 \tTraining Loss: 24.981636\n",
      "Epoch: 1200 \tTraining Loss: 24.981632\n",
      "Epoch: 1201 \tTraining Loss: 24.981632\n",
      "Epoch: 1202 \tTraining Loss: 24.981632\n",
      "Epoch: 1203 \tTraining Loss: 24.981632\n",
      "Epoch: 1204 \tTraining Loss: 24.981630\n",
      "Epoch: 1205 \tTraining Loss: 24.981628\n",
      "Epoch: 1206 \tTraining Loss: 24.981630\n",
      "Epoch: 1207 \tTraining Loss: 24.981630\n",
      "Epoch: 1208 \tTraining Loss: 24.981625\n",
      "Epoch: 1209 \tTraining Loss: 24.981630\n",
      "Epoch: 1210 \tTraining Loss: 24.981630\n",
      "Epoch: 1211 \tTraining Loss: 24.981628\n",
      "Epoch: 1212 \tTraining Loss: 24.981628\n",
      "Epoch: 1213 \tTraining Loss: 24.981628\n",
      "Epoch: 1214 \tTraining Loss: 24.981628\n",
      "Epoch: 1215 \tTraining Loss: 24.981628\n",
      "Epoch: 1216 \tTraining Loss: 24.981625\n",
      "Epoch: 1217 \tTraining Loss: 24.981625\n",
      "Epoch: 1218 \tTraining Loss: 24.981625\n",
      "Epoch: 1219 \tTraining Loss: 24.981619\n",
      "Epoch: 1220 \tTraining Loss: 24.981623\n",
      "Epoch: 1221 \tTraining Loss: 24.981619\n",
      "Epoch: 1222 \tTraining Loss: 24.981617\n",
      "Epoch: 1223 \tTraining Loss: 24.981619\n",
      "Epoch: 1224 \tTraining Loss: 24.981619\n",
      "Epoch: 1225 \tTraining Loss: 24.981617\n",
      "Epoch: 1226 \tTraining Loss: 24.981617\n",
      "Epoch: 1227 \tTraining Loss: 24.981617\n",
      "Epoch: 1228 \tTraining Loss: 24.981615\n",
      "Epoch: 1229 \tTraining Loss: 24.981615\n",
      "Epoch: 1230 \tTraining Loss: 24.981617\n",
      "Epoch: 1231 \tTraining Loss: 24.981615\n",
      "Epoch: 1232 \tTraining Loss: 24.981615\n",
      "Epoch: 1233 \tTraining Loss: 24.981615\n",
      "Epoch: 1234 \tTraining Loss: 24.981615\n",
      "Epoch: 1235 \tTraining Loss: 24.981611\n",
      "Epoch: 1236 \tTraining Loss: 24.981611\n",
      "Epoch: 1237 \tTraining Loss: 24.981609\n",
      "Epoch: 1238 \tTraining Loss: 24.981609\n",
      "Epoch: 1239 \tTraining Loss: 24.981609\n",
      "Epoch: 1240 \tTraining Loss: 24.981609\n",
      "Epoch: 1241 \tTraining Loss: 24.981607\n",
      "Epoch: 1242 \tTraining Loss: 24.981607\n",
      "Epoch: 1243 \tTraining Loss: 24.981607\n",
      "Epoch: 1244 \tTraining Loss: 24.981607\n",
      "Epoch: 1245 \tTraining Loss: 24.981604\n",
      "Epoch: 1246 \tTraining Loss: 24.981607\n",
      "Epoch: 1247 \tTraining Loss: 24.981604\n",
      "Epoch: 1248 \tTraining Loss: 24.981604\n",
      "Epoch: 1249 \tTraining Loss: 24.981604\n",
      "Epoch: 1250 \tTraining Loss: 24.981604\n",
      "Epoch: 1251 \tTraining Loss: 24.981602\n",
      "Epoch: 1252 \tTraining Loss: 24.981598\n",
      "Epoch: 1253 \tTraining Loss: 24.981602\n",
      "Epoch: 1254 \tTraining Loss: 24.981598\n",
      "Epoch: 1255 \tTraining Loss: 24.981598\n",
      "Epoch: 1256 \tTraining Loss: 24.981598\n",
      "Epoch: 1257 \tTraining Loss: 24.981596\n",
      "Epoch: 1258 \tTraining Loss: 24.981596\n",
      "Epoch: 1259 \tTraining Loss: 24.981596\n",
      "Epoch: 1260 \tTraining Loss: 24.981596\n",
      "Epoch: 1261 \tTraining Loss: 24.981596\n",
      "Epoch: 1262 \tTraining Loss: 24.981594\n",
      "Epoch: 1263 \tTraining Loss: 24.981594\n",
      "Epoch: 1264 \tTraining Loss: 24.981594\n",
      "Epoch: 1265 \tTraining Loss: 24.981594\n",
      "Epoch: 1266 \tTraining Loss: 24.981590\n",
      "Epoch: 1267 \tTraining Loss: 24.981590\n",
      "Epoch: 1268 \tTraining Loss: 24.981590\n",
      "Epoch: 1269 \tTraining Loss: 24.981590\n",
      "Epoch: 1270 \tTraining Loss: 24.981590\n",
      "Epoch: 1271 \tTraining Loss: 24.981590\n",
      "Epoch: 1272 \tTraining Loss: 24.981588\n",
      "Epoch: 1273 \tTraining Loss: 24.981586\n",
      "Epoch: 1274 \tTraining Loss: 24.981588\n",
      "Epoch: 1275 \tTraining Loss: 24.981588\n",
      "Epoch: 1276 \tTraining Loss: 24.981588\n",
      "Epoch: 1277 \tTraining Loss: 24.981586\n",
      "Epoch: 1278 \tTraining Loss: 24.981586\n",
      "Epoch: 1279 \tTraining Loss: 24.981583\n",
      "Epoch: 1280 \tTraining Loss: 24.981583\n",
      "Epoch: 1281 \tTraining Loss: 24.981581\n",
      "Epoch: 1282 \tTraining Loss: 24.981583\n",
      "Epoch: 1283 \tTraining Loss: 24.981586\n",
      "Epoch: 1284 \tTraining Loss: 24.981586\n",
      "Epoch: 1285 \tTraining Loss: 24.981586\n",
      "Epoch: 1286 \tTraining Loss: 24.981586\n",
      "Epoch: 1287 \tTraining Loss: 24.981586\n",
      "Epoch: 1288 \tTraining Loss: 24.981581\n",
      "Epoch: 1289 \tTraining Loss: 24.981583\n",
      "Epoch: 1290 \tTraining Loss: 24.981581\n",
      "Epoch: 1291 \tTraining Loss: 24.981577\n",
      "Epoch: 1292 \tTraining Loss: 24.981575\n",
      "Epoch: 1293 \tTraining Loss: 24.981575\n",
      "Epoch: 1294 \tTraining Loss: 24.981577\n",
      "Epoch: 1295 \tTraining Loss: 24.981575\n",
      "Epoch: 1296 \tTraining Loss: 24.981575\n",
      "Epoch: 1297 \tTraining Loss: 24.981575\n",
      "Epoch: 1298 \tTraining Loss: 24.981575\n",
      "Epoch: 1299 \tTraining Loss: 24.981575\n",
      "Epoch: 1300 \tTraining Loss: 24.981575\n",
      "Epoch: 1301 \tTraining Loss: 24.981573\n",
      "Epoch: 1302 \tTraining Loss: 24.981573\n",
      "Epoch: 1303 \tTraining Loss: 24.981569\n",
      "Epoch: 1304 \tTraining Loss: 24.981569\n",
      "Epoch: 1305 \tTraining Loss: 24.981569\n",
      "Epoch: 1306 \tTraining Loss: 24.981567\n",
      "Epoch: 1307 \tTraining Loss: 24.981567\n",
      "Epoch: 1308 \tTraining Loss: 24.981567\n",
      "Epoch: 1309 \tTraining Loss: 24.981564\n",
      "Epoch: 1310 \tTraining Loss: 24.981567\n",
      "Epoch: 1311 \tTraining Loss: 24.981564\n",
      "Epoch: 1312 \tTraining Loss: 24.981564\n",
      "Epoch: 1313 \tTraining Loss: 24.981564\n",
      "Epoch: 1314 \tTraining Loss: 24.981562\n",
      "Epoch: 1315 \tTraining Loss: 24.981562\n",
      "Epoch: 1316 \tTraining Loss: 24.981562\n",
      "Epoch: 1317 \tTraining Loss: 24.981562\n",
      "Epoch: 1318 \tTraining Loss: 24.981560\n",
      "Epoch: 1319 \tTraining Loss: 24.981560\n",
      "Epoch: 1320 \tTraining Loss: 24.981560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1321 \tTraining Loss: 24.981560\n",
      "Epoch: 1322 \tTraining Loss: 24.981560\n",
      "Epoch: 1323 \tTraining Loss: 24.981556\n",
      "Epoch: 1324 \tTraining Loss: 24.981556\n",
      "Epoch: 1325 \tTraining Loss: 24.981554\n",
      "Epoch: 1326 \tTraining Loss: 24.981556\n",
      "Epoch: 1327 \tTraining Loss: 24.981554\n",
      "Epoch: 1328 \tTraining Loss: 24.981554\n",
      "Epoch: 1329 \tTraining Loss: 24.981552\n",
      "Epoch: 1330 \tTraining Loss: 24.981554\n",
      "Epoch: 1331 \tTraining Loss: 24.981552\n",
      "Epoch: 1332 \tTraining Loss: 24.981552\n",
      "Epoch: 1333 \tTraining Loss: 24.981552\n",
      "Epoch: 1334 \tTraining Loss: 24.981554\n",
      "Epoch: 1335 \tTraining Loss: 24.981552\n",
      "Epoch: 1336 \tTraining Loss: 24.981548\n",
      "Epoch: 1337 \tTraining Loss: 24.981552\n",
      "Epoch: 1338 \tTraining Loss: 24.981548\n",
      "Epoch: 1339 \tTraining Loss: 24.981546\n",
      "Epoch: 1340 \tTraining Loss: 24.981548\n",
      "Epoch: 1341 \tTraining Loss: 24.981546\n",
      "Epoch: 1342 \tTraining Loss: 24.981546\n",
      "Epoch: 1343 \tTraining Loss: 24.981546\n",
      "Epoch: 1344 \tTraining Loss: 24.981543\n",
      "Epoch: 1345 \tTraining Loss: 24.981543\n",
      "Epoch: 1346 \tTraining Loss: 24.981543\n",
      "Epoch: 1347 \tTraining Loss: 24.981543\n",
      "Epoch: 1348 \tTraining Loss: 24.981543\n",
      "Epoch: 1349 \tTraining Loss: 24.981541\n",
      "Epoch: 1350 \tTraining Loss: 24.981541\n",
      "Epoch: 1351 \tTraining Loss: 24.981539\n",
      "Epoch: 1352 \tTraining Loss: 24.981539\n",
      "Epoch: 1353 \tTraining Loss: 24.981539\n",
      "Epoch: 1354 \tTraining Loss: 24.981539\n",
      "Epoch: 1355 \tTraining Loss: 24.981539\n",
      "Epoch: 1356 \tTraining Loss: 24.981539\n",
      "Epoch: 1357 \tTraining Loss: 24.981539\n",
      "Epoch: 1358 \tTraining Loss: 24.981539\n",
      "Epoch: 1359 \tTraining Loss: 24.981539\n",
      "Epoch: 1360 \tTraining Loss: 24.981533\n",
      "Epoch: 1361 \tTraining Loss: 24.981533\n",
      "Epoch: 1362 \tTraining Loss: 24.981531\n",
      "Epoch: 1363 \tTraining Loss: 24.981533\n",
      "Epoch: 1364 \tTraining Loss: 24.981533\n",
      "Epoch: 1365 \tTraining Loss: 24.981527\n",
      "Epoch: 1366 \tTraining Loss: 24.981527\n",
      "Epoch: 1367 \tTraining Loss: 24.981527\n",
      "Epoch: 1368 \tTraining Loss: 24.981527\n",
      "Epoch: 1369 \tTraining Loss: 24.981527\n",
      "Epoch: 1370 \tTraining Loss: 24.981527\n",
      "Epoch: 1371 \tTraining Loss: 24.981527\n",
      "Epoch: 1372 \tTraining Loss: 24.981531\n",
      "Epoch: 1373 \tTraining Loss: 24.981525\n",
      "Epoch: 1374 \tTraining Loss: 24.981525\n",
      "Epoch: 1375 \tTraining Loss: 24.981525\n",
      "Epoch: 1376 \tTraining Loss: 24.981525\n",
      "Epoch: 1377 \tTraining Loss: 24.981525\n",
      "Epoch: 1378 \tTraining Loss: 24.981525\n",
      "Epoch: 1379 \tTraining Loss: 24.981525\n",
      "Epoch: 1380 \tTraining Loss: 24.981522\n",
      "Epoch: 1381 \tTraining Loss: 24.981525\n",
      "Epoch: 1382 \tTraining Loss: 24.981525\n",
      "Epoch: 1383 \tTraining Loss: 24.981520\n",
      "Epoch: 1384 \tTraining Loss: 24.981520\n",
      "Epoch: 1385 \tTraining Loss: 24.981525\n",
      "Epoch: 1386 \tTraining Loss: 24.981520\n",
      "Epoch: 1387 \tTraining Loss: 24.981520\n",
      "Epoch: 1388 \tTraining Loss: 24.981520\n",
      "Epoch: 1389 \tTraining Loss: 24.981520\n",
      "Epoch: 1390 \tTraining Loss: 24.981520\n",
      "Epoch: 1391 \tTraining Loss: 24.981520\n",
      "Epoch: 1392 \tTraining Loss: 24.981520\n",
      "Epoch: 1393 \tTraining Loss: 24.981520\n",
      "Epoch: 1394 \tTraining Loss: 24.981520\n",
      "Epoch: 1395 \tTraining Loss: 24.981520\n",
      "Epoch: 1396 \tTraining Loss: 24.981514\n",
      "Epoch: 1397 \tTraining Loss: 24.981514\n",
      "Epoch: 1398 \tTraining Loss: 24.981514\n",
      "Epoch: 1399 \tTraining Loss: 24.981514\n",
      "Epoch: 1400 \tTraining Loss: 24.981514\n",
      "Epoch: 1401 \tTraining Loss: 24.981514\n",
      "Epoch: 1402 \tTraining Loss: 24.981512\n",
      "Epoch: 1403 \tTraining Loss: 24.981512\n",
      "Epoch: 1404 \tTraining Loss: 24.981512\n",
      "Epoch: 1405 \tTraining Loss: 24.981512\n",
      "Epoch: 1406 \tTraining Loss: 24.981512\n",
      "Epoch: 1407 \tTraining Loss: 24.981512\n",
      "Epoch: 1408 \tTraining Loss: 24.981510\n",
      "Epoch: 1409 \tTraining Loss: 24.981504\n",
      "Epoch: 1410 \tTraining Loss: 24.981506\n",
      "Epoch: 1411 \tTraining Loss: 24.981506\n",
      "Epoch: 1412 \tTraining Loss: 24.981504\n",
      "Epoch: 1413 \tTraining Loss: 24.981504\n",
      "Epoch: 1414 \tTraining Loss: 24.981504\n",
      "Epoch: 1415 \tTraining Loss: 24.981504\n",
      "Epoch: 1416 \tTraining Loss: 24.981501\n",
      "Epoch: 1417 \tTraining Loss: 24.981501\n",
      "Epoch: 1418 \tTraining Loss: 24.981501\n",
      "Epoch: 1419 \tTraining Loss: 24.981501\n",
      "Epoch: 1420 \tTraining Loss: 24.981501\n",
      "Epoch: 1421 \tTraining Loss: 24.981497\n",
      "Epoch: 1422 \tTraining Loss: 24.981497\n",
      "Epoch: 1423 \tTraining Loss: 24.981497\n",
      "Epoch: 1424 \tTraining Loss: 24.981497\n",
      "Epoch: 1425 \tTraining Loss: 24.981493\n",
      "Epoch: 1426 \tTraining Loss: 24.981497\n",
      "Epoch: 1427 \tTraining Loss: 24.981493\n",
      "Epoch: 1428 \tTraining Loss: 24.981493\n",
      "Epoch: 1429 \tTraining Loss: 24.981493\n",
      "Epoch: 1430 \tTraining Loss: 24.981493\n",
      "Epoch: 1431 \tTraining Loss: 24.981491\n",
      "Epoch: 1432 \tTraining Loss: 24.981489\n",
      "Epoch: 1433 \tTraining Loss: 24.981489\n",
      "Epoch: 1434 \tTraining Loss: 24.981485\n",
      "Epoch: 1435 \tTraining Loss: 24.981485\n",
      "Epoch: 1436 \tTraining Loss: 24.981485\n",
      "Epoch: 1437 \tTraining Loss: 24.981485\n",
      "Epoch: 1438 \tTraining Loss: 24.981485\n",
      "Epoch: 1439 \tTraining Loss: 24.981485\n",
      "Epoch: 1440 \tTraining Loss: 24.981485\n",
      "Epoch: 1441 \tTraining Loss: 24.981483\n",
      "Epoch: 1442 \tTraining Loss: 24.981483\n",
      "Epoch: 1443 \tTraining Loss: 24.981483\n",
      "Epoch: 1444 \tTraining Loss: 24.981483\n",
      "Epoch: 1445 \tTraining Loss: 24.981483\n",
      "Epoch: 1446 \tTraining Loss: 24.981480\n",
      "Epoch: 1447 \tTraining Loss: 24.981480\n",
      "Epoch: 1448 \tTraining Loss: 24.981478\n",
      "Epoch: 1449 \tTraining Loss: 24.981478\n",
      "Epoch: 1450 \tTraining Loss: 24.981478\n",
      "Epoch: 1451 \tTraining Loss: 24.981480\n",
      "Epoch: 1452 \tTraining Loss: 24.981478\n",
      "Epoch: 1453 \tTraining Loss: 24.981478\n",
      "Epoch: 1454 \tTraining Loss: 24.981478\n",
      "Epoch: 1455 \tTraining Loss: 24.981472\n",
      "Epoch: 1456 \tTraining Loss: 24.981478\n",
      "Epoch: 1457 \tTraining Loss: 24.981476\n",
      "Epoch: 1458 \tTraining Loss: 24.981476\n",
      "Epoch: 1459 \tTraining Loss: 24.981476\n",
      "Epoch: 1460 \tTraining Loss: 24.981476\n",
      "Epoch: 1461 \tTraining Loss: 24.981478\n",
      "Epoch: 1462 \tTraining Loss: 24.981476\n",
      "Epoch: 1463 \tTraining Loss: 24.981472\n",
      "Epoch: 1464 \tTraining Loss: 24.981476\n",
      "Epoch: 1465 \tTraining Loss: 24.981470\n",
      "Epoch: 1466 \tTraining Loss: 24.981470\n",
      "Epoch: 1467 \tTraining Loss: 24.981470\n",
      "Epoch: 1468 \tTraining Loss: 24.981470\n",
      "Epoch: 1469 \tTraining Loss: 24.981470\n",
      "Epoch: 1470 \tTraining Loss: 24.981470\n",
      "Epoch: 1471 \tTraining Loss: 24.981470\n",
      "Epoch: 1472 \tTraining Loss: 24.981470\n",
      "Epoch: 1473 \tTraining Loss: 24.981470\n",
      "Epoch: 1474 \tTraining Loss: 24.981468\n",
      "Epoch: 1475 \tTraining Loss: 24.981464\n",
      "Epoch: 1476 \tTraining Loss: 24.981464\n",
      "Epoch: 1477 \tTraining Loss: 24.981462\n",
      "Epoch: 1478 \tTraining Loss: 24.981462\n",
      "Epoch: 1479 \tTraining Loss: 24.981462\n",
      "Epoch: 1480 \tTraining Loss: 24.981462\n",
      "Epoch: 1481 \tTraining Loss: 24.981462\n",
      "Epoch: 1482 \tTraining Loss: 24.981459\n",
      "Epoch: 1483 \tTraining Loss: 24.981459\n",
      "Epoch: 1484 \tTraining Loss: 24.981462\n",
      "Epoch: 1485 \tTraining Loss: 24.981459\n",
      "Epoch: 1486 \tTraining Loss: 24.981457\n",
      "Epoch: 1487 \tTraining Loss: 24.981457\n",
      "Epoch: 1488 \tTraining Loss: 24.981457\n",
      "Epoch: 1489 \tTraining Loss: 24.981455\n",
      "Epoch: 1490 \tTraining Loss: 24.981451\n",
      "Epoch: 1491 \tTraining Loss: 24.981451\n",
      "Epoch: 1492 \tTraining Loss: 24.981451\n",
      "Epoch: 1493 \tTraining Loss: 24.981455\n",
      "Epoch: 1494 \tTraining Loss: 24.981451\n",
      "Epoch: 1495 \tTraining Loss: 24.981449\n",
      "Epoch: 1496 \tTraining Loss: 24.981449\n",
      "Epoch: 1497 \tTraining Loss: 24.981449\n",
      "Epoch: 1498 \tTraining Loss: 24.981449\n",
      "Epoch: 1499 \tTraining Loss: 24.981449\n",
      "Epoch: 1500 \tTraining Loss: 24.981447\n",
      "Epoch: 1501 \tTraining Loss: 24.981447\n",
      "Epoch: 1502 \tTraining Loss: 24.981443\n",
      "Epoch: 1503 \tTraining Loss: 24.981441\n",
      "Epoch: 1504 \tTraining Loss: 24.981443\n",
      "Epoch: 1505 \tTraining Loss: 24.981443\n",
      "Epoch: 1506 \tTraining Loss: 24.981443\n",
      "Epoch: 1507 \tTraining Loss: 24.981443\n",
      "Epoch: 1508 \tTraining Loss: 24.981441\n",
      "Epoch: 1509 \tTraining Loss: 24.981441\n",
      "Epoch: 1510 \tTraining Loss: 24.981441\n",
      "Epoch: 1511 \tTraining Loss: 24.981443\n",
      "Epoch: 1512 \tTraining Loss: 24.981443\n",
      "Epoch: 1513 \tTraining Loss: 24.981441\n",
      "Epoch: 1514 \tTraining Loss: 24.981441\n",
      "Epoch: 1515 \tTraining Loss: 24.981438\n",
      "Epoch: 1516 \tTraining Loss: 24.981438\n",
      "Epoch: 1517 \tTraining Loss: 24.981438\n",
      "Epoch: 1518 \tTraining Loss: 24.981436\n",
      "Epoch: 1519 \tTraining Loss: 24.981436\n",
      "Epoch: 1520 \tTraining Loss: 24.981436\n",
      "Epoch: 1521 \tTraining Loss: 24.981436\n",
      "Epoch: 1522 \tTraining Loss: 24.981436\n",
      "Epoch: 1523 \tTraining Loss: 24.981436\n",
      "Epoch: 1524 \tTraining Loss: 24.981430\n",
      "Epoch: 1525 \tTraining Loss: 24.981430\n",
      "Epoch: 1526 \tTraining Loss: 24.981430\n",
      "Epoch: 1527 \tTraining Loss: 24.981428\n",
      "Epoch: 1528 \tTraining Loss: 24.981424\n",
      "Epoch: 1529 \tTraining Loss: 24.981428\n",
      "Epoch: 1530 \tTraining Loss: 24.981428\n",
      "Epoch: 1531 \tTraining Loss: 24.981428\n",
      "Epoch: 1532 \tTraining Loss: 24.981424\n",
      "Epoch: 1533 \tTraining Loss: 24.981428\n",
      "Epoch: 1534 \tTraining Loss: 24.981424\n",
      "Epoch: 1535 \tTraining Loss: 24.981424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1536 \tTraining Loss: 24.981424\n",
      "Epoch: 1537 \tTraining Loss: 24.981424\n",
      "Epoch: 1538 \tTraining Loss: 24.981422\n",
      "Epoch: 1539 \tTraining Loss: 24.981421\n",
      "Epoch: 1540 \tTraining Loss: 24.981421\n",
      "Epoch: 1541 \tTraining Loss: 24.981421\n",
      "Epoch: 1542 \tTraining Loss: 24.981421\n",
      "Epoch: 1543 \tTraining Loss: 24.981417\n",
      "Epoch: 1544 \tTraining Loss: 24.981417\n",
      "Epoch: 1545 \tTraining Loss: 24.981417\n",
      "Epoch: 1546 \tTraining Loss: 24.981417\n",
      "Epoch: 1547 \tTraining Loss: 24.981417\n",
      "Epoch: 1548 \tTraining Loss: 24.981417\n",
      "Epoch: 1549 \tTraining Loss: 24.981417\n",
      "Epoch: 1550 \tTraining Loss: 24.981415\n",
      "Epoch: 1551 \tTraining Loss: 24.981417\n",
      "Epoch: 1552 \tTraining Loss: 24.981413\n",
      "Epoch: 1553 \tTraining Loss: 24.981415\n",
      "Epoch: 1554 \tTraining Loss: 24.981413\n",
      "Epoch: 1555 \tTraining Loss: 24.981413\n",
      "Epoch: 1556 \tTraining Loss: 24.981413\n",
      "Epoch: 1557 \tTraining Loss: 24.981413\n",
      "Epoch: 1558 \tTraining Loss: 24.981413\n",
      "Epoch: 1559 \tTraining Loss: 24.981413\n",
      "Epoch: 1560 \tTraining Loss: 24.981413\n",
      "Epoch: 1561 \tTraining Loss: 24.981409\n",
      "Epoch: 1562 \tTraining Loss: 24.981409\n",
      "Epoch: 1563 \tTraining Loss: 24.981407\n",
      "Epoch: 1564 \tTraining Loss: 24.981407\n",
      "Epoch: 1565 \tTraining Loss: 24.981403\n",
      "Epoch: 1566 \tTraining Loss: 24.981403\n",
      "Epoch: 1567 \tTraining Loss: 24.981403\n",
      "Epoch: 1568 \tTraining Loss: 24.981407\n",
      "Epoch: 1569 \tTraining Loss: 24.981403\n",
      "Epoch: 1570 \tTraining Loss: 24.981407\n",
      "Epoch: 1571 \tTraining Loss: 24.981403\n",
      "Epoch: 1572 \tTraining Loss: 24.981403\n",
      "Epoch: 1573 \tTraining Loss: 24.981401\n",
      "Epoch: 1574 \tTraining Loss: 24.981401\n",
      "Epoch: 1575 \tTraining Loss: 24.981401\n",
      "Epoch: 1576 \tTraining Loss: 24.981401\n",
      "Epoch: 1577 \tTraining Loss: 24.981400\n",
      "Epoch: 1578 \tTraining Loss: 24.981400\n",
      "Epoch: 1579 \tTraining Loss: 24.981400\n",
      "Epoch: 1580 \tTraining Loss: 24.981396\n",
      "Epoch: 1581 \tTraining Loss: 24.981396\n",
      "Epoch: 1582 \tTraining Loss: 24.981396\n",
      "Epoch: 1583 \tTraining Loss: 24.981396\n",
      "Epoch: 1584 \tTraining Loss: 24.981394\n",
      "Epoch: 1585 \tTraining Loss: 24.981394\n",
      "Epoch: 1586 \tTraining Loss: 24.981394\n",
      "Epoch: 1587 \tTraining Loss: 24.981392\n",
      "Epoch: 1588 \tTraining Loss: 24.981392\n",
      "Epoch: 1589 \tTraining Loss: 24.981392\n",
      "Epoch: 1590 \tTraining Loss: 24.981388\n",
      "Epoch: 1591 \tTraining Loss: 24.981392\n",
      "Epoch: 1592 \tTraining Loss: 24.981388\n",
      "Epoch: 1593 \tTraining Loss: 24.981388\n",
      "Epoch: 1594 \tTraining Loss: 24.981386\n",
      "Epoch: 1595 \tTraining Loss: 24.981386\n",
      "Epoch: 1596 \tTraining Loss: 24.981382\n",
      "Epoch: 1597 \tTraining Loss: 24.981382\n",
      "Epoch: 1598 \tTraining Loss: 24.981382\n",
      "Epoch: 1599 \tTraining Loss: 24.981380\n",
      "Epoch: 1600 \tTraining Loss: 24.981380\n",
      "Epoch: 1601 \tTraining Loss: 24.981380\n",
      "Epoch: 1602 \tTraining Loss: 24.981380\n",
      "Epoch: 1603 \tTraining Loss: 24.981379\n",
      "Epoch: 1604 \tTraining Loss: 24.981379\n",
      "Epoch: 1605 \tTraining Loss: 24.981379\n",
      "Epoch: 1606 \tTraining Loss: 24.981375\n",
      "Epoch: 1607 \tTraining Loss: 24.981379\n",
      "Epoch: 1608 \tTraining Loss: 24.981375\n",
      "Epoch: 1609 \tTraining Loss: 24.981375\n",
      "Epoch: 1610 \tTraining Loss: 24.981375\n",
      "Epoch: 1611 \tTraining Loss: 24.981375\n",
      "Epoch: 1612 \tTraining Loss: 24.981375\n",
      "Epoch: 1613 \tTraining Loss: 24.981373\n",
      "Epoch: 1614 \tTraining Loss: 24.981373\n",
      "Epoch: 1615 \tTraining Loss: 24.981373\n",
      "Epoch: 1616 \tTraining Loss: 24.981373\n",
      "Epoch: 1617 \tTraining Loss: 24.981373\n",
      "Epoch: 1618 \tTraining Loss: 24.981371\n",
      "Epoch: 1619 \tTraining Loss: 24.981371\n",
      "Epoch: 1620 \tTraining Loss: 24.981371\n",
      "Epoch: 1621 \tTraining Loss: 24.981371\n",
      "Epoch: 1622 \tTraining Loss: 24.981371\n",
      "Epoch: 1623 \tTraining Loss: 24.981367\n",
      "Epoch: 1624 \tTraining Loss: 24.981367\n",
      "Epoch: 1625 \tTraining Loss: 24.981367\n",
      "Epoch: 1626 \tTraining Loss: 24.981367\n",
      "Epoch: 1627 \tTraining Loss: 24.981367\n",
      "Epoch: 1628 \tTraining Loss: 24.981365\n",
      "Epoch: 1629 \tTraining Loss: 24.981365\n",
      "Epoch: 1630 \tTraining Loss: 24.981361\n",
      "Epoch: 1631 \tTraining Loss: 24.981365\n",
      "Epoch: 1632 \tTraining Loss: 24.981365\n",
      "Epoch: 1633 \tTraining Loss: 24.981365\n",
      "Epoch: 1634 \tTraining Loss: 24.981361\n",
      "Epoch: 1635 \tTraining Loss: 24.981359\n",
      "Epoch: 1636 \tTraining Loss: 24.981361\n",
      "Epoch: 1637 \tTraining Loss: 24.981361\n",
      "Epoch: 1638 \tTraining Loss: 24.981361\n",
      "Epoch: 1639 \tTraining Loss: 24.981361\n",
      "Epoch: 1640 \tTraining Loss: 24.981359\n",
      "Epoch: 1641 \tTraining Loss: 24.981358\n",
      "Epoch: 1642 \tTraining Loss: 24.981358\n",
      "Epoch: 1643 \tTraining Loss: 24.981358\n",
      "Epoch: 1644 \tTraining Loss: 24.981358\n",
      "Epoch: 1645 \tTraining Loss: 24.981358\n",
      "Epoch: 1646 \tTraining Loss: 24.981358\n",
      "Epoch: 1647 \tTraining Loss: 24.981354\n",
      "Epoch: 1648 \tTraining Loss: 24.981354\n",
      "Epoch: 1649 \tTraining Loss: 24.981354\n",
      "Epoch: 1650 \tTraining Loss: 24.981354\n",
      "Epoch: 1651 \tTraining Loss: 24.981354\n",
      "Epoch: 1652 \tTraining Loss: 24.981352\n",
      "Epoch: 1653 \tTraining Loss: 24.981350\n",
      "Epoch: 1654 \tTraining Loss: 24.981350\n",
      "Epoch: 1655 \tTraining Loss: 24.981350\n",
      "Epoch: 1656 \tTraining Loss: 24.981350\n",
      "Epoch: 1657 \tTraining Loss: 24.981350\n",
      "Epoch: 1658 \tTraining Loss: 24.981346\n",
      "Epoch: 1659 \tTraining Loss: 24.981346\n",
      "Epoch: 1660 \tTraining Loss: 24.981346\n",
      "Epoch: 1661 \tTraining Loss: 24.981346\n",
      "Epoch: 1662 \tTraining Loss: 24.981344\n",
      "Epoch: 1663 \tTraining Loss: 24.981340\n",
      "Epoch: 1664 \tTraining Loss: 24.981344\n",
      "Epoch: 1665 \tTraining Loss: 24.981340\n",
      "Epoch: 1666 \tTraining Loss: 24.981340\n",
      "Epoch: 1667 \tTraining Loss: 24.981340\n",
      "Epoch: 1668 \tTraining Loss: 24.981339\n",
      "Epoch: 1669 \tTraining Loss: 24.981339\n",
      "Epoch: 1670 \tTraining Loss: 24.981339\n",
      "Epoch: 1671 \tTraining Loss: 24.981339\n",
      "Epoch: 1672 \tTraining Loss: 24.981339\n",
      "Epoch: 1673 \tTraining Loss: 24.981337\n",
      "Epoch: 1674 \tTraining Loss: 24.981333\n",
      "Epoch: 1675 \tTraining Loss: 24.981331\n",
      "Epoch: 1676 \tTraining Loss: 24.981331\n",
      "Epoch: 1677 \tTraining Loss: 24.981331\n",
      "Epoch: 1678 \tTraining Loss: 24.981331\n",
      "Epoch: 1679 \tTraining Loss: 24.981331\n",
      "Epoch: 1680 \tTraining Loss: 24.981333\n",
      "Epoch: 1681 \tTraining Loss: 24.981329\n",
      "Epoch: 1682 \tTraining Loss: 24.981331\n",
      "Epoch: 1683 \tTraining Loss: 24.981331\n",
      "Epoch: 1684 \tTraining Loss: 24.981333\n",
      "Epoch: 1685 \tTraining Loss: 24.981331\n",
      "Epoch: 1686 \tTraining Loss: 24.981331\n",
      "Epoch: 1687 \tTraining Loss: 24.981329\n",
      "Epoch: 1688 \tTraining Loss: 24.981329\n",
      "Epoch: 1689 \tTraining Loss: 24.981329\n",
      "Epoch: 1690 \tTraining Loss: 24.981325\n",
      "Epoch: 1691 \tTraining Loss: 24.981329\n",
      "Epoch: 1692 \tTraining Loss: 24.981329\n",
      "Epoch: 1693 \tTraining Loss: 24.981325\n",
      "Epoch: 1694 \tTraining Loss: 24.981325\n",
      "Epoch: 1695 \tTraining Loss: 24.981325\n",
      "Epoch: 1696 \tTraining Loss: 24.981323\n",
      "Epoch: 1697 \tTraining Loss: 24.981323\n",
      "Epoch: 1698 \tTraining Loss: 24.981323\n",
      "Epoch: 1699 \tTraining Loss: 24.981318\n",
      "Epoch: 1700 \tTraining Loss: 24.981318\n",
      "Epoch: 1701 \tTraining Loss: 24.981318\n",
      "Epoch: 1702 \tTraining Loss: 24.981318\n",
      "Epoch: 1703 \tTraining Loss: 24.981318\n",
      "Epoch: 1704 \tTraining Loss: 24.981318\n",
      "Epoch: 1705 \tTraining Loss: 24.981318\n",
      "Epoch: 1706 \tTraining Loss: 24.981316\n",
      "Epoch: 1707 \tTraining Loss: 24.981316\n",
      "Epoch: 1708 \tTraining Loss: 24.981312\n",
      "Epoch: 1709 \tTraining Loss: 24.981312\n",
      "Epoch: 1710 \tTraining Loss: 24.981312\n",
      "Epoch: 1711 \tTraining Loss: 24.981312\n",
      "Epoch: 1712 \tTraining Loss: 24.981312\n",
      "Epoch: 1713 \tTraining Loss: 24.981312\n",
      "Epoch: 1714 \tTraining Loss: 24.981312\n",
      "Epoch: 1715 \tTraining Loss: 24.981312\n",
      "Epoch: 1716 \tTraining Loss: 24.981312\n",
      "Epoch: 1717 \tTraining Loss: 24.981310\n",
      "Epoch: 1718 \tTraining Loss: 24.981310\n",
      "Epoch: 1719 \tTraining Loss: 24.981308\n",
      "Epoch: 1720 \tTraining Loss: 24.981308\n",
      "Epoch: 1721 \tTraining Loss: 24.981308\n",
      "Epoch: 1722 \tTraining Loss: 24.981308\n",
      "Epoch: 1723 \tTraining Loss: 24.981304\n",
      "Epoch: 1724 \tTraining Loss: 24.981304\n",
      "Epoch: 1725 \tTraining Loss: 24.981304\n",
      "Epoch: 1726 \tTraining Loss: 24.981304\n",
      "Epoch: 1727 \tTraining Loss: 24.981304\n",
      "Epoch: 1728 \tTraining Loss: 24.981304\n",
      "Epoch: 1729 \tTraining Loss: 24.981302\n",
      "Epoch: 1730 \tTraining Loss: 24.981302\n",
      "Epoch: 1731 \tTraining Loss: 24.981298\n",
      "Epoch: 1732 \tTraining Loss: 24.981297\n",
      "Epoch: 1733 \tTraining Loss: 24.981298\n",
      "Epoch: 1734 \tTraining Loss: 24.981297\n",
      "Epoch: 1735 \tTraining Loss: 24.981297\n",
      "Epoch: 1736 \tTraining Loss: 24.981297\n",
      "Epoch: 1737 \tTraining Loss: 24.981297\n",
      "Epoch: 1738 \tTraining Loss: 24.981297\n",
      "Epoch: 1739 \tTraining Loss: 24.981295\n",
      "Epoch: 1740 \tTraining Loss: 24.981295\n",
      "Epoch: 1741 \tTraining Loss: 24.981295\n",
      "Epoch: 1742 \tTraining Loss: 24.981295\n",
      "Epoch: 1743 \tTraining Loss: 24.981291\n",
      "Epoch: 1744 \tTraining Loss: 24.981291\n",
      "Epoch: 1745 \tTraining Loss: 24.981291\n",
      "Epoch: 1746 \tTraining Loss: 24.981291\n",
      "Epoch: 1747 \tTraining Loss: 24.981289\n",
      "Epoch: 1748 \tTraining Loss: 24.981289\n",
      "Epoch: 1749 \tTraining Loss: 24.981289\n",
      "Epoch: 1750 \tTraining Loss: 24.981287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1751 \tTraining Loss: 24.981287\n",
      "Epoch: 1752 \tTraining Loss: 24.981287\n",
      "Epoch: 1753 \tTraining Loss: 24.981287\n",
      "Epoch: 1754 \tTraining Loss: 24.981283\n",
      "Epoch: 1755 \tTraining Loss: 24.981283\n",
      "Epoch: 1756 \tTraining Loss: 24.981283\n",
      "Epoch: 1757 \tTraining Loss: 24.981283\n",
      "Epoch: 1758 \tTraining Loss: 24.981283\n",
      "Epoch: 1759 \tTraining Loss: 24.981281\n",
      "Epoch: 1760 \tTraining Loss: 24.981281\n",
      "Epoch: 1761 \tTraining Loss: 24.981283\n",
      "Epoch: 1762 \tTraining Loss: 24.981281\n",
      "Epoch: 1763 \tTraining Loss: 24.981277\n",
      "Epoch: 1764 \tTraining Loss: 24.981277\n",
      "Epoch: 1765 \tTraining Loss: 24.981276\n",
      "Epoch: 1766 \tTraining Loss: 24.981276\n",
      "Epoch: 1767 \tTraining Loss: 24.981276\n",
      "Epoch: 1768 \tTraining Loss: 24.981276\n",
      "Epoch: 1769 \tTraining Loss: 24.981276\n",
      "Epoch: 1770 \tTraining Loss: 24.981274\n",
      "Epoch: 1771 \tTraining Loss: 24.981274\n",
      "Epoch: 1772 \tTraining Loss: 24.981274\n",
      "Epoch: 1773 \tTraining Loss: 24.981274\n",
      "Epoch: 1774 \tTraining Loss: 24.981270\n",
      "Epoch: 1775 \tTraining Loss: 24.981270\n",
      "Epoch: 1776 \tTraining Loss: 24.981270\n",
      "Epoch: 1777 \tTraining Loss: 24.981270\n",
      "Epoch: 1778 \tTraining Loss: 24.981270\n",
      "Epoch: 1779 \tTraining Loss: 24.981264\n",
      "Epoch: 1780 \tTraining Loss: 24.981268\n",
      "Epoch: 1781 \tTraining Loss: 24.981264\n",
      "Epoch: 1782 \tTraining Loss: 24.981264\n",
      "Epoch: 1783 \tTraining Loss: 24.981262\n",
      "Epoch: 1784 \tTraining Loss: 24.981262\n",
      "Epoch: 1785 \tTraining Loss: 24.981262\n",
      "Epoch: 1786 \tTraining Loss: 24.981260\n",
      "Epoch: 1787 \tTraining Loss: 24.981260\n",
      "Epoch: 1788 \tTraining Loss: 24.981260\n",
      "Epoch: 1789 \tTraining Loss: 24.981260\n",
      "Epoch: 1790 \tTraining Loss: 24.981260\n",
      "Epoch: 1791 \tTraining Loss: 24.981260\n",
      "Epoch: 1792 \tTraining Loss: 24.981256\n",
      "Epoch: 1793 \tTraining Loss: 24.981256\n",
      "Epoch: 1794 \tTraining Loss: 24.981255\n",
      "Epoch: 1795 \tTraining Loss: 24.981256\n",
      "Epoch: 1796 \tTraining Loss: 24.981255\n",
      "Epoch: 1797 \tTraining Loss: 24.981253\n",
      "Epoch: 1798 \tTraining Loss: 24.981255\n",
      "Epoch: 1799 \tTraining Loss: 24.981253\n",
      "Epoch: 1800 \tTraining Loss: 24.981255\n",
      "Epoch: 1801 \tTraining Loss: 24.981255\n",
      "Epoch: 1802 \tTraining Loss: 24.981255\n",
      "Epoch: 1803 \tTraining Loss: 24.981255\n",
      "Epoch: 1804 \tTraining Loss: 24.981255\n",
      "Epoch: 1805 \tTraining Loss: 24.981255\n",
      "Epoch: 1806 \tTraining Loss: 24.981253\n",
      "Epoch: 1807 \tTraining Loss: 24.981253\n",
      "Epoch: 1808 \tTraining Loss: 24.981249\n",
      "Epoch: 1809 \tTraining Loss: 24.981249\n",
      "Epoch: 1810 \tTraining Loss: 24.981249\n",
      "Epoch: 1811 \tTraining Loss: 24.981247\n",
      "Epoch: 1812 \tTraining Loss: 24.981247\n",
      "Epoch: 1813 \tTraining Loss: 24.981247\n",
      "Epoch: 1814 \tTraining Loss: 24.981247\n",
      "Epoch: 1815 \tTraining Loss: 24.981243\n",
      "Epoch: 1816 \tTraining Loss: 24.981243\n",
      "Epoch: 1817 \tTraining Loss: 24.981243\n",
      "Epoch: 1818 \tTraining Loss: 24.981243\n",
      "Epoch: 1819 \tTraining Loss: 24.981241\n",
      "Epoch: 1820 \tTraining Loss: 24.981241\n",
      "Epoch: 1821 \tTraining Loss: 24.981241\n",
      "Epoch: 1822 \tTraining Loss: 24.981241\n",
      "Epoch: 1823 \tTraining Loss: 24.981241\n",
      "Epoch: 1824 \tTraining Loss: 24.981239\n",
      "Epoch: 1825 \tTraining Loss: 24.981239\n",
      "Epoch: 1826 \tTraining Loss: 24.981239\n",
      "Epoch: 1827 \tTraining Loss: 24.981239\n",
      "Epoch: 1828 \tTraining Loss: 24.981239\n",
      "Epoch: 1829 \tTraining Loss: 24.981236\n",
      "Epoch: 1830 \tTraining Loss: 24.981234\n",
      "Epoch: 1831 \tTraining Loss: 24.981234\n",
      "Epoch: 1832 \tTraining Loss: 24.981236\n",
      "Epoch: 1833 \tTraining Loss: 24.981234\n",
      "Epoch: 1834 \tTraining Loss: 24.981234\n",
      "Epoch: 1835 \tTraining Loss: 24.981232\n",
      "Epoch: 1836 \tTraining Loss: 24.981232\n",
      "Epoch: 1837 \tTraining Loss: 24.981228\n",
      "Epoch: 1838 \tTraining Loss: 24.981228\n",
      "Epoch: 1839 \tTraining Loss: 24.981228\n",
      "Epoch: 1840 \tTraining Loss: 24.981228\n",
      "Epoch: 1841 \tTraining Loss: 24.981226\n",
      "Epoch: 1842 \tTraining Loss: 24.981226\n",
      "Epoch: 1843 \tTraining Loss: 24.981226\n",
      "Epoch: 1844 \tTraining Loss: 24.981222\n",
      "Epoch: 1845 \tTraining Loss: 24.981222\n",
      "Epoch: 1846 \tTraining Loss: 24.981222\n",
      "Epoch: 1847 \tTraining Loss: 24.981220\n",
      "Epoch: 1848 \tTraining Loss: 24.981220\n",
      "Epoch: 1849 \tTraining Loss: 24.981220\n",
      "Epoch: 1850 \tTraining Loss: 24.981220\n",
      "Epoch: 1851 \tTraining Loss: 24.981220\n",
      "Epoch: 1852 \tTraining Loss: 24.981218\n",
      "Epoch: 1853 \tTraining Loss: 24.981218\n",
      "Epoch: 1854 \tTraining Loss: 24.981218\n",
      "Epoch: 1855 \tTraining Loss: 24.981218\n",
      "Epoch: 1856 \tTraining Loss: 24.981218\n",
      "Epoch: 1857 \tTraining Loss: 24.981215\n",
      "Epoch: 1858 \tTraining Loss: 24.981215\n",
      "Epoch: 1859 \tTraining Loss: 24.981213\n",
      "Epoch: 1860 \tTraining Loss: 24.981215\n",
      "Epoch: 1861 \tTraining Loss: 24.981213\n",
      "Epoch: 1862 \tTraining Loss: 24.981211\n",
      "Epoch: 1863 \tTraining Loss: 24.981211\n",
      "Epoch: 1864 \tTraining Loss: 24.981211\n",
      "Epoch: 1865 \tTraining Loss: 24.981213\n",
      "Epoch: 1866 \tTraining Loss: 24.981211\n",
      "Epoch: 1867 \tTraining Loss: 24.981211\n",
      "Epoch: 1868 \tTraining Loss: 24.981211\n",
      "Epoch: 1869 \tTraining Loss: 24.981211\n",
      "Epoch: 1870 \tTraining Loss: 24.981207\n",
      "Epoch: 1871 \tTraining Loss: 24.981211\n",
      "Epoch: 1872 \tTraining Loss: 24.981207\n",
      "Epoch: 1873 \tTraining Loss: 24.981205\n",
      "Epoch: 1874 \tTraining Loss: 24.981207\n",
      "Epoch: 1875 \tTraining Loss: 24.981207\n",
      "Epoch: 1876 \tTraining Loss: 24.981207\n",
      "Epoch: 1877 \tTraining Loss: 24.981205\n",
      "Epoch: 1878 \tTraining Loss: 24.981205\n",
      "Epoch: 1879 \tTraining Loss: 24.981205\n",
      "Epoch: 1880 \tTraining Loss: 24.981205\n",
      "Epoch: 1881 \tTraining Loss: 24.981201\n",
      "Epoch: 1882 \tTraining Loss: 24.981201\n",
      "Epoch: 1883 \tTraining Loss: 24.981201\n",
      "Epoch: 1884 \tTraining Loss: 24.981199\n",
      "Epoch: 1885 \tTraining Loss: 24.981199\n",
      "Epoch: 1886 \tTraining Loss: 24.981199\n",
      "Epoch: 1887 \tTraining Loss: 24.981199\n",
      "Epoch: 1888 \tTraining Loss: 24.981197\n",
      "Epoch: 1889 \tTraining Loss: 24.981197\n",
      "Epoch: 1890 \tTraining Loss: 24.981194\n",
      "Epoch: 1891 \tTraining Loss: 24.981194\n",
      "Epoch: 1892 \tTraining Loss: 24.981192\n",
      "Epoch: 1893 \tTraining Loss: 24.981192\n",
      "Epoch: 1894 \tTraining Loss: 24.981190\n",
      "Epoch: 1895 \tTraining Loss: 24.981190\n",
      "Epoch: 1896 \tTraining Loss: 24.981190\n",
      "Epoch: 1897 \tTraining Loss: 24.981190\n",
      "Epoch: 1898 \tTraining Loss: 24.981190\n",
      "Epoch: 1899 \tTraining Loss: 24.981186\n",
      "Epoch: 1900 \tTraining Loss: 24.981186\n",
      "Epoch: 1901 \tTraining Loss: 24.981186\n",
      "Epoch: 1902 \tTraining Loss: 24.981186\n",
      "Epoch: 1903 \tTraining Loss: 24.981184\n",
      "Epoch: 1904 \tTraining Loss: 24.981184\n",
      "Epoch: 1905 \tTraining Loss: 24.981184\n",
      "Epoch: 1906 \tTraining Loss: 24.981184\n",
      "Epoch: 1907 \tTraining Loss: 24.981180\n",
      "Epoch: 1908 \tTraining Loss: 24.981180\n",
      "Epoch: 1909 \tTraining Loss: 24.981184\n",
      "Epoch: 1910 \tTraining Loss: 24.981180\n",
      "Epoch: 1911 \tTraining Loss: 24.981180\n",
      "Epoch: 1912 \tTraining Loss: 24.981180\n",
      "Epoch: 1913 \tTraining Loss: 24.981176\n",
      "Epoch: 1914 \tTraining Loss: 24.981178\n",
      "Epoch: 1915 \tTraining Loss: 24.981178\n",
      "Epoch: 1916 \tTraining Loss: 24.981176\n",
      "Epoch: 1917 \tTraining Loss: 24.981176\n",
      "Epoch: 1918 \tTraining Loss: 24.981176\n",
      "Epoch: 1919 \tTraining Loss: 24.981176\n",
      "Epoch: 1920 \tTraining Loss: 24.981176\n",
      "Epoch: 1921 \tTraining Loss: 24.981173\n",
      "Epoch: 1922 \tTraining Loss: 24.981173\n",
      "Epoch: 1923 \tTraining Loss: 24.981173\n",
      "Epoch: 1924 \tTraining Loss: 24.981171\n",
      "Epoch: 1925 \tTraining Loss: 24.981171\n",
      "Epoch: 1926 \tTraining Loss: 24.981171\n",
      "Epoch: 1927 \tTraining Loss: 24.981169\n",
      "Epoch: 1928 \tTraining Loss: 24.981169\n",
      "Epoch: 1929 \tTraining Loss: 24.981169\n",
      "Epoch: 1930 \tTraining Loss: 24.981165\n",
      "Epoch: 1931 \tTraining Loss: 24.981165\n",
      "Epoch: 1932 \tTraining Loss: 24.981165\n",
      "Epoch: 1933 \tTraining Loss: 24.981165\n",
      "Epoch: 1934 \tTraining Loss: 24.981163\n",
      "Epoch: 1935 \tTraining Loss: 24.981163\n",
      "Epoch: 1936 \tTraining Loss: 24.981163\n",
      "Epoch: 1937 \tTraining Loss: 24.981165\n",
      "Epoch: 1938 \tTraining Loss: 24.981163\n",
      "Epoch: 1939 \tTraining Loss: 24.981159\n",
      "Epoch: 1940 \tTraining Loss: 24.981159\n",
      "Epoch: 1941 \tTraining Loss: 24.981159\n",
      "Epoch: 1942 \tTraining Loss: 24.981157\n",
      "Epoch: 1943 \tTraining Loss: 24.981157\n",
      "Epoch: 1944 \tTraining Loss: 24.981157\n",
      "Epoch: 1945 \tTraining Loss: 24.981159\n",
      "Epoch: 1946 \tTraining Loss: 24.981155\n",
      "Epoch: 1947 \tTraining Loss: 24.981155\n",
      "Epoch: 1948 \tTraining Loss: 24.981155\n",
      "Epoch: 1949 \tTraining Loss: 24.981155\n",
      "Epoch: 1950 \tTraining Loss: 24.981155\n",
      "Epoch: 1951 \tTraining Loss: 24.981155\n",
      "Epoch: 1952 \tTraining Loss: 24.981150\n",
      "Epoch: 1953 \tTraining Loss: 24.981150\n",
      "Epoch: 1954 \tTraining Loss: 24.981150\n",
      "Epoch: 1955 \tTraining Loss: 24.981150\n",
      "Epoch: 1956 \tTraining Loss: 24.981150\n",
      "Epoch: 1957 \tTraining Loss: 24.981144\n",
      "Epoch: 1958 \tTraining Loss: 24.981148\n",
      "Epoch: 1959 \tTraining Loss: 24.981148\n",
      "Epoch: 1960 \tTraining Loss: 24.981144\n",
      "Epoch: 1961 \tTraining Loss: 24.981148\n",
      "Epoch: 1962 \tTraining Loss: 24.981148\n",
      "Epoch: 1963 \tTraining Loss: 24.981148\n",
      "Epoch: 1964 \tTraining Loss: 24.981148\n",
      "Epoch: 1965 \tTraining Loss: 24.981148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1966 \tTraining Loss: 24.981148\n",
      "Epoch: 1967 \tTraining Loss: 24.981148\n",
      "Epoch: 1968 \tTraining Loss: 24.981148\n",
      "Epoch: 1969 \tTraining Loss: 24.981148\n",
      "Epoch: 1970 \tTraining Loss: 24.981142\n",
      "Epoch: 1971 \tTraining Loss: 24.981142\n",
      "Epoch: 1972 \tTraining Loss: 24.981144\n",
      "Epoch: 1973 \tTraining Loss: 24.981142\n",
      "Epoch: 1974 \tTraining Loss: 24.981142\n",
      "Epoch: 1975 \tTraining Loss: 24.981142\n",
      "Epoch: 1976 \tTraining Loss: 24.981142\n",
      "Epoch: 1977 \tTraining Loss: 24.981142\n",
      "Epoch: 1978 \tTraining Loss: 24.981142\n",
      "Epoch: 1979 \tTraining Loss: 24.981142\n",
      "Epoch: 1980 \tTraining Loss: 24.981142\n",
      "Epoch: 1981 \tTraining Loss: 24.981138\n",
      "Epoch: 1982 \tTraining Loss: 24.981138\n",
      "Epoch: 1983 \tTraining Loss: 24.981136\n",
      "Epoch: 1984 \tTraining Loss: 24.981136\n",
      "Epoch: 1985 \tTraining Loss: 24.981136\n",
      "Epoch: 1986 \tTraining Loss: 24.981134\n",
      "Epoch: 1987 \tTraining Loss: 24.981134\n",
      "Epoch: 1988 \tTraining Loss: 24.981134\n",
      "Epoch: 1989 \tTraining Loss: 24.981134\n",
      "Epoch: 1990 \tTraining Loss: 24.981131\n",
      "Epoch: 1991 \tTraining Loss: 24.981129\n",
      "Epoch: 1992 \tTraining Loss: 24.981125\n",
      "Epoch: 1993 \tTraining Loss: 24.981125\n",
      "Epoch: 1994 \tTraining Loss: 24.981125\n",
      "Epoch: 1995 \tTraining Loss: 24.981125\n",
      "Epoch: 1996 \tTraining Loss: 24.981125\n",
      "Epoch: 1997 \tTraining Loss: 24.981125\n",
      "Epoch: 1998 \tTraining Loss: 24.981125\n",
      "Epoch: 1999 \tTraining Loss: 24.981123\n",
      "Epoch: 2000 \tTraining Loss: 24.981123\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000 # the number of epochs can be tuned for better performance\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(teacher.parameters(), lr=0.01)\n",
    "teacher.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # train the model \n",
    "    for idx, (data, labels) in enumerate(train_loader):\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = teacher(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss\n",
    "\n",
    "        # ------------------\n",
    "\n",
    "    # print the mean squared loss of the training dataset normalized by the mean square of the training dataset labels\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss / torch.mean(torch.pow(y_train, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean squared error 24.944223\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the trained model\n",
    "\n",
    "teacher.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "for idx, (data, labels) in enumerate(test_loader):\n",
    "    # forward pass\n",
    "    output = teacher(data)\n",
    "    test_loss += criterion(output, labels).item()\n",
    "\n",
    "# print the mean squared loss of the test dataset normalized by the mean square of the test  labels\n",
    "print('Average mean squared error {:.6f}'.format(test_loss / torch.mean(torch.pow(y_test, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width: 1\n",
      "Average mean squared error 24.944986\n",
      "width: 2\n",
      "Average mean squared error 24.945137\n",
      "width: 3\n",
      "Average mean squared error 24.953253\n",
      "width: 4\n",
      "Average mean squared error 24.945488\n",
      "width: 5\n",
      "Average mean squared error 24.949078\n",
      "width: 6\n",
      "Average mean squared error 24.944769\n",
      "width: 7\n",
      "Average mean squared error 24.944811\n",
      "width: 8\n",
      "Average mean squared error 24.945496\n",
      "width: 9\n"
     ]
    }
   ],
   "source": [
    "test_error = []\n",
    "for w in range(1,21):\n",
    "    print(\"width:\",w)\n",
    "    test_error.append(student_net(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
